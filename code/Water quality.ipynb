{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ecf1a375",
   "metadata": {},
   "source": [
    "# Recognising water potability\n",
    "\n",
    "\n",
    "By: Karin Falconer-Bailey\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4c5045",
   "metadata": {},
   "source": [
    "#### Table of contents\n",
    "* [Chapter 1: Importing functions](#Functions)\n",
    "* [Chapter 2: Data preparation](#chapter02)\n",
    "    * [2.1. Missing values](#section_2_1)\n",
    "* [Chapter 3: Data derivation](#chapter03)\n",
    "    * [3.1 Renaming columns](#section_3_1)\n",
    "    * [3.2 Detecting outliers](#section_3_2)\n",
    "* [Chapter 4: Modelling the data](#chapter04)\n",
    "    * [4.1 EDA](#section_4_1)\n",
    "        * [4.1.1. Central tendencies](#section_4_1_1)\n",
    "        * [4.1.2. Correlation plot](#section_4_1_2)\n",
    "        * [4.1.3. 5 rows](#section_4_1_3)\n",
    "        * [4.1.4. Sampling the data](#section_4_1_4)\n",
    "    * [4.2 Model construction](#section_4_2)\n",
    "        * [4.2.1. Shuffling the data](#section_4_2_1)\n",
    "        * [4.2.2. Predictor and response](#section_4_2_2)\n",
    "        * [4.2.3. Test and train split](#section_4_2_3)\n",
    "        * [4.2.4. Feature scaling](#section_4_2_4)\n",
    "        * [4.2.5. SMOTE](#section_4_2_5)\n",
    "* [Chapter 5: Building a MLP](#chapter05)\n",
    "    * [5.1 Layers and Parameters](#section_5_1)\n",
    "    * [5.2 Training results](#section_5_2)\n",
    "    * [5.3 Hyperparameter tuning](#section_5_2)\n",
    "* [Chapter 6: Building a SVM](#chapter06)\n",
    "    * [6.1 Parameters](#section_6_1)\n",
    "    * [6.2 Training results](#section_6_2)\n",
    "    * [6.3 Hyperparameter tuning](#section_6_2)\n",
    "* [Chapter 7: Final models](#chapter07)    \n",
    "    * [7.1 MLP](#section_7_1)\n",
    "    * [7.2 SVM](#section_7_2)\n",
    "    * [7.3 Confusion matrix](#section_7_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c2ec83",
   "metadata": {},
   "source": [
    "### Chapter 1: Importing functions <a class=\"anchor\" id=\"Functions\"></a>\n",
    "\n",
    "Importing the necessary Python functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "5c21f3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# scipy functions\n",
    "import scipy\n",
    "from scipy import stats\n",
    "from scipy.stats import pearsonr\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# sklearn functions\n",
    "import sklearn as sklearn\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, r2_score, accuracy_score, confusion_matrix, plot_confusion_matrix\n",
    "from sklearn import preprocessing \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_validate, GridSearchCV, train_test_split, RandomizedSearchCV\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# torch functions\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "device = 'cpu'\n",
    "\n",
    "# skorch function\n",
    "import skorch\n",
    "from skorch import NeuralNetClassifier\n",
    "from skorch.callbacks import EarlyStopping\n",
    "\n",
    "# smote functions\n",
    "from imblearn import over_sampling\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "import math\n",
    "\n",
    "from platform import python_version\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee1a285",
   "metadata": {},
   "source": [
    "print the versions of the code's packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "f356d3f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python:  3.8.8\n",
      "Sklearn: 0.24.1\n",
      "Seaborn: 0.11.1\n",
      "Scipy: 1.6.2\n",
      "Torch: 1.11.0\n",
      "Skorch: 0.11.0\n"
     ]
    }
   ],
   "source": [
    "# Reference: https://www.codegrepper.com/code-examples/python/check+package+version+python\n",
    "# Necessary packages\n",
    "\n",
    "print('Python: ', python_version())\n",
    "print('Sklearn:', sklearn.__version__)\n",
    "print('Seaborn:', sns.__version__)\n",
    "print('Scipy:', scipy.__version__)\n",
    "print('Torch:', torch.__version__)\n",
    "print('Skorch:', skorch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd706bab",
   "metadata": {},
   "source": [
    "### Chapter 2: Data preparation <a class=\"anchor\" id=\"chapter02\"></a>\n",
    "\n",
    "\n",
    "First we open the file, and save it as its own dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "4f2726c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('water_quality.csv')\n",
    "waterquality = pd.read_csv('water_quality.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3daee7bc",
   "metadata": {},
   "source": [
    "Here, I display the dataframe to show the contents of the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "3fa0a9cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ph</th>\n",
       "      <th>Hardness</th>\n",
       "      <th>Solids</th>\n",
       "      <th>Chloramines</th>\n",
       "      <th>Sulfate</th>\n",
       "      <th>Conductivity</th>\n",
       "      <th>Organic_carbon</th>\n",
       "      <th>Trihalomethanes</th>\n",
       "      <th>Turbidity</th>\n",
       "      <th>Potability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>204.890456</td>\n",
       "      <td>20791.31898</td>\n",
       "      <td>7.300212</td>\n",
       "      <td>368.516441</td>\n",
       "      <td>564.308654</td>\n",
       "      <td>10.379783</td>\n",
       "      <td>86.990970</td>\n",
       "      <td>2.963135</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.716080</td>\n",
       "      <td>129.422921</td>\n",
       "      <td>18630.05786</td>\n",
       "      <td>6.635246</td>\n",
       "      <td>NaN</td>\n",
       "      <td>592.885359</td>\n",
       "      <td>15.180013</td>\n",
       "      <td>56.329076</td>\n",
       "      <td>4.500656</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.099124</td>\n",
       "      <td>224.236259</td>\n",
       "      <td>19909.54173</td>\n",
       "      <td>9.275884</td>\n",
       "      <td>NaN</td>\n",
       "      <td>418.606213</td>\n",
       "      <td>16.868637</td>\n",
       "      <td>66.420093</td>\n",
       "      <td>3.055934</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8.316766</td>\n",
       "      <td>214.373394</td>\n",
       "      <td>22018.41744</td>\n",
       "      <td>8.059332</td>\n",
       "      <td>356.886136</td>\n",
       "      <td>363.266516</td>\n",
       "      <td>18.436525</td>\n",
       "      <td>100.341674</td>\n",
       "      <td>4.628771</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9.092223</td>\n",
       "      <td>181.101509</td>\n",
       "      <td>17978.98634</td>\n",
       "      <td>6.546600</td>\n",
       "      <td>310.135738</td>\n",
       "      <td>398.410813</td>\n",
       "      <td>11.558279</td>\n",
       "      <td>31.997993</td>\n",
       "      <td>4.075075</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3271</th>\n",
       "      <td>4.668102</td>\n",
       "      <td>193.681736</td>\n",
       "      <td>47580.99160</td>\n",
       "      <td>7.166639</td>\n",
       "      <td>359.948574</td>\n",
       "      <td>526.424171</td>\n",
       "      <td>13.894419</td>\n",
       "      <td>66.687695</td>\n",
       "      <td>4.435821</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3272</th>\n",
       "      <td>7.808856</td>\n",
       "      <td>193.553212</td>\n",
       "      <td>17329.80216</td>\n",
       "      <td>8.061362</td>\n",
       "      <td>NaN</td>\n",
       "      <td>392.449580</td>\n",
       "      <td>19.903225</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.798243</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3273</th>\n",
       "      <td>9.419510</td>\n",
       "      <td>175.762646</td>\n",
       "      <td>33155.57822</td>\n",
       "      <td>7.350233</td>\n",
       "      <td>NaN</td>\n",
       "      <td>432.044783</td>\n",
       "      <td>11.039070</td>\n",
       "      <td>69.845400</td>\n",
       "      <td>3.298875</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3274</th>\n",
       "      <td>5.126763</td>\n",
       "      <td>230.603758</td>\n",
       "      <td>11983.86938</td>\n",
       "      <td>6.303357</td>\n",
       "      <td>NaN</td>\n",
       "      <td>402.883113</td>\n",
       "      <td>11.168946</td>\n",
       "      <td>77.488213</td>\n",
       "      <td>4.708658</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3275</th>\n",
       "      <td>7.874671</td>\n",
       "      <td>195.102299</td>\n",
       "      <td>17404.17706</td>\n",
       "      <td>7.509306</td>\n",
       "      <td>NaN</td>\n",
       "      <td>327.459761</td>\n",
       "      <td>16.140368</td>\n",
       "      <td>78.698446</td>\n",
       "      <td>2.309149</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3276 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            ph    Hardness       Solids  Chloramines     Sulfate  \\\n",
       "0          NaN  204.890456  20791.31898     7.300212  368.516441   \n",
       "1     3.716080  129.422921  18630.05786     6.635246         NaN   \n",
       "2     8.099124  224.236259  19909.54173     9.275884         NaN   \n",
       "3     8.316766  214.373394  22018.41744     8.059332  356.886136   \n",
       "4     9.092223  181.101509  17978.98634     6.546600  310.135738   \n",
       "...        ...         ...          ...          ...         ...   \n",
       "3271  4.668102  193.681736  47580.99160     7.166639  359.948574   \n",
       "3272  7.808856  193.553212  17329.80216     8.061362         NaN   \n",
       "3273  9.419510  175.762646  33155.57822     7.350233         NaN   \n",
       "3274  5.126763  230.603758  11983.86938     6.303357         NaN   \n",
       "3275  7.874671  195.102299  17404.17706     7.509306         NaN   \n",
       "\n",
       "      Conductivity  Organic_carbon  Trihalomethanes  Turbidity  Potability  \n",
       "0       564.308654       10.379783        86.990970   2.963135           0  \n",
       "1       592.885359       15.180013        56.329076   4.500656           0  \n",
       "2       418.606213       16.868637        66.420093   3.055934           0  \n",
       "3       363.266516       18.436525       100.341674   4.628771           0  \n",
       "4       398.410813       11.558279        31.997993   4.075075           0  \n",
       "...            ...             ...              ...        ...         ...  \n",
       "3271    526.424171       13.894419        66.687695   4.435821           1  \n",
       "3272    392.449580       19.903225              NaN   2.798243           1  \n",
       "3273    432.044783       11.039070        69.845400   3.298875           1  \n",
       "3274    402.883113       11.168946        77.488213   4.708658           1  \n",
       "3275    327.459761       16.140368        78.698446   2.309149           1  \n",
       "\n",
       "[3276 rows x 10 columns]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "waterquality "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e551696",
   "metadata": {},
   "source": [
    "### 2.1. Missing values <a class=\"anchor\" id=\"section_2_1\"></a>\n",
    "\n",
    "Now, we check the dataframe of missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "b8813c3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ph                 491\n",
       "Hardness             0\n",
       "Solids               0\n",
       "Chloramines          0\n",
       "Sulfate            781\n",
       "Conductivity         0\n",
       "Organic_carbon       0\n",
       "Trihalomethanes    162\n",
       "Turbidity            0\n",
       "Potability           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "waterquality.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0230a6",
   "metadata": {},
   "source": [
    "Replace missing values with the corresponding column's mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "92ab5a7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ph</th>\n",
       "      <th>Hardness</th>\n",
       "      <th>Solids</th>\n",
       "      <th>Chloramines</th>\n",
       "      <th>Sulfate</th>\n",
       "      <th>Conductivity</th>\n",
       "      <th>Organic_carbon</th>\n",
       "      <th>Trihalomethanes</th>\n",
       "      <th>Turbidity</th>\n",
       "      <th>Potability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.080795</td>\n",
       "      <td>204.890456</td>\n",
       "      <td>20791.31898</td>\n",
       "      <td>7.300212</td>\n",
       "      <td>368.516441</td>\n",
       "      <td>564.308654</td>\n",
       "      <td>10.379783</td>\n",
       "      <td>86.990970</td>\n",
       "      <td>2.963135</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.716080</td>\n",
       "      <td>129.422921</td>\n",
       "      <td>18630.05786</td>\n",
       "      <td>6.635246</td>\n",
       "      <td>333.775777</td>\n",
       "      <td>592.885359</td>\n",
       "      <td>15.180013</td>\n",
       "      <td>56.329076</td>\n",
       "      <td>4.500656</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.099124</td>\n",
       "      <td>224.236259</td>\n",
       "      <td>19909.54173</td>\n",
       "      <td>9.275884</td>\n",
       "      <td>333.775777</td>\n",
       "      <td>418.606213</td>\n",
       "      <td>16.868637</td>\n",
       "      <td>66.420093</td>\n",
       "      <td>3.055934</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8.316766</td>\n",
       "      <td>214.373394</td>\n",
       "      <td>22018.41744</td>\n",
       "      <td>8.059332</td>\n",
       "      <td>356.886136</td>\n",
       "      <td>363.266516</td>\n",
       "      <td>18.436525</td>\n",
       "      <td>100.341674</td>\n",
       "      <td>4.628771</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9.092223</td>\n",
       "      <td>181.101509</td>\n",
       "      <td>17978.98634</td>\n",
       "      <td>6.546600</td>\n",
       "      <td>310.135738</td>\n",
       "      <td>398.410813</td>\n",
       "      <td>11.558279</td>\n",
       "      <td>31.997993</td>\n",
       "      <td>4.075075</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3271</th>\n",
       "      <td>4.668102</td>\n",
       "      <td>193.681736</td>\n",
       "      <td>47580.99160</td>\n",
       "      <td>7.166639</td>\n",
       "      <td>359.948574</td>\n",
       "      <td>526.424171</td>\n",
       "      <td>13.894419</td>\n",
       "      <td>66.687695</td>\n",
       "      <td>4.435821</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3272</th>\n",
       "      <td>7.808856</td>\n",
       "      <td>193.553212</td>\n",
       "      <td>17329.80216</td>\n",
       "      <td>8.061362</td>\n",
       "      <td>333.775777</td>\n",
       "      <td>392.449580</td>\n",
       "      <td>19.903225</td>\n",
       "      <td>66.396293</td>\n",
       "      <td>2.798243</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3273</th>\n",
       "      <td>9.419510</td>\n",
       "      <td>175.762646</td>\n",
       "      <td>33155.57822</td>\n",
       "      <td>7.350233</td>\n",
       "      <td>333.775777</td>\n",
       "      <td>432.044783</td>\n",
       "      <td>11.039070</td>\n",
       "      <td>69.845400</td>\n",
       "      <td>3.298875</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3274</th>\n",
       "      <td>5.126763</td>\n",
       "      <td>230.603758</td>\n",
       "      <td>11983.86938</td>\n",
       "      <td>6.303357</td>\n",
       "      <td>333.775777</td>\n",
       "      <td>402.883113</td>\n",
       "      <td>11.168946</td>\n",
       "      <td>77.488213</td>\n",
       "      <td>4.708658</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3275</th>\n",
       "      <td>7.874671</td>\n",
       "      <td>195.102299</td>\n",
       "      <td>17404.17706</td>\n",
       "      <td>7.509306</td>\n",
       "      <td>333.775777</td>\n",
       "      <td>327.459761</td>\n",
       "      <td>16.140368</td>\n",
       "      <td>78.698446</td>\n",
       "      <td>2.309149</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3276 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            ph    Hardness       Solids  Chloramines     Sulfate  \\\n",
       "0     7.080795  204.890456  20791.31898     7.300212  368.516441   \n",
       "1     3.716080  129.422921  18630.05786     6.635246  333.775777   \n",
       "2     8.099124  224.236259  19909.54173     9.275884  333.775777   \n",
       "3     8.316766  214.373394  22018.41744     8.059332  356.886136   \n",
       "4     9.092223  181.101509  17978.98634     6.546600  310.135738   \n",
       "...        ...         ...          ...          ...         ...   \n",
       "3271  4.668102  193.681736  47580.99160     7.166639  359.948574   \n",
       "3272  7.808856  193.553212  17329.80216     8.061362  333.775777   \n",
       "3273  9.419510  175.762646  33155.57822     7.350233  333.775777   \n",
       "3274  5.126763  230.603758  11983.86938     6.303357  333.775777   \n",
       "3275  7.874671  195.102299  17404.17706     7.509306  333.775777   \n",
       "\n",
       "      Conductivity  Organic_carbon  Trihalomethanes  Turbidity  Potability  \n",
       "0       564.308654       10.379783        86.990970   2.963135           0  \n",
       "1       592.885359       15.180013        56.329076   4.500656           0  \n",
       "2       418.606213       16.868637        66.420093   3.055934           0  \n",
       "3       363.266516       18.436525       100.341674   4.628771           0  \n",
       "4       398.410813       11.558279        31.997993   4.075075           0  \n",
       "...            ...             ...              ...        ...         ...  \n",
       "3271    526.424171       13.894419        66.687695   4.435821           1  \n",
       "3272    392.449580       19.903225        66.396293   2.798243           1  \n",
       "3273    432.044783       11.039070        69.845400   3.298875           1  \n",
       "3274    402.883113       11.168946        77.488213   4.708658           1  \n",
       "3275    327.459761       16.140368        78.698446   2.309149           1  \n",
       "\n",
       "[3276 rows x 10 columns]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "waterquality['ph'] = waterquality['ph'].fillna(waterquality['ph'].mean())\n",
    "waterquality['Sulfate'] = waterquality['Sulfate'].fillna(waterquality['Sulfate'].mean())\n",
    "waterquality['Trihalomethanes'] = waterquality['Trihalomethanes'].fillna(waterquality['Trihalomethanes'].mean())\n",
    "\n",
    "\n",
    "waterquality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d148ec",
   "metadata": {},
   "source": [
    "Ensure missing data has been filled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "df115105",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ph                 0\n",
       "Hardness           0\n",
       "Solids             0\n",
       "Chloramines        0\n",
       "Sulfate            0\n",
       "Conductivity       0\n",
       "Organic_carbon     0\n",
       "Trihalomethanes    0\n",
       "Turbidity          0\n",
       "Potability         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "waterquality.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25809977",
   "metadata": {},
   "source": [
    "### Chapter 3: Data derivation <a class=\"anchor\" id=\"chapter03\"></a>\n",
    "\n",
    "#### 3.1. Renaming columns <a class=\"anchor\" id=\"section_3_1\"></a>\n",
    "\n",
    "\n",
    "Rename certain columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "7fa74acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "waterquality.rename(columns = {'ph':'pH','Hardness':'Hardness','Solids':'Solids',\n",
    "                         'Chloramines':'Chloramines','Sulfate':'Sulfates','Conductivity':'Conductivity',\n",
    "                         'Organic_carbon':'Organic_carbon','Trihalomethanes':'Trihalomethanes','Turbidity':'Turbidity'\n",
    "                               ,'Potability':'Potability'}, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f624e71",
   "metadata": {},
   "source": [
    "Call the dataframe to ensure columns have been renamed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "7d07b37e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pH\n",
      "Hardness\n",
      "Solids\n",
      "Chloramines\n",
      "Sulfates\n",
      "Conductivity\n",
      "Organic_carbon\n",
      "Trihalomethanes\n",
      "Turbidity\n",
      "Potability\n"
     ]
    }
   ],
   "source": [
    "# Reference: https://wwww.geeksforgeeks.org/how-to-get-column-names-in-pandas-dataframe/amp/\n",
    "\n",
    "for col in waterquality.columns:\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d51430c",
   "metadata": {},
   "source": [
    "#### 3.2. Detecting outliers <a class=\"anchor\" id=\"section_3_2\"></a>\n",
    "\n",
    "Create histograms for all features within the data to check for their distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "557a9c46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 12 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "waterquality.hist()\n",
    "plt.show()\n",
    "plt.savefig('variabledistributions.jpeg') # Reference: https://www.tutorialspoint.com/save-figure-as-file-from-ipython-notebook-using-matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4709510b",
   "metadata": {},
   "source": [
    "H0: The feature has a normal distribution\n",
    "\n",
    "HA: The feature does not have a normal distribution\n",
    "\n",
    "a = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "0d6e9368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jarque_beraResult(statistic=257.42661280993593, pvalue=0.0)\n",
      "Jarque_beraResult(statistic=52.13699758884013, pvalue=4.770850381419223e-12)\n",
      "Jarque_beraResult(statistic=237.26207610380845, pvalue=0.0)\n",
      "Jarque_beraResult(statistic=47.14080495718171, pvalue=5.801015223738659e-11)\n",
      "Jarque_beraResult(statistic=436.0413610591753, pvalue=0.0)\n",
      "Jarque_beraResult(statistic=48.747825344325726, pvalue=2.5974333794920312e-11)\n",
      "Jarque_beraResult(statistic=0.6022947173534984, pvalue=0.7399687239002685)\n",
      "Jarque_beraResult(statistic=26.306933943475485, pvalue=1.9387502921963673e-06)\n",
      "Jarque_beraResult(statistic=0.6018342386167425, pvalue=0.7401391134463167)\n",
      "Jarque_beraResult(statistic=551.6261325743462, pvalue=0.0)\n"
     ]
    }
   ],
   "source": [
    "print(stats.jarque_bera(waterquality['pH']))\n",
    "print(stats.jarque_bera(waterquality['Hardness']))\n",
    "print(stats.jarque_bera(waterquality['Solids']))\n",
    "print(stats.jarque_bera(waterquality['Chloramines']))\n",
    "print(stats.jarque_bera(waterquality['Sulfates']))\n",
    "print(stats.jarque_bera(waterquality['Conductivity']))\n",
    "print(stats.jarque_bera(waterquality['Organic_carbon']))\n",
    "print(stats.jarque_bera(waterquality['Trihalomethanes']))\n",
    "print(stats.jarque_bera(waterquality['Turbidity']))\n",
    "print(stats.jarque_bera(waterquality['Potability']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70932954",
   "metadata": {},
   "source": [
    "Based on the results we reject the null hypothesis for the following columns:\n",
    "\n",
    "    Hardness, Chloramines, Conductivity, Organic_carbon, Trihalomethanes, Turbidity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a624c9af",
   "metadata": {},
   "source": [
    "Create a boxplot for the distributions of Hardness, Chloramines, Conductivity, Organic_carbon, Trihalomethanes, and Turbidity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "40aaf758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25    176.850538\n",
      "0.50    196.967627\n",
      "0.75    216.667456\n",
      "Name: Hardness, dtype: float64\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(waterquality['Hardness'].quantile([0.25, 0.5, 0.75]))\n",
    "waterquality['Hardness'].plot(kind='box', title='Hardness distribution',color='r', figsize=(5,10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "936dab86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25    6.127421\n",
      "0.50    7.130299\n",
      "0.75    8.114887\n",
      "Name: Chloramines, dtype: float64\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(waterquality['Chloramines'].quantile([0.25, 0.5, 0.75]))\n",
    "waterquality['Chloramines'].plot(kind='box', title='Chloramine distribution',color='r', figsize=(5,10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "5b962c05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25    365.734414\n",
      "0.50    421.884968\n",
      "0.75    481.792305\n",
      "Name: Conductivity, dtype: float64\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAT8AAAJOCAYAAADBMABSAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAftElEQVR4nO3df5SddX3g8fenQ0wqKj8DxYCCmtqBOUewU2oLxxq1FftDaLdIUm1jmcJhF9Pa2lZwXNf+GLZF1i2bLeumJm1aZUigVbL2x0Lp0N1xu8XBogYCCzX8SBMhSBSIJU7Gz/5xn9CbMMlMMpPeufm8X+fMufd+7/M893tnJu8897k/JjITSarmOzo9AUnqBOMnqSTjJ6kk4yepJOMnqSTjJ6kk41dYRLwnIkYPw3ZfERHPRkTPNJb9eET8+9meQ7PtN0XElrbL90bEm2Zp2++KiNvaLmdEvGY2tt1s79mIeNVsbU8vZPzmoIj4mYgYa/4BbIuIv4yI8zs9r/2JiIcj4q17Lmfmo5n5ksycmGrdzLwiM3+r2c5esZptmXlWZt55oGUi4vQmZEdNsa1PZeaPzMa8IuLOiPiFfbb/ksz8ymxsX5MzfnNMRPwK8HvANcDJwCuAG4ALOzgttZkqjOoSmenXHPkCjgGeBS4+wDLzacVxa/P1e8D85ro3AVuA9wNPANuAn29b9wRgA/A0cBfwW8Boc93pQAJHtS1/J/ALbZcvAzYBzwD3Aa8H/gT4NvDPzdx/vX1bwFJgbJ/78MvAhub8HwG/DRzdbOPbzXaeBV4OfBM4oW3d7wW2A/Mm+d58Z7O9Hc38fg3Y0nb9w8Bbm/PnAmPN9+Jx4GPN+KPN3PfM4QeA9wCfA/4z8FQz3/fs+d416yXwi8BXgCeBjwLf0Vz3EeCTbcu2f3+GgAngueb2/mvb9l7T9nvxx839fgT4UNu23wOMAtc193sz8PZO/y53w5d7fnPLDwALgE8fYJlB4A3A2cDraP0j/lDb9d9F6x/LImAA+P2IOK657vdp/SM7Bbi0+ZqWiLiY1j/inwNeBrwD+Fpm/iytYPxEth6qXbvPqhuA10bE4raxnwFubF8oM3cCbwe2Ntt5SWZupRXgd7Yt+m7gpswcn2Sa/wF4dfP1NmD5Ae7S9cD1mfmyZvn1zfgbm9Njmzn8XXP5+2mF7SRawZrMTwL9tP5TuJBpfH8zcxD438B7m9t77ySLraT1M30V8EO0fgY/33b99wMPACcC1wKrIyKmuu3qjN/ccgLwZGbuPsAy7wJ+MzOfyMztwG8AP9t2/Xhz/Xhm/gWtvYnXNk8+/Bvgw5m5MzM3AmsPYm6/AFybmZ/Plocy85GpVsrMbwK3AssAmgh+D60oTsdaWsGjuQ/LaO1tTuadwFBmPpWZjwH/5QDbHQdeExEnZuazmfl/p5jH1sxcmZm7M/Of97PM7za3/SitPfJlU2xzSs19vgS4OjOfycyHgf/E3j/zRzLzD7J1jHUtrf/cTp7pbR/pjN/c8jXgxCmOKb2c1kOfPR5pxp7fxj7x/CbwEmAhrYdZj+2z7nSdBvzjQSzf7kb+JQQ/A3ymieJ03Aqc2Tzz+cPANzLzrv0s+3Kmf/8GgO8G7o+Iz0fEj08xj8emuH7fZfb9uRyqE4EX8cKf+aK2y1/dc6bt+/qSWbjtI5rxm1v+jtbD0osOsMxW4JVtl1/RjE1lO7CbVsTa191jZ3P64rax72o7/xith4eTmeqjgW6jFfWzaUXwxv0s94LtZOZztB6SvovW3s7+9vqgdYxzf/dv3+0+mJnLaD2M/V3glog4erI57G9uk9j3tvf8XHay/+/rVNt+ktZe6r4/83+axnx0AMZvDsnMbwAfpnWc7qKIeHFEzIuIt0fEnmNpw8CHImJhRJzYLP/JaWx7Avgz4CPNds+k7ZhY8xD6n4B3R0RPRFzK3rH7BPCrEfG90fKaiNjzD/JxWsej9nfbu4FbaD0JcDxw+34WfRw4ISKO2Wf8j2kd2H/HFPd1PXB1RBwXEacCK/a3YES8OyIWZua3ga83wxO0/pP49oHuzwH8WnPbpwG/BKxrxu8B3ti8/vEY4Op91tvv96/5ua0HhiLipc33/FeYxs9cB2b85pjM/BitX+4P0fqH+BjwXuAzzSK/TetZyi8BXwa+0IxNx3tpPRz6Kq1nRf9wn+svo/UM6deAs4D/0zavm2kd6L+R1rO9n6EVMoD/SCvIX4+IX93Pbd8IvBW4eX/HNDPzflpx/0qzrZc345+jFaQvNMe89uc3aD0k3Exrb/NAe4kXAPdGxLO0nvxYmpnPNQ8bh4DPNXN4wwG2sa9bgbtpxe7PgdXN/G+nFcIvNdd/dp/1rgd+OiJ2RMRkxylX0Np7/AqtZ3ZvBNYcxLw0icj0w0w190XE3wA3ZuYnOj0XHRmMn+a8iPg+Wg+VT8vMZzo9Hx0ZfNirOS0i1gJ/DbzP8Gk2uecnqST3/CSVNCfeoH3iiSfm6aef3ulpSDrC3H333U9m5sLJrpsT8Tv99NMZGxvr9DQkHWEiYr/v8vFhr6SSjJ+kkoyfpJKMn6SSjJ+kkoyfpJKMn6SSjJ+kkoyfpJKMn6SSjJ+kkoyfpJKMn6SSjJ+kkoyfpJKMn6SSjJ+kkoyfpJKMn6SSjJ+kkoyfusbw8DB9fX309PTQ19fH8PBwp6ekLjYn/nqbNJXh4WEGBwdZvXo1559/PqOjowwMDACwbNmyDs9O3Sgys9NzoL+/P/3TlTqQvr4+Vq5cyZIlS54fGxkZYcWKFWzcuLGDM9NcFhF3Z2b/pNcZP3WDnp4ennvuOebNm/f82Pj4OAsWLGBiYqKDM9NcdqD4ecxPXaG3t5fR0dG9xkZHR+nt7e3QjNTtjJ+6wuDgIAMDA4yMjDA+Ps7IyAgDAwMMDg52emrqUj7hoa6w50mNFStWsGnTJnp7exkaGvLJDh0yj/lJOmJ5zE+S9mH8JJVk/CSVZPwklWT8JJVk/CSVZPwklWT8JJVk/CSVZPwklWT8JJVk/CSVZPwklWT8JJVk/CSVZPwklWT8JJVk/CSVZPwklWT8JJVk/CSVZPwklWT8JJVk/CSVZPwklWT8JJVk/CSVZPwklWT8JJVk/CSVZPwklWT81DWGh4fp6+ujp6eHvr4+hoeHOz0ldbGjOj0BaTqGh4cZHBxk9erVnH/++YyOjjIwMADAsmXLOjw7daPIzE7Pgf7+/hwbG+v0NDSH9fX1sXLlSpYsWfL82MjICCtWrGDjxo0dnJnmsoi4OzP7J73O+Kkb9PT08NxzzzFv3rznx8bHx1mwYAETExMdnJnmsgPFz2N+6gq9vb2Mjo7uNTY6Okpvb2+HZqRuZ/zUFQYHBxkYGGBkZITx8XFGRkYYGBhgcHCw01NTl/IJD3WFPU9qrFixgk2bNtHb28vQ0JBPduiQTXnMLyJeC6xrG3oV8GHgj5vx04GHgXdm5o5mnauBAWAC+MXM/J8Hug2P+Uk6HGZ0zC8zH8jMszPzbOB7gW8CnwauAu7IzMXAHc1lIuJMYClwFnABcENE9MzGHZGk2XKwx/zeAvxjZj4CXAisbcbXAhc15y8EbsrMXZm5GXgIOHcW5ipJs+Zg47cU2POy+pMzcxtAc3pSM74IeKxtnS3N2F4i4vKIGIuIse3btx/kNCRpZqYdv4h4EfAO4OapFp1k7AUHFjNzVWb2Z2b/woULpzsNFebb2zSbDubZ3rcDX8jMx5vLj0fEKZm5LSJOAZ5oxrcAp7WtdyqwdeZTVWW+vU2z7WAe9i7jXx7yAmwAljfnlwO3to0vjYj5EXEGsBi4a6YTVW1DQ0OsXr2aJUuWMG/ePJYsWcLq1asZGhrq9NTUpab19raIeDGt43ivysxvNGMnAOuBVwCPAhdn5lPNdYPApcBu4H2Z+ZcH2r4vddFUfHubDsWM396Wmd/MzBP2hK8Z+1pmviUzFzenT7VdN5SZr87M104VPmk6fHubZptvb1NX8O1tmm2+vU1dwbe3abb5kVaSjlh+pJUk7cP4SSrJ+EkqyfhJKsn4SSrJ+EkqyfhJKsn4SSrJ+EkqyfhJKsn4qWv4Sc6aTX6wgbqCn+Ss2eYHG6gr9PX1cdFFF/GZz3zm+U912XN548aNnZ6e5qgDfbCBe37qCvfddx87d+5kzZo1z+/5XXrppTzyyCOdnpq6lPFTV3jRi17Eeeedt9fn+Z133nls27at01NTl/IJD3WFXbt2sW7dOi699FKeeeYZLr30UtatW8euXbs6PTV1KeOnrjB//nwuueQS1qxZw0tf+lLWrFnDJZdcwvz58zs9NXUp46eu8K1vfYvbbruNnTt3ArBz505uu+02vvWtb3V4ZupWxk9dYdGiRYyPjwOw5xUK4+PjLFq0qJPTUhczfuoaCxYsYM2aNezatYs1a9awYMGCTk9JXcz4qSts3bqVa6+9lhUrVrBgwQJWrFjBtddey9atWzs9NXUpX+qirtDb28upp5661wuaR0ZG/KPlOmTGT11hcHCQSy65hKOPPppHHnmEV77ylezcuZPrr7++01NTl/Jhr7pORHR6CjoCGD91haGhIdatW8fmzZuZmJhg8+bNrFu3jqGhoU5PTV3K+KkrbNq0iZtvvpkFCxYQESxYsICbb76ZTZs2dXpq6lLGT13h2GOPZdWqVVxzzTXs3LmTa665hlWrVnHsscd2emrqUsZPXeHpp5/mmGOO4ZxzzmHevHmcc845HHPMMTz99NOdnpq6lPFTV9i9ezfXXXfdXq/zu+6669i9e3enp6YuZfzUFebPn8+OHTvYuHEjExMTbNy4kR07dvjBBjpkvs5PXeGyyy7jAx/4AABXXHEFH//4x/nABz7AFVdc0eGZqVsZP3WFlStXAvDBD36Q97///cyfP58rrrji+XHpYPk3PDQ3HK4XLs+B3291jn/DQ3PfwUQqwqhpxnzCQ1JJxk9SScZPUknGT1JJxk9SScZPUknGT1JJxk9SScZPUknGT1JJxk9SScZPUknGT1JJxk9SScZPUknGT1JJxk9SScZPUknGT1JJxk9SScZPUknGT1JJxk9SScZPUknGT1JJxk9SScZPUknGT1JJxk9SScZPUknGT1JJxk9SScZPUknGT1JJxk9SScZPUknGT1JJxk9SScZPUknGT1JJ04pfRBwbEbdExP0RsSkifiAijo+I2yPiweb0uLblr46IhyLigYh42+GbviQdmunu+V0P/FVmfg/wOmATcBVwR2YuBu5oLhMRZwJLgbOAC4AbIqJnticuSTMxZfwi4mXAG4HVAJn5rcz8OnAhsLZZbC1wUXP+QuCmzNyVmZuBh4BzZ3fakjQz09nzexWwHfjDiPiHiPhERBwNnJyZ2wCa05Oa5RcBj7Wtv6UZ20tEXB4RYxExtn379hndCUk6WNOJ31HA64H/lpnnADtpHuLuR0wyli8YyFyVmf2Z2b9w4cJpTVaSZst04rcF2JKZf99cvoVWDB+PiFMAmtMn2pY/rW39U4GtszNdSZodU8YvM78KPBYRr22G3gLcB2wAljdjy4Fbm/MbgKURMT8izgAWA3fN6qwlaYaOmuZyK4BPRcSLgK8AP08rnOsjYgB4FLgYIDPvjYj1tAK5G7gyMydmfeaSNAPTil9m3gP0T3LVW/az/BAwdOjTkqTDy3d4SCrJ+EkqyfhJKsn4SSrJ+EkqyfhJKsn4SSrJ+EkqyfhJKsn4SSrJ+EkqyfhJKsn4SSrJ+EkqyfhJKsn4SSrJ+EkqyfhJKsn4SSrJ+EkqyfhJKsn4SSrJ+EkqyfhJKsn4SSrJ+EkqyfhJKsn4SSrJ+EkqyfhJKsn4SSrJ+EkqyfhJKsn4SSrJ+EkqyfhJKsn4SSrJ+EkqyfhJKsn4SSrJ+EkqyfhJKsn4SSrJ+EkqyfhJKsn4SSrJ+EkqyfhJKsn4SSrJ+EkqyfhJKsn4SSrJ+EkqyfhJKsn4SSrJ+EkqyfhJKsn4SSrJ+EkqyfhJKsn4SSrJ+EkqyfhJKsn4SSrJ+EkqyfhJKsn4SSrJ+EkqyfhJKsn4SSrJ+EkqyfhJKsn4SSrJ+EkqyfhJKsn4SSrJ+EkqyfhJKmla8YuIhyPiyxFxT0SMNWPHR8TtEfFgc3pc2/JXR8RDEfFARLztcE1ekg7Vwez5LcnMszOzv7l8FXBHZi4G7mguExFnAkuBs4ALgBsiomcW5yxJMzaTh70XAmub82uBi9rGb8rMXZm5GXgIOHcGtyNJs2668Uvgtoi4OyIub8ZOzsxtAM3pSc34IuCxtnW3NGN7iYjLI2IsIsa2b99+aLOXpEN01DSXOy8zt0bEScDtEXH/AZaNScbyBQOZq4BVAP39/S+4XpIOp2nt+WXm1ub0CeDTtB7GPh4RpwA0p080i28BTmtb/VRg62xNWJJmw5Txi4ijI+Kle84DPwJsBDYAy5vFlgO3Nuc3AEsjYn5EnAEsBu6a7YlL0kxM52HvycCnI2LP8jdm5l9FxOeB9RExADwKXAyQmfdGxHrgPmA3cGVmThyW2UvSIZoyfpn5FeB1k4x/DXjLftYZAoZmPDtJOkx8h4ekkoyfpJKMn6SSjJ+kkoyfpJKMn6SSjJ+kkoyfpJKMn6SSjJ+kkqb7kVbSoTn+eNixY/a3G5N9ctoMHHccPPXU7G5Tc5rx0+G1YwdkF3xc42zHVHOeD3sllWT8JJVk/CSVZPwklWT8JJVk/CSVZPwklWT8JJVk/CSVZPwklWT8JJVk/CSVZPwklWT8JJVk/CSVZPwklWT8JJVk/CSVZPwklWT8JJVk/CSVZPwklWT8JJVk/CSVZPwklWT8JJVk/CSVZPwklWT8JJVk/CSVZPwklWT8JJVk/CSVZPwklWT8JJVk/CSVZPwklWT8JJVk/CSVZPwklWT8JJVk/CSVZPwklWT8JJVk/CSVZPwklWT8JJVk/CSVZPwklXRUpyegAiI6PQPpBYyfDr/MTs9gaga6HB/2SirJ+EkqyfhJKsn4SSrJ+EkqyfhJKsn4SSrJ+EkqyfhJKsn4SSrJ+EkqyfhJKsn4SSrJ+Ekqadrxi4ieiPiHiPhsc/n4iLg9Ih5sTo9rW/bqiHgoIh6IiLcdjolL0kwczJ7fLwGb2i5fBdyRmYuBO5rLRMSZwFLgLOAC4IaI6Jmd6UrS7JhW/CLiVODHgE+0DV8IrG3OrwUuahu/KTN3ZeZm4CHg3FmZrSTNkunu+f0e8OvAt9vGTs7MbQDN6UnN+CLgsbbltjRje4mIyyNiLCLGtm/ffrDzlqQZmTJ+EfHjwBOZefc0tznZ54G/4HPMM3NVZvZnZv/ChQunuWlJmh3T+Rse5wHviIgfBRYAL4uITwKPR8QpmbktIk4BnmiW3wKc1rb+qcDW2Zy0JM3UlHt+mXl1Zp6amafTeiLjbzLz3cAGYHmz2HLg1ub8BmBpRMyPiDOAxcBdsz5zSZqBmfz1tt8B1kfEAPAocDFAZt4bEeuB+4DdwJWZOTHjmUrSLIqcA39WsL+/P8fGxjo9DR0OEd3zpyu7YZ46KBFxd2b2T3ad7/CQVJLxk1SS8ZNUkvGTVJLxk1SS8ZNUkvGTVJLxk1SS8ZNUkvGTVJLxk1SS8ZNUkvGTVJLxk1SS8ZNUkvGTVJLxk1SS8ZNUkvGTVJLxk1SS8ZNUkvGTVJLxk1SS8ZNU0lGdnoAKiOj0DKZ23HGdnoH+lRk/HV6Zs7/NiMOzXZXiw15JJRk/SSUZP0klGT9JJRk/SSUZP0klGT9JJRk/SSUZP0klGT9JJRk/SSUZP0klGT9JJRk/SSUZP0klGT9JJRk/SSUZP0klGT9JJRk/SSUZP0klGT9JJRk/SSUZP0klGT9JJRk/SSUZP0klGT9JJRk/SSUZP0klGT9JJRk/SSUZP0klGT9JJRk/SSUZP0klGT9JJRk/SSUZP0klGT9JJRk/SSUZP0klGT9JJRk/SSUZP0klGT9JJRk/SSUZP0klGT9JJRk/SSUZP0klGT9JJU0Zv4hYEBF3RcQXI+LeiPiNZvz4iLg9Ih5sTo9rW+fqiHgoIh6IiLcdzjsgSYdiOnt+u4A3Z+brgLOBCyLiDcBVwB2ZuRi4o7lMRJwJLAXOAi4AboiInsMwd0k6ZFPGL1uebS7Oa74SuBBY24yvBS5qzl8I3JSZuzJzM/AQcO5sTlqSZmpax/wioici7gGeAG7PzL8HTs7MbQDN6UnN4ouAx9pW39KM7bvNyyNiLCLGtm/fPoO7IEkHb1rxy8yJzDwbOBU4NyL6DrB4TLaJSba5KjP7M7N/4cKF05qsJM2Wg3q2NzO/DtxJ61je4xFxCkBz+kSz2BbgtLbVTgW2znSikjSbpvNs78KIOLY5/53AW4H7gQ3A8max5cCtzfkNwNKImB8RZwCLgbtmed6SNCNHTWOZU4C1zTO23wGsz8zPRsTfAesjYgB4FLgYIDPvjYj1wH3AbuDKzJw4PNOXpEMTmS84HPevrr+/P8fGxjo9DXWLCJgDv7ea+yLi7szsn+w63+EhqSTjJ6kk4yepJOMnqSTjJ6kk4yepJOMnqSTjJ6kk4yepJOMnqSTjJ6kk4yepJOMnqSTjJ6kk4yepJOMnqSTjJ6kk4yepJOMnqSTjJ6kk4yepJOMnqSTjJ6kk4yepJOMnqSTjJ6kk4yepJOMnqSTjJ6kk4yepJOMnqSTjJ6kk4yepJOMnqSTjJ6kk4yepJOMnqSTjJ6kk4yepJOMnqSTjJ6kk4yepJOMnqSTjJ6kk4yepJOMnqSTjJ6kk4yepJOMnqSTjJ6kk4yepJOMnqSTjJ6kk4yepJOMnqSTjJ6kk4yepJOMnqSTjJ6kk4yepJOMnqSTjJ6kk4yepJOMnqSTjJ6kk4yepJOMnqSTjJ6kk4yepJOMnqSTjJ6kk4yepJOMnqSTjJ6kk4yepJOMnqSTjJ6mkKeMXEadFxEhEbIqIeyPil5rx4yPi9oh4sDk9rm2dqyPioYh4ICLedjjvgCQdiuns+e0G3p+ZvcAbgCsj4kzgKuCOzFwM3NFcprluKXAWcAFwQ0T0HI7JS9KhmjJ+mbktM7/QnH8G2AQsAi4E1jaLrQUuas5fCNyUmbsyczPwEHDuLM9bR5qI6X8dzPLSfhzUMb+IOB04B/h74OTM3AatQAInNYstAh5rW21LM7bvti6PiLGIGNu+ffshTF1HlMzD8yXtx7TjFxEvAf4UeF9mPn2gRScZe8FvYWauysz+zOxfuHDhdKchSbNiWvGLiHm0wvepzPyzZvjxiDiluf4U4IlmfAtwWtvqpwJbZ2e6kjQ7pvNsbwCrgU2Z+bG2qzYAy5vzy4Fb28aXRsT8iDgDWAzcNXtTlqSZO2oay5wH/Czw5Yi4pxn7IPA7wPqIGAAeBS4GyMx7I2I9cB+tZ4qvzMyJ2Z64JM3ElPHLzFEmP44H8Jb9rDMEDM1gXpJ0WPkOD0klGT9JJRk/SSUZP0klGT9JJRk/SSUZP0klGT91jeHhYfr6+ujp6aGvr4/h4eFOT0ldbDrv8JA6bnh4mMHBQVavXs3555/P6OgoAwMDACxbtqzDs1M3ipwDH/vT39+fY2NjnZ6G5rC+vj5WrlzJkiVLnh8bGRlhxYoVbNy4sYMz01wWEXdnZv+k1xk/dYOenh6ee+455s2b9/zY+Pg4CxYsYGLCt45rcgeKn8f81BV6e3sZHR3da2x0dJTe3t4OzUjdzvipKwwODjIwMMDIyAjj4+OMjIwwMDDA4OBgp6emLuUTHuoKe57UWLFiBZs2baK3t5ehoSGf7NAh85ifpCOWx/wkaR/GT1JJxk9SScZPUknGT1JJxk9SScZPUknGT1JJxk9SScZPUknGT1JJxk9SScZPUknGT1JJxk9SScZPUknGT1JJxk9SScZPUknGT1JJc+IPGEXEduCRTs9DXeNE4MlOT0Jd4ZWZuXCyK+ZE/KSDERFj+/uLXNJ0+bBXUknGT1JJxk/daFWnJ6Du5zE/SSW55yepJOMnqSTjp0MWEd8VETdFxD9GxH0R8RcR8d0z3OabIuKzh7juRRFxZtvl34yIt06xzl9ExLHN1787lNtVdzJ+OiQREcCngTsz89WZeSbwQeDkDk7rIuD5+GXmhzPzrw+0Qmb+aGZ+HTgWMH6FGD8dqiXAeGZ+fM9AZt4DjEbERyNiY0R8OSIugef36O6MiFsi4v6I+FQTUCLigmZsFPipPduLiI9ExK+2Xd4YEac3538uIr4UEV+MiD+JiB8E3gF8NCLuiYhXR8QfRcRPR8TbI2J923beFBH/ozn/cEScCPwO8Opm3Y8227ywbZ1PRcQ7DsP3UR1yVKcnoK7VB9w9yfhPAWcDr6P1NrTPR8T/aq47BzgL2Ap8DjgvIsaAPwDeDDwErJvqhiPiLGAQOC8zn4yI4zPzqYjYAHw2M29pltuzyu3Af4+IozNzJ3DJJLdzFdCXmWc36/4Q8MvArRFxDPCDwPKp5qbu4Z6fZtv5wHBmTmTm48DfAt/XXHdXZm7JzG8D9wCnA98DbM7MB7P1uqtPTuM23gzckplPAmTmUwdaODN3A38F/EREHAX8GHDrFOv8LfCaiDgJWAb8abMdHSHc89Ohuhf46UnGY5KxPXa1nZ/gX37/9vdi093s/R/0grbbONgXqK4DrgSeAj6fmc9MY50/Ad4FLAUuPcjb0xznnp8O1d8A8yPisj0DEfF9wA7gkojoiYiFwBuBuw6wnfuBMyLi1c3lZW3XPQy8vtn264EzmvE7gHdGxAnNdcc3488AL93P7dzZbOsyJn9oPdm6fwS8DyAz7z3AfVAXMn46JM1D1J8Efrh5qcu9wEeAG4EvAV+kFchfz8yvHmA7zwGXA3/ePOHR/tFmfwocHxH3AP8W+H/NOvcCQ8DfRsQXgY81y98E/FpE/ENbTPfczgTwWeDtzem+8/ga8LnmSZWPNmOPA5uAP5zu90Xdw7e3SfsRES8Gvgy8PjO/0en5aHa55ydNonlx9P3ASsN3ZHLPT1JJ7vlJKsn4SSrJ+EkqyfhJKsn4SSrp/wOrdjVicFxxjgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 360x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(waterquality['Conductivity'].quantile([0.25, 0.5, 0.75]))\n",
    "waterquality['Conductivity'].plot(kind='box', title='Conductivity distribution',color='r', figsize=(5,10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "bc39e30f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25    12.065801\n",
      "0.50    14.218338\n",
      "0.75    16.557652\n",
      "Name: Organic_carbon, dtype: float64\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(waterquality['Organic_carbon'].quantile([0.25, 0.5, 0.75]))\n",
    "waterquality['Organic_carbon'].plot(kind='box', title='Organic carbon distribution',color='r', figsize=(5,10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "a70d0799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25    56.647656\n",
      "0.50    66.396293\n",
      "0.75    76.666609\n",
      "Name: Trihalomethanes, dtype: float64\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(waterquality['Trihalomethanes'].quantile([0.25, 0.5, 0.75]))\n",
    "waterquality['Trihalomethanes'].plot(kind='box', title='Trihalomethane distribution',color='r', figsize=(5,10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "b041d66b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25    3.439711\n",
      "0.50    3.955028\n",
      "0.75    4.500320\n",
      "Name: Turbidity, dtype: float64\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAATIAAAJOCAYAAAA0zoviAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAbaUlEQVR4nO3df3CcB33n8c8XSdbajmkSorlDNWnC3ZVbZ9uGokK5+BxkGgbKAZ4ObbFJ0yCB47l2z0zS2K23P+Cu1tSZNkeQO9W4sUgOLB1pAHdCS6fpeN1W9EpOTnw5JyI3U5pcZAdQcKC2XGFF+d4f+0i3VrTSrry7j77i/ZrxZPfRs89+1wlvnufZZ1fm7gKAyF6T9gAAcLkIGYDwCBmA8AgZgPAIGYDwCBmA8AjZKmRmx83soxV+dq2ZnTezlgo//4SZfa6ades5p5l92Mz+so7bfsrM3pHcnntNddr2PjO7v17bw+UjZCtEEozZP6+Y2T+X3f9wvZ7H3f+vu1/h7jO1rrtYIOsw1xF3f9dS65nZA2b2u1Vs7wZ3P365c5nZO8xsfN62+9y9IX8PWJ7WtAdAibtfMXvbzJ6V9FF3/6tatmFmJsnqPFooZtbq7i+nPQeaiz2yFW7+YZGZXWdmbmatyf3jZrbfzL4q6YKkNyar/isze8zMvmdmf2pmV1d4/PVm9tdmds7MHpV0zULPZWb7Jf17SQeTvcSDZvaHZvYH8+Z9xMw+XuG13GJmX09mOqiy6JrZ7WY2ktw2M/uvZvbtZN0nzSxnZjslfVjSnmSGR5L1nzWzvWb2pKTJZN5nzexnyp4+Y2afT17n42b2E2XP7Wb2r8vuP2Bmv2tm6yV9RVJn2d5x5wL/Tt6fHMp+N/n3kS372bNm9mvJa/heMkOm0r9vLA8hWx1+SdJOSRskPZcsu01Sj6ROSS9L+nSFxw5JOqFSwP6LpF9eaCV3L0j6W0m/mhxu/qqkByVtN7PXSJKZXSPpnZKG5z8++dkXJP1m8lz/IOmmCjO9S9IWST8q6UpJvyjpO+5+SNIRSfckM7yv7DHbJb1X0pUV9sg+IOlPJF2dvOajZtZW4flnX/OkpPdIOpM83xXufmbe6/rR5PV+XFKHpD+X9IiZrSlb7RckvVvS9ZJ+XNLtiz0vakfIVocH3P0pd3/Z3aeTZZ9191PJ/xh/S9IvzD9pb2bXSvopSb/l7t9397+R9Ei1T+ruj0n6nkrxkqQPSTru7t9aYPWflfS0uz+czPgpSd+ssOlplaL8byWZu4+5+wtLjPNpd3/e3f+5ws9PlD33vZIykn56iW1W4xcl/Zm7P5ps+/clrZX07+bNdsbdz6r093tjHZ4XZQjZ6vD8Esuek9SmssPGRKekl5LYla9biwcl3ZrcvlXSZyus11k+k5e+rWChueXuxyQdlPSHkr5lZofM7LVLzLHgthb6ubu/Imk8melydars7yzZ9vOSfrhsnfJgX5B0hVBXhGzlm5S0ruz+v1xgnYW+wuQNZbevVWkv58V567wg6arkXFD5upUs9Dyfk/SB5JxTVtLRCo99oXym5I2JN1RYV+7+aXd/i6QbVDrEvHuRGRZbPqv8uV8jaaOk2cPEC6r8d7zUds9I+pGybc++rtNLPA51RMhWvpOStljpmq4fkvQbVT7uVjPbZGbrJP1nSQ/Pv+TC3Z+TNCrpk2a2xsw2S3rfAtua9S39/zcTZrcxLul/qrQn9oVFDu3+TNINZvZzyRsN/0kLR1lm9lNm9rbkHNakpClJs7O/aoYqvaXsuT8u6fuS/j752UlJO8ysxczeLenmssd9S9Lrkr/7hTwk6b1m9s5k3ruSbf/dMmbEMhGyFc7dH5X0eUlPqnRS/stVPvSzkh5Q6bAmo1I4FrJD0tsknZX0O5L+2yLbvE/SB83sJTMrf/PgQUk/psqHlXL3FyX9vKTfk/QdSf9G0lcrrP5aSX8s6SWVDtu+o9K5J0k6LGlT8g7h0UVmne9PVTqf9ZJKb478XNn5xN0qBfy7Kr0rOrddd/+6Sifzv5E85yWHo+7+jEqH1P0q7fG+T9L73P1iDbPhMhlfrIjLZWZbVDrEvC45RwQ0FXtkuCzJ4dRuSfcTMaSFkGHZkgs/vyvp9SpdTgGkYslDSzN7k0rnaGa9UdJvu/unGjgXAFStpnNkyQWVpyW9LXnHCwBSV+uHxt8p6R+Witg111zj11133bKHAoCFnDhx4kV375i/vNaQfUgLfI5OkpIP9O6UpGuvvVajo6M1DwkAizGzBXeiqj7Zn3wI9v0qffD2Vdz9kLt3uXtXR8erggkADVPLu5bvkfR4hQ8EA0BqagnZdlU4rASANFUVsuTzerdI+mJjxwGA2lV1st/dL0h6XYNnAYBl4cp+AOERMgDhETIA4REyAOERMgDhETIA4REyAOERMgDhETIA4REyAOERMgDhETIA4REyAOERMgDhETIA4REypGJ4eFi5XE4tLS3K5XIaHubLh7F8tf4WJeCyDQ8Pq1Ao6PDhw9q8ebNGRkbU29srSdq+fXvK0yGimn5Bb7W6urqcXweHSnK5nPr7+9Xd3T23rFgsKp/P69SpUylOhpXOzE64e9erlhMyNFtLS4umpqbU1tY2t2x6elqZTEYzMzMpToaVrlLIOEeGpstmsxoZGblk2cjIiLLZbEoTITpChqYrFArq7e1VsVjU9PS0isWient7VSgU0h4NQXGyH003e0I/n89rbGxM2WxW+/fv50Q/lo1zZADC4BwZgFWLkAEIj5ABCI+QAQiPkAEIj5ABCI+QAQiPkAEIj5ABCI+QAQiPkAEIj5ABCI+QAQiPkAEIj5ABCI+QIRX5fF6ZTEZmpkwmo3w+n/ZICIyQoeny+bwGBgbU19enyclJ9fX1aWBggJhh2fiGWDRdJpNRX1+f7rzzzrll9957r/bt26epqakUJ8NKx6+Dw4phZpqcnNS6devmll24cEHr169XI/57xOrBV11jxWhvb9fAwMAlywYGBtTe3p7SRIiO36KEpvvYxz6mvXv3SpJ27dqlgYEB7d27V7t27Up5MkRFyNB0/f39kqR9+/bprrvuUnt7u3bt2jW3HKgV58gAhME5MgCrFiEDEB4hQyqGh4eVy+XU0tKiXC6n4eHhtEdCYJzsR9MNDw+rUCjo8OHD2rx5s0ZGRtTb2ytJ2r59e8rTISJO9qPpcrmc+vv71d3dPbesWCwqn8/r1KlTKU6GlY6T/VgxxsbGND4+fsmh5fj4uMbGxtIeDUFxaImm6+zs1J49ezQ0NDR3aLljxw51dnamPRqCYo8MqTCzRe8DtSBkaLozZ87owIEDc99Jls/ndeDAAZ05cybt0RAUIUPTZbNZPfPMM5cse+aZZ5TNZlOaCNERMjRdd3e3Dhw4oJ6eHp07d049PT06cODAJe9iArUgZGi6YrGovXv3anBwUBs2bNDg4KD27t2rYrGY9mgIiuvI0HQtLS2amppSW1vb3LLp6WllMhnNzMykOBlWOq4jw4qRzWY1MjJyybKRkRHOkWHZCBmarlAoqLe3V8ViUdPT0yoWi+rt7VWhUEh7NATFBbFoutnPU+bzeY2NjSmbzWr//v18zhLLxjkyAGFwjgwrCl/jg3ri0BJNx9f4oN44tETT5XI5bdu2TUePHp07RzZ7n6/xwWIqHVqyR4ame/rppzU5OanBwcG5PbKenh4999xzaY+GoDhHhqZbs2aN8vm8uru71dbWpu7ubuXzea1Zsybt0RAUIUPTXbx4UQcPHrzkOrKDBw/q4sWLaY+GoDi0RNNt2rRJ27Ztu+Q6sh07dujo0aNpj4ag2CND0xUKBQ0NDam/v19TU1Pq7+/X0NAQV/Zj2dgjQ9NxZT/qjcsvAITBlf0AVi1CBiA8QoZU8FlL1BMn+9F0fNYS9cbJfjRdLpdTf3//Jb9spFgsKp/P81lLLKrSyX5ChqbjO/uxXLxrieYyq/gn+8orGlmz5pJlI2vWKPvKK4s+Tvw2clRAyNAY7hX/FIaG1Hv99SoeO6ZpScVjx9R7/fUqDA0t+jg14OgBqwMn+9F0l1zZLymbz3NlPy4L58iQLjP2tFA1zpEBWLWqCpmZXWlmD5vZ181szMze3ujBAKBa1Z4ju0/SX7j7B81sjaR1DZwJAGqyZMjM7LWStki6XZLc/aIkvsoTwIpRzaHlGyVNSPqMmT1hZveb2fr5K5nZTjMbNbPRiYmJug8KAJVUE7JWST8p6Y/c/c2SJiX9+vyV3P2Qu3e5e1dHR0edxwSAyqoJ2bikcXf/WnL/YZXCBgArwpIhc/dvSnrezN6ULHqnpKcbOhUA1KDady3zko4k71h+Q9JHGjcSANSmqpC5+0lJr7qaFgBWAq7sBxAeIQMQHiEDEB4hAxAeIQMQHiEDEB4hAxAeIQMQHiEDEB4hAxAeIQMQHiEDEB4hAxAeIQMQHiEDEB4hAxAeIQMQHiEDEB4hAxAeIQMQHiEDEB4hAxAeIQMQHiEDEB4hAxAeIQMQHiEDEB4hAxAeIQMQHiEDEB4hAxAeIQMQHiEDEB4hAxAeIQMQHiEDEB4hAxAeIQMQHiEDEB4hAxAeIQMQHiEDEB4hAxAeIQMQHiEDEB4hAxAeIQMQHiEDEB4hAxAeIQMQHiEDEB4hAxAeIQMQHiEDEB4hAxAeIQMQHiEDEB4hAxAeIQMQHiEDEB4hAxAeIQMQHiEDEB4hAxAeIQMQHiEDEB4hAxAeIQMQHiEDEB4hAxAeIQMQHiEDEB4hAxAeIQMQHiEDEB4hAxAeIQMQHiEDEB4hAxAeIQMQHiEDEB4hAxAeIQMQHiEDEF5rNSuZ2bOSzkmakfSyu3c1cigAqEVVIUt0u/uLDZsEAJaJQ0sA4VUbMpf0l2Z2wsx2LrSCme00s1EzG52YmKjfhACwhGpDdpO7/6Sk90j6FTPbMn8Fdz/k7l3u3tXR0VHXIQFgMVWFzN3PJP/8tqQvSXprI4cCgFosGTIzW29mG2ZvS3qXpFONHgwAqlXNu5b/QtKXzGx2/SF3/4uGTgUANVgyZO7+DUk/0YRZAGBZuPwCQHiEDEB4hAxAeIQMQHiEDEB4hAxAeIQMQHiEDEB4hAxAeIQMQHiEDEB4hAxAeIQMQHiEDEB4hAxAeIQMQHiEDEB4hAxAeIQMQHiEDEB4hAxAeIQMQHiEDEB4hAxAeIQMQHiEDEB4hAxAeIQMQHiEDEB4hAxAeIQMQHiEDEB4hAxAeIQMQHiEDEB4hAxAeIQMQHiEDEB4hAxAeIQMQHiEDEB4rWkPgECuvlp66aX6b9esvtu76irp7Nn6bhMrGiFD9V56SXJPe4ql1TuMWPE4tAQQHiEDEB4hAxAeIQMQHiEDEB4hAxAeIQMQHiEDEB4hAxAeIQMQHiEDEB4hAxAeIQMQHiEDEB4hAxAeIQMQHiEDEB4hAxAeIQMQHiEDEB4hAxAeIQMQHiEDEB4hAxAeIQMQHiEDEB4hAxAeIQMQHiEDEB4hAxAeIQMQHiEDEB4hAxAeIQMQHiEDEB4hAxAeIQMQHiEDEB4hAxAeIQMQHiEDEB4hAxBe1SEzsxYze8LMvtzIgQCgVrXske2WNNaoQQBguaoKmZltlPReSfc3dhwAqF21e2SfkrRH0iuVVjCznWY2amajExMT9ZgNAKqyZMjM7D9I+ra7n1hsPXc/5O5d7t7V0dFRtwEBYCnV7JHdJOn9ZvaspP8uaauZfa6hUwFADZYMmbv/hrtvdPfrJH1I0jF3v7XhkwFAlVrTHgDBmKU9AfAqNYXM3Y9LOt6QSRCDe9oTLI3Y/sDhyn4A4REyAOERMgDhETIA4REyAOERMgDhETIA4REyAOERMgDhETIA4REyAOERMgDhETIA4REyAOERMgDhETIA4REyAOERMgDhETIA4REyAOERMgDhETIA4REyAOERMgDhETIA4REyAOERMgDhETIA4REyAOERMgDhETIA4REyAOERMgDhETIA4REyAOERMgDhETIA4REyAOERMgDhETIA4REyAOERMgDhETIA4REyAOERMgDhETIA4bWmPQCCMUt7gqVddVXaE6DJCBmq517/bZo1Zrv4gcKhJYDwCBmA8AgZgPAIGYDwCBmA8AgZgPAIGYDwCBmA8AgZgPAIGYDwCBmA8AgZgPAIGYDwCBmA8AgZgPAIGYDwCBmA8AgZgPAIGYDwCBmA8AgZgPAIGYDwCBmA8AgZgPAIGYDwCBmA8AgZgPAIGYDwCBmA8AgZgPAIGYDwCBmA8AgZgPAIGYDwCBmA8AgZgPCWDJmZZczsMTP7X2b2lJl9shmDAUC1WqtY5/uStrr7eTNrkzRiZl9x979v8GwAUJUlQ+buLul8crct+eONHAoAalHVOTIzazGzk5K+LelRd//aAuvsNLNRMxudmJio85gAUFlVIXP3GXe/UdJGSW81s9wC6xxy9y537+ro6KjzmABQWU3vWrr7dyUdl/TuRgwDAMtRzbuWHWZ2ZXJ7raSfkfT1Bs8FAFWr5l3L10t60MxaVArfQ+7+5caOBQDVq+ZdyyclvbkJswDAsnBlP4DwCBmA8AgZgPAIGYDwCBmA8AgZgPAIGYDwCBmA8AgZgPAIGYDwCBmA8AgZgPAIGYDwCBmA8AgZgPAIGYDwCBmA8AgZgPAIGYDwCBmA8AgZgPAIGYDwCBmA8AgZgPAIGYDwCBmA8AgZgPAIGYDwCBmA8AgZgPAIGYDwCBmA8AgZgPAIGYDwCBmA8AgZgPAIGYDwCBmA8AgZgPAIGYDwCBmA8AgZgPAIGYDwCBmA8AgZgPAIGYDwCBmA8AgZgPAIGYDwCBmA8AgZgPAIGYDwCBmA8AgZgPAIGYDwCBmA8AgZgPAIGYDwCBmA8AgZgPAIGYDwCBmA8AgZgPAIGYDwCBmA8AgZgPAIGYDwCBmA8AgZgPAIGYDwCBmA8AgZgPAIGYDwCBmA8AgZgPAIGYDwCBmA8AgZgPAIGYDwCBmA8AgZgPAIGYDwCBmA8AgZgPAIGYDwlgyZmb3BzIpmNmZmT5nZ7mYMBgDVaq1inZcl3eXuj5vZBkknzOxRd3+6wbMBQFWW3CNz9xfc/fHk9jlJY5J+uNGDAUC1ajpHZmbXSXqzpK8t8LOdZjZqZqMTExN1Gg8AllZ1yMzsCklfkPRxd/+n+T9390Pu3uXuXR0dHfWcEQAWVVXIzKxNpYgdcfcvNnYkAKhNNe9amqTDksbc/d7GjwQAtalmj+wmSb8kaauZnUz+/GyD5wKAqi15+YW7j0iyJswCAMvClf0AwiNkAMIjZADCI2QAwiNkAMIjZEjF8PCwcrmcWiTlcjkNDw+nPRICq+bbL4DaWeUrdoYlFVS6ynqzpJGnnlLvjh3Sjh3avtR23es2IlYP9sjQGO4V/+y/4QYdPnZM3e5qc1e3uw4fO6b9N9yw6OOIGCohZGi6sbExjY+Plw4tW1qUy+U0Pj6usbGxtEdDUIQMTdfZ2al8Pq/JyUlJ0uTkpPL5vDo7O1OeDFERMjTdhQsXdP78eeXzeZ07d075fF7nz5/XhQsX0h4NQREyNN3Zs2d19913a3BwUBs2bNDg4KDuvvtunT17Nu3REBQhQyq2bt2qU6dOaWZmRqdOndLWrVvTHgmBETI03caNG3XbbbepWCxqenpaxWJRt912mzZu3Jj2aAiKkKHp7rnnHs3MzKinp0ft7e3q6enRzMyM7rnnnrRHQ1CEDE23fft23XfffVq/fr3MTOvXr9d9992n7duXvBwWWJB5Ay4y7Orq8tHR0bpvF8APNjM74e5d85ezRwYgPEIGIDxCBiA8QoZUzH2NT/JZS77GB5eDkKHphoeHtXv3bk1OTsrdNTk5qd27dxMzLBshQ9Pt2bNHFy9elCRZ8r1lFy9e1J49e9IcC4ERMjTd+Pi41q5dq8HBQU1NTWlwcFBr167V+Ph42qMhKEKGVHR3dyufzyuTySifz6u7uzvtkRAYIUMqHnroIfX09OjcuXPq6enRQw89lPZICIyQoelaW1uVyWTU39+vDRs2qL+/X5lMRq2t/AoJLA8hQ9PNzMxo7dq1kqTZj8itXbtWMzMzaY6FwAgZmm7Tpk264447LvnQ+B133KFNmzalPRqCImRoukKhoKGhIfX392tqakr9/f0aGhpSoVBIezQExUkJNN3s1/Xk83mNjY0pm81q//79fI0Plo2v8QEQBl/jA2DVImQAwiNkAMIjZADCI2QAwiNkAMIjZEjF7DdfmNncN2AAy0XI0HT5fF4DAwPq6+vT5OSk+vr6NDAwQMywbFwQi6bLZDLq6+vTnXfeObfs3nvv1b59+zQ1NZXiZFjpKl0QS8jQdGamyclJrVu3bm7ZhQsXtH79ejXiv0esHpVCxmct0XTt7e3auXOnTp48OfdZyxtvvFHt7e1pj4agOEeGprv55pt15MgRbdmyRWfPntWWLVt05MgR3XzzzWmPhqAIGZru9OnT2rZtmwYHB3XllVdqcHBQ27Zt0+nTp9MeDUFxaImmGxsb0xNPPKG2tra5ZdPT08pkMilOhcjYI0PTZbNZjYyMXLJsZGRE2Ww2pYkQHSFD0xUKBfX29qpYLGp6elrFYlG9vb18QyyWjUNLNB3fEIt64zoyAGHwDbEAVi1CBiA8QgYgPEIGIDxCBiA8QgYgPEIGIDxCBiA8QgYgPEIGIDxCBiA8QgYgPEIGIDxCBiA8QgYgPEIGIDxCBiA8QgYgPEIGIDxCBiC8hvzyETObkPRc3TeM1egaSS+mPQTC+BF375i/sCEhA6plZqML/VYcoBYcWgIIj5ABCI+QIW2H0h4A8XGODEB47JEBCI+QAQiPkKEqZvY6MzuZ/PmmmZ0uu79micdeZ2anKvzsfjPbtMDy283sYHJ7l5ndVra8sx6vCatHa9oDIAZ3/46kGyXJzD4h6by7//5SjzOzliW2+9Eqnnug7O7tkk5JOrPU4/CDgz0yLJuZPWBmHyy7fz755zvMrGhmQ5L+d/LjVjN70MyeNLOHzWxdsu5xM+tKbn/EzP6Pmf21pJvKtvsJM/u15Lm6JB1J9gTfa2ZfKlvvFjP7YsNfOFYcQoZGeaukgrvPHja+SdIhd/9xSf8k6T+Wr2xmr5f0SZUCdoukVx1uuvvDkkYlfdjdb5T055KyZjb7kZWPSPpM/V8KVjpChkZ5zN3/sez+8+7+1eT25yRtnrf+2yQdd/cJd78o6fNLPYGXrh36rKRbzexKSW+X9JXLnhzhcI4Ml+NlJf9naGYmqfyk/+S8dedfsLjQBYzLuajxM5IekTQl6U/c/eVlbAPBsUeGy/GspLcktz8gqW2Rda81s7cnt7dLGpn3869Jekfy7mibpJ+vsJ1zkjbM3nH3Myqd+P9NSQ/UMjxWD0KGy/HHkm42s8dUOjScvxdWbkzSL5vZk5KulvRH5T909xckfULS/5D0V5Ier7CdByQNJCf71ybLjqh06Pr0Ml8HguMjSggvud7sCXc/nPYsSAchQ2hmdkKlPcFb3P37ac+DdBAyAOFxjgxAeIQMQHiEDEB4hAxAeIQMQHj/DypUjfiKTMSkAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 360x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(waterquality['Turbidity'].quantile([0.25, 0.5, 0.75]))\n",
    "waterquality['Turbidity'].plot(kind='box', title='Turbidity distribution',color='r', figsize=(5,10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da98d048",
   "metadata": {},
   "source": [
    "Convert the outliers for all above variables into NaN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "a709c04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference : https://www.journaldev.com/51466/python-catboost-classifier-module\n",
    "\n",
    "for x in ['Hardness']:\n",
    "    q85,q15 = np.percentile(waterquality.loc[:,x],[85,15])\n",
    "    intr_qr = q85-q15\n",
    "    maximum = q85+(1.5*intr_qr)\n",
    "    minimum = q15-(1.5*intr_qr)\n",
    "    waterquality.loc[waterquality[x] < minimum,x] = np.nan\n",
    "    waterquality.loc[waterquality[x] > maximum,x] = np.nan\n",
    "    \n",
    "for x in ['Chloramines']:\n",
    "    q85,q15 = np.percentile(waterquality.loc[:,x],[85,15])\n",
    "    intr_qr = q85-q15\n",
    "    maximum = q85+(1.5*intr_qr)\n",
    "    minimum = q15-(1.5*intr_qr)\n",
    "    waterquality.loc[waterquality[x] < minimum,x] = np.nan\n",
    "    waterquality.loc[waterquality[x] > maximum,x] = np.nan\n",
    "       \n",
    "for x in ['Conductivity']:\n",
    "    q85,q15 = np.percentile(waterquality.loc[:,x],[85,15])\n",
    "    intr_qr = q85-q15\n",
    "    maximum = q85+(1.5*intr_qr)\n",
    "    minimum = q15-(1.5*intr_qr)\n",
    "    waterquality.loc[waterquality[x] < minimum,x] = np.nan\n",
    "    waterquality.loc[waterquality[x] > maximum,x] = np.nan\n",
    "\n",
    "for x in ['Organic_carbon']:\n",
    "    q85,q15 = np.percentile(waterquality.loc[:,x],[85,15])\n",
    "    intr_qr = q85-q15\n",
    "    maximum = q85+(1.5*intr_qr)\n",
    "    minimum = q15-(1.5*intr_qr)\n",
    "    waterquality.loc[waterquality[x] < minimum,x] = np.nan\n",
    "    waterquality.loc[waterquality[x] > maximum,x] = np.nan\n",
    "    \n",
    "for x in ['Trihalomethanes']:\n",
    "    q85,q15 = np.percentile(waterquality.loc[:,x],[85,15])\n",
    "    intr_qr = q85-q15\n",
    "    maximum = q85+(1.5*intr_qr)\n",
    "    minimum = q15-(1.5*intr_qr)\n",
    "    waterquality.loc[waterquality[x] < minimum,x] = np.nan\n",
    "    waterquality.loc[waterquality[x] > maximum,x] = np.nan\n",
    "    \n",
    "for x in ['Turbidity']:\n",
    "    q85,q15 = np.percentile(waterquality.loc[:,x],[85,15])\n",
    "    intr_qr = q85-q15\n",
    "    maximum = q85+(1.5*intr_qr)\n",
    "    minimum = q15-(1.5*intr_qr)\n",
    "    waterquality.loc[waterquality[x] < minimum,x] = np.nan\n",
    "    waterquality.loc[waterquality[x] > maximum,x] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89feeca",
   "metadata": {},
   "source": [
    "Since we have converted the outliers into NaN values, this will have to be dealt with as previously done before.\n",
    "\n",
    "Again we check, this time just to ensure we do have NaN values present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "095b50d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pH                 0\n",
       "Hardness           1\n",
       "Solids             0\n",
       "Chloramines        2\n",
       "Sulfates           0\n",
       "Conductivity       0\n",
       "Organic_carbon     1\n",
       "Trihalomethanes    1\n",
       "Turbidity          0\n",
       "Potability         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "waterquality.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88bcd98",
   "metadata": {},
   "source": [
    "Replace the NaN values with the corresponding column's mean.\n",
    "\n",
    "Call for the display of NaN values to ensure the dataframe has been cleaned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "3f7c60f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pH                 0\n",
       "Hardness           0\n",
       "Solids             0\n",
       "Chloramines        0\n",
       "Sulfates           0\n",
       "Conductivity       0\n",
       "Organic_carbon     0\n",
       "Trihalomethanes    0\n",
       "Turbidity          0\n",
       "Potability         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "waterquality['Hardness'] = waterquality['Hardness'].fillna(waterquality['Hardness'].mean())\n",
    "waterquality['Chloramines'] = waterquality['Chloramines'].fillna(waterquality['Chloramines'].mean())\n",
    "waterquality['Conductivity'] = waterquality['Conductivity'].fillna(waterquality['Conductivity'].mean())\n",
    "waterquality['Organic_carbon'] = waterquality['Organic_carbon'].fillna(waterquality['Organic_carbon'].mean())\n",
    "waterquality['Trihalomethanes'] = waterquality['Trihalomethanes'].fillna(waterquality['Trihalomethanes'].mean())\n",
    "waterquality['Turbidity'] = waterquality['Turbidity'].fillna(waterquality['Turbidity'].mean())\n",
    "\n",
    "waterquality.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "c424b01f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 12 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "waterquality.hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76644eb",
   "metadata": {},
   "source": [
    "### Chapter 4: Modelling the data <a class=\"anchor\" id=\"chapter04\"></a>\n",
    "\n",
    "### 4.1. EDA <a class=\"anchor\" id=\"section_4_1\"></a>\n",
    "\n",
    "Characterise the data, and show important attributes of the dataframe using descriptive statistics.\n",
    "\n",
    "##### 4.1.1. Central tendencies <a class=\"anchor\" id=\"section_4_1_1\"></a>\n",
    "\n",
    "Create a table displaying the central tendencies of the dataframe i.e. mean, standard deviation, minimum values etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "66cc1e66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pH</th>\n",
       "      <th>Hardness</th>\n",
       "      <th>Solids</th>\n",
       "      <th>Chloramines</th>\n",
       "      <th>Sulfates</th>\n",
       "      <th>Conductivity</th>\n",
       "      <th>Organic_carbon</th>\n",
       "      <th>Trihalomethanes</th>\n",
       "      <th>Turbidity</th>\n",
       "      <th>Potability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3276.000000</td>\n",
       "      <td>3276.000000</td>\n",
       "      <td>3276.000000</td>\n",
       "      <td>3276.000000</td>\n",
       "      <td>3276.000000</td>\n",
       "      <td>3276.000000</td>\n",
       "      <td>3276.000000</td>\n",
       "      <td>3276.000000</td>\n",
       "      <td>3276.000000</td>\n",
       "      <td>3276.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>7.080795</td>\n",
       "      <td>196.414973</td>\n",
       "      <td>22014.092526</td>\n",
       "      <td>7.126358</td>\n",
       "      <td>333.775777</td>\n",
       "      <td>426.205111</td>\n",
       "      <td>14.280691</td>\n",
       "      <td>66.416341</td>\n",
       "      <td>3.966786</td>\n",
       "      <td>0.390110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.469956</td>\n",
       "      <td>32.776568</td>\n",
       "      <td>8768.570828</td>\n",
       "      <td>1.574445</td>\n",
       "      <td>36.142612</td>\n",
       "      <td>80.824064</td>\n",
       "      <td>3.299082</td>\n",
       "      <td>15.728077</td>\n",
       "      <td>0.780382</td>\n",
       "      <td>0.487849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>73.492234</td>\n",
       "      <td>320.942611</td>\n",
       "      <td>1.390871</td>\n",
       "      <td>129.000000</td>\n",
       "      <td>181.483754</td>\n",
       "      <td>2.200000</td>\n",
       "      <td>8.175876</td>\n",
       "      <td>1.450000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>6.277673</td>\n",
       "      <td>176.856266</td>\n",
       "      <td>15666.690300</td>\n",
       "      <td>6.130361</td>\n",
       "      <td>317.094638</td>\n",
       "      <td>365.734414</td>\n",
       "      <td>12.065801</td>\n",
       "      <td>56.696169</td>\n",
       "      <td>3.439711</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>7.080795</td>\n",
       "      <td>196.967627</td>\n",
       "      <td>20927.833605</td>\n",
       "      <td>7.130299</td>\n",
       "      <td>333.775777</td>\n",
       "      <td>421.884968</td>\n",
       "      <td>14.218338</td>\n",
       "      <td>66.396293</td>\n",
       "      <td>3.955028</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>7.870050</td>\n",
       "      <td>216.667456</td>\n",
       "      <td>27332.762125</td>\n",
       "      <td>8.114887</td>\n",
       "      <td>350.385756</td>\n",
       "      <td>481.792305</td>\n",
       "      <td>16.553701</td>\n",
       "      <td>76.666609</td>\n",
       "      <td>4.500320</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>14.000000</td>\n",
       "      <td>323.124000</td>\n",
       "      <td>61227.196010</td>\n",
       "      <td>13.127000</td>\n",
       "      <td>481.030642</td>\n",
       "      <td>753.342620</td>\n",
       "      <td>27.006707</td>\n",
       "      <td>124.000000</td>\n",
       "      <td>6.739000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                pH     Hardness        Solids  Chloramines     Sulfates  \\\n",
       "count  3276.000000  3276.000000   3276.000000  3276.000000  3276.000000   \n",
       "mean      7.080795   196.414973  22014.092526     7.126358   333.775777   \n",
       "std       1.469956    32.776568   8768.570828     1.574445    36.142612   \n",
       "min       0.000000    73.492234    320.942611     1.390871   129.000000   \n",
       "25%       6.277673   176.856266  15666.690300     6.130361   317.094638   \n",
       "50%       7.080795   196.967627  20927.833605     7.130299   333.775777   \n",
       "75%       7.870050   216.667456  27332.762125     8.114887   350.385756   \n",
       "max      14.000000   323.124000  61227.196010    13.127000   481.030642   \n",
       "\n",
       "       Conductivity  Organic_carbon  Trihalomethanes    Turbidity   Potability  \n",
       "count   3276.000000     3276.000000      3276.000000  3276.000000  3276.000000  \n",
       "mean     426.205111       14.280691        66.416341     3.966786     0.390110  \n",
       "std       80.824064        3.299082        15.728077     0.780382     0.487849  \n",
       "min      181.483754        2.200000         8.175876     1.450000     0.000000  \n",
       "25%      365.734414       12.065801        56.696169     3.439711     0.000000  \n",
       "50%      421.884968       14.218338        66.396293     3.955028     0.000000  \n",
       "75%      481.792305       16.553701        76.666609     4.500320     1.000000  \n",
       "max      753.342620       27.006707       124.000000     6.739000     1.000000  "
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "waterquality.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c14fe7",
   "metadata": {},
   "source": [
    "##### 4.1.2. Correlation plot <a class=\"anchor\" id=\"section_4_1_2\"></a>\n",
    "\n",
    "Conduct a correlation plot to ensure that multicollinearity does not exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "c35b6e16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize = (8,5))\n",
    "waterquality_correlation = waterquality.corr()\n",
    "axis = sns.heatmap(waterquality_correlation, cmap='Blues')\n",
    "axis.set_title(\"Pearson's Correlation Heatmap\")\n",
    "plt.show()\n",
    "plt.savefig('correlationmap.jpeg', bbox_inches = 'tight') # Reference: https://www.tutorialspoint.com/save-figure-as-file-from-ipython-notebook-using-matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "57108b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = waterquality[['pH','Hardness','Solids','Chloramines','Sulfates','Conductivity','Organic_carbon','Trihalomethanes','Turbidity']]\n",
    "y = waterquality[['Potability']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "5ab7d82d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>       <td>Potability</td>    <th>  R-squared (uncentered):</th>      <td>   0.390</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared (uncentered):</th> <td>   0.389</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th>          <td>   232.4</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Sun, 08 May 2022</td> <th>  Prob (F-statistic):</th>           <td>  0.00</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>15:31:29</td>     <th>  Log-Likelihood:    </th>          <td> -2296.1</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  3276</td>      <th>  AIC:               </th>          <td>   4610.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  3267</td>      <th>  BIC:               </th>          <td>   4665.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     9</td>      <th>                     </th>              <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>              <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "         <td></td>            <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>pH</th>              <td>    0.0058</td> <td>    0.006</td> <td>    1.031</td> <td> 0.303</td> <td>   -0.005</td> <td>    0.017</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Hardness</th>        <td>    0.0002</td> <td>    0.000</td> <td>    0.706</td> <td> 0.480</td> <td>   -0.000</td> <td>    0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Solids</th>          <td> 2.755e-06</td> <td> 9.51e-07</td> <td>    2.898</td> <td> 0.004</td> <td> 8.91e-07</td> <td> 4.62e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Chloramines</th>     <td>    0.0140</td> <td>    0.005</td> <td>    2.701</td> <td> 0.007</td> <td>    0.004</td> <td>    0.024</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Sulfates</th>        <td>    0.0002</td> <td>    0.000</td> <td>    1.225</td> <td> 0.221</td> <td>   -0.000</td> <td>    0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Conductivity</th>    <td> 5.664e-05</td> <td>    0.000</td> <td>    0.565</td> <td> 0.572</td> <td>   -0.000</td> <td>    0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Organic_carbon</th>  <td>   -0.0023</td> <td>    0.003</td> <td>   -0.919</td> <td> 0.358</td> <td>   -0.007</td> <td>    0.003</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Trihalomethanes</th> <td>    0.0006</td> <td>    0.001</td> <td>    1.179</td> <td> 0.238</td> <td>   -0.000</td> <td>    0.002</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Turbidity</th>       <td>    0.0110</td> <td>    0.010</td> <td>    1.060</td> <td> 0.289</td> <td>   -0.009</td> <td>    0.031</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>13598.076</td> <th>  Durbin-Watson:     </th> <td>   0.031</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th>  <td> 0.000</td>   <th>  Jarque-Bera (JB):  </th> <td> 540.441</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>           <td> 0.448</td>   <th>  Prob(JB):          </th> <td>4.41e-118</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>       <td> 1.223</td>   <th>  Cond. No.          </th> <td>2.89e+04</td> \n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] R² is computed without centering (uncentered) since the model does not contain a constant.<br/>[2] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[3] The condition number is large, 2.89e+04. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                                 OLS Regression Results                                \n",
       "=======================================================================================\n",
       "Dep. Variable:             Potability   R-squared (uncentered):                   0.390\n",
       "Model:                            OLS   Adj. R-squared (uncentered):              0.389\n",
       "Method:                 Least Squares   F-statistic:                              232.4\n",
       "Date:                Sun, 08 May 2022   Prob (F-statistic):                        0.00\n",
       "Time:                        15:31:29   Log-Likelihood:                         -2296.1\n",
       "No. Observations:                3276   AIC:                                      4610.\n",
       "Df Residuals:                    3267   BIC:                                      4665.\n",
       "Df Model:                           9                                                  \n",
       "Covariance Type:            nonrobust                                                  \n",
       "===================================================================================\n",
       "                      coef    std err          t      P>|t|      [0.025      0.975]\n",
       "-----------------------------------------------------------------------------------\n",
       "pH                  0.0058      0.006      1.031      0.303      -0.005       0.017\n",
       "Hardness            0.0002      0.000      0.706      0.480      -0.000       0.001\n",
       "Solids           2.755e-06   9.51e-07      2.898      0.004    8.91e-07    4.62e-06\n",
       "Chloramines         0.0140      0.005      2.701      0.007       0.004       0.024\n",
       "Sulfates            0.0002      0.000      1.225      0.221      -0.000       0.001\n",
       "Conductivity     5.664e-05      0.000      0.565      0.572      -0.000       0.000\n",
       "Organic_carbon     -0.0023      0.003     -0.919      0.358      -0.007       0.003\n",
       "Trihalomethanes     0.0006      0.001      1.179      0.238      -0.000       0.002\n",
       "Turbidity           0.0110      0.010      1.060      0.289      -0.009       0.031\n",
       "==============================================================================\n",
       "Omnibus:                    13598.076   Durbin-Watson:                   0.031\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              540.441\n",
       "Skew:                           0.448   Prob(JB):                    4.41e-118\n",
       "Kurtosis:                       1.223   Cond. No.                     2.89e+04\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] R² is computed without centering (uncentered) since the model does not contain a constant.\n",
       "[2] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[3] The condition number is large, 2.89e+04. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg = sm.OLS(y, X).fit()\n",
    "reg.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e468ec",
   "metadata": {},
   "source": [
    "No multicollinearity exists between observations - multiple models do not have to be created."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc46e1a",
   "metadata": {},
   "source": [
    "##### 4.1.3. 5 rows <a class=\"anchor\" id=\"section_4_1_3\"></a>\n",
    "\n",
    "Print the first 5 rows and the last 5 rows of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "0a6f622b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pH</th>\n",
       "      <th>Hardness</th>\n",
       "      <th>Solids</th>\n",
       "      <th>Chloramines</th>\n",
       "      <th>Sulfates</th>\n",
       "      <th>Conductivity</th>\n",
       "      <th>Organic_carbon</th>\n",
       "      <th>Trihalomethanes</th>\n",
       "      <th>Turbidity</th>\n",
       "      <th>Potability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.080795</td>\n",
       "      <td>204.890456</td>\n",
       "      <td>20791.31898</td>\n",
       "      <td>7.300212</td>\n",
       "      <td>368.516441</td>\n",
       "      <td>564.308654</td>\n",
       "      <td>10.379783</td>\n",
       "      <td>86.990970</td>\n",
       "      <td>2.963135</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.716080</td>\n",
       "      <td>129.422921</td>\n",
       "      <td>18630.05786</td>\n",
       "      <td>6.635246</td>\n",
       "      <td>333.775777</td>\n",
       "      <td>592.885359</td>\n",
       "      <td>15.180013</td>\n",
       "      <td>56.329076</td>\n",
       "      <td>4.500656</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.099124</td>\n",
       "      <td>224.236259</td>\n",
       "      <td>19909.54173</td>\n",
       "      <td>9.275884</td>\n",
       "      <td>333.775777</td>\n",
       "      <td>418.606213</td>\n",
       "      <td>16.868637</td>\n",
       "      <td>66.420093</td>\n",
       "      <td>3.055934</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8.316766</td>\n",
       "      <td>214.373394</td>\n",
       "      <td>22018.41744</td>\n",
       "      <td>8.059332</td>\n",
       "      <td>356.886136</td>\n",
       "      <td>363.266516</td>\n",
       "      <td>18.436525</td>\n",
       "      <td>100.341674</td>\n",
       "      <td>4.628771</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9.092223</td>\n",
       "      <td>181.101509</td>\n",
       "      <td>17978.98634</td>\n",
       "      <td>6.546600</td>\n",
       "      <td>310.135738</td>\n",
       "      <td>398.410813</td>\n",
       "      <td>11.558279</td>\n",
       "      <td>31.997993</td>\n",
       "      <td>4.075075</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         pH    Hardness       Solids  Chloramines    Sulfates  Conductivity  \\\n",
       "0  7.080795  204.890456  20791.31898     7.300212  368.516441    564.308654   \n",
       "1  3.716080  129.422921  18630.05786     6.635246  333.775777    592.885359   \n",
       "2  8.099124  224.236259  19909.54173     9.275884  333.775777    418.606213   \n",
       "3  8.316766  214.373394  22018.41744     8.059332  356.886136    363.266516   \n",
       "4  9.092223  181.101509  17978.98634     6.546600  310.135738    398.410813   \n",
       "\n",
       "   Organic_carbon  Trihalomethanes  Turbidity  Potability  \n",
       "0       10.379783        86.990970   2.963135           0  \n",
       "1       15.180013        56.329076   4.500656           0  \n",
       "2       16.868637        66.420093   3.055934           0  \n",
       "3       18.436525       100.341674   4.628771           0  \n",
       "4       11.558279        31.997993   4.075075           0  "
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "waterquality.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "0dca06a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pH</th>\n",
       "      <th>Hardness</th>\n",
       "      <th>Solids</th>\n",
       "      <th>Chloramines</th>\n",
       "      <th>Sulfates</th>\n",
       "      <th>Conductivity</th>\n",
       "      <th>Organic_carbon</th>\n",
       "      <th>Trihalomethanes</th>\n",
       "      <th>Turbidity</th>\n",
       "      <th>Potability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3271</th>\n",
       "      <td>4.668102</td>\n",
       "      <td>193.681736</td>\n",
       "      <td>47580.99160</td>\n",
       "      <td>7.166639</td>\n",
       "      <td>359.948574</td>\n",
       "      <td>526.424171</td>\n",
       "      <td>13.894419</td>\n",
       "      <td>66.687695</td>\n",
       "      <td>4.435821</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3272</th>\n",
       "      <td>7.808856</td>\n",
       "      <td>193.553212</td>\n",
       "      <td>17329.80216</td>\n",
       "      <td>8.061362</td>\n",
       "      <td>333.775777</td>\n",
       "      <td>392.449580</td>\n",
       "      <td>19.903225</td>\n",
       "      <td>66.396293</td>\n",
       "      <td>2.798243</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3273</th>\n",
       "      <td>9.419510</td>\n",
       "      <td>175.762646</td>\n",
       "      <td>33155.57822</td>\n",
       "      <td>7.350233</td>\n",
       "      <td>333.775777</td>\n",
       "      <td>432.044783</td>\n",
       "      <td>11.039070</td>\n",
       "      <td>69.845400</td>\n",
       "      <td>3.298875</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3274</th>\n",
       "      <td>5.126763</td>\n",
       "      <td>230.603758</td>\n",
       "      <td>11983.86938</td>\n",
       "      <td>6.303357</td>\n",
       "      <td>333.775777</td>\n",
       "      <td>402.883113</td>\n",
       "      <td>11.168946</td>\n",
       "      <td>77.488213</td>\n",
       "      <td>4.708658</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3275</th>\n",
       "      <td>7.874671</td>\n",
       "      <td>195.102299</td>\n",
       "      <td>17404.17706</td>\n",
       "      <td>7.509306</td>\n",
       "      <td>333.775777</td>\n",
       "      <td>327.459761</td>\n",
       "      <td>16.140368</td>\n",
       "      <td>78.698446</td>\n",
       "      <td>2.309149</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            pH    Hardness       Solids  Chloramines    Sulfates  \\\n",
       "3271  4.668102  193.681736  47580.99160     7.166639  359.948574   \n",
       "3272  7.808856  193.553212  17329.80216     8.061362  333.775777   \n",
       "3273  9.419510  175.762646  33155.57822     7.350233  333.775777   \n",
       "3274  5.126763  230.603758  11983.86938     6.303357  333.775777   \n",
       "3275  7.874671  195.102299  17404.17706     7.509306  333.775777   \n",
       "\n",
       "      Conductivity  Organic_carbon  Trihalomethanes  Turbidity  Potability  \n",
       "3271    526.424171       13.894419        66.687695   4.435821           1  \n",
       "3272    392.449580       19.903225        66.396293   2.798243           1  \n",
       "3273    432.044783       11.039070        69.845400   3.298875           1  \n",
       "3274    402.883113       11.168946        77.488213   4.708658           1  \n",
       "3275    327.459761       16.140368        78.698446   2.309149           1  "
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "waterquality.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ccbd75c",
   "metadata": {},
   "source": [
    "##### 4.1.4. Sampling the data <a class=\"anchor\" id=\"section_4_1_4\"></a>\n",
    "\n",
    "\n",
    "Take a sample of the data from 10 random rows to understand the characteristics of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "14eadbf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pH</th>\n",
       "      <th>Hardness</th>\n",
       "      <th>Solids</th>\n",
       "      <th>Chloramines</th>\n",
       "      <th>Sulfates</th>\n",
       "      <th>Conductivity</th>\n",
       "      <th>Organic_carbon</th>\n",
       "      <th>Trihalomethanes</th>\n",
       "      <th>Turbidity</th>\n",
       "      <th>Potability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2114</th>\n",
       "      <td>7.910641</td>\n",
       "      <td>190.427600</td>\n",
       "      <td>16461.50232</td>\n",
       "      <td>5.454476</td>\n",
       "      <td>394.305876</td>\n",
       "      <td>294.178639</td>\n",
       "      <td>17.658652</td>\n",
       "      <td>82.417022</td>\n",
       "      <td>3.886603</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017</th>\n",
       "      <td>8.111953</td>\n",
       "      <td>217.266472</td>\n",
       "      <td>38184.46957</td>\n",
       "      <td>7.254122</td>\n",
       "      <td>311.910224</td>\n",
       "      <td>281.069203</td>\n",
       "      <td>13.027921</td>\n",
       "      <td>78.582094</td>\n",
       "      <td>4.430750</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2071</th>\n",
       "      <td>7.341547</td>\n",
       "      <td>187.672402</td>\n",
       "      <td>21273.45707</td>\n",
       "      <td>7.784003</td>\n",
       "      <td>333.775777</td>\n",
       "      <td>332.084293</td>\n",
       "      <td>16.842334</td>\n",
       "      <td>55.019151</td>\n",
       "      <td>4.025644</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2173</th>\n",
       "      <td>7.080795</td>\n",
       "      <td>204.258263</td>\n",
       "      <td>26321.68919</td>\n",
       "      <td>5.937496</td>\n",
       "      <td>333.775777</td>\n",
       "      <td>487.273628</td>\n",
       "      <td>20.407809</td>\n",
       "      <td>70.340601</td>\n",
       "      <td>3.305588</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1132</th>\n",
       "      <td>7.058183</td>\n",
       "      <td>187.947191</td>\n",
       "      <td>26608.92915</td>\n",
       "      <td>8.556396</td>\n",
       "      <td>405.403423</td>\n",
       "      <td>462.314567</td>\n",
       "      <td>18.428387</td>\n",
       "      <td>41.290330</td>\n",
       "      <td>3.613835</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1886</th>\n",
       "      <td>4.293961</td>\n",
       "      <td>192.360737</td>\n",
       "      <td>24600.76069</td>\n",
       "      <td>8.261635</td>\n",
       "      <td>261.357041</td>\n",
       "      <td>475.876415</td>\n",
       "      <td>10.799428</td>\n",
       "      <td>53.619847</td>\n",
       "      <td>5.105121</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3237</th>\n",
       "      <td>9.609834</td>\n",
       "      <td>227.366949</td>\n",
       "      <td>22690.04587</td>\n",
       "      <td>5.385440</td>\n",
       "      <td>314.317481</td>\n",
       "      <td>394.731563</td>\n",
       "      <td>16.988356</td>\n",
       "      <td>75.339694</td>\n",
       "      <td>4.308358</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1468</th>\n",
       "      <td>6.279842</td>\n",
       "      <td>175.890640</td>\n",
       "      <td>11582.50525</td>\n",
       "      <td>7.059985</td>\n",
       "      <td>333.775777</td>\n",
       "      <td>430.322092</td>\n",
       "      <td>15.674785</td>\n",
       "      <td>86.760257</td>\n",
       "      <td>3.278584</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2252</th>\n",
       "      <td>6.867522</td>\n",
       "      <td>182.672169</td>\n",
       "      <td>20247.20103</td>\n",
       "      <td>6.867073</td>\n",
       "      <td>330.805652</td>\n",
       "      <td>420.333457</td>\n",
       "      <td>17.952822</td>\n",
       "      <td>60.068800</td>\n",
       "      <td>4.837268</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>6.271012</td>\n",
       "      <td>213.444685</td>\n",
       "      <td>24846.14187</td>\n",
       "      <td>8.372997</td>\n",
       "      <td>333.775777</td>\n",
       "      <td>543.391744</td>\n",
       "      <td>17.916526</td>\n",
       "      <td>84.578869</td>\n",
       "      <td>5.303581</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            pH    Hardness       Solids  Chloramines    Sulfates  \\\n",
       "2114  7.910641  190.427600  16461.50232     5.454476  394.305876   \n",
       "2017  8.111953  217.266472  38184.46957     7.254122  311.910224   \n",
       "2071  7.341547  187.672402  21273.45707     7.784003  333.775777   \n",
       "2173  7.080795  204.258263  26321.68919     5.937496  333.775777   \n",
       "1132  7.058183  187.947191  26608.92915     8.556396  405.403423   \n",
       "1886  4.293961  192.360737  24600.76069     8.261635  261.357041   \n",
       "3237  9.609834  227.366949  22690.04587     5.385440  314.317481   \n",
       "1468  6.279842  175.890640  11582.50525     7.059985  333.775777   \n",
       "2252  6.867522  182.672169  20247.20103     6.867073  330.805652   \n",
       "217   6.271012  213.444685  24846.14187     8.372997  333.775777   \n",
       "\n",
       "      Conductivity  Organic_carbon  Trihalomethanes  Turbidity  Potability  \n",
       "2114    294.178639       17.658652        82.417022   3.886603           0  \n",
       "2017    281.069203       13.027921        78.582094   4.430750           1  \n",
       "2071    332.084293       16.842334        55.019151   4.025644           0  \n",
       "2173    487.273628       20.407809        70.340601   3.305588           0  \n",
       "1132    462.314567       18.428387        41.290330   3.613835           1  \n",
       "1886    475.876415       10.799428        53.619847   5.105121           0  \n",
       "3237    394.731563       16.988356        75.339694   4.308358           1  \n",
       "1468    430.322092       15.674785        86.760257   3.278584           0  \n",
       "2252    420.333457       17.952822        60.068800   4.837268           0  \n",
       "217     543.391744       17.916526        84.578869   5.303581           0  "
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "waterquality.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb7e1e8",
   "metadata": {},
   "source": [
    "### 4.2. Model construction <a class=\"anchor\" id=\"section_4_2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82e105b",
   "metadata": {},
   "source": [
    "##### 4.2.1. Shuffling the data <a class=\"anchor\" id=\"section_4_2_1\"></a>\n",
    "\n",
    "Here, we shuffle the data to reduce the chances of overfitting and prevent familiarity within the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "369e6704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " UNSHUFFLED DATAFRAME:\n",
      "            pH    Hardness       Solids  Chloramines    Sulfates  \\\n",
      "0     7.080795  204.890456  20791.31898     7.300212  368.516441   \n",
      "1     3.716080  129.422921  18630.05786     6.635246  333.775777   \n",
      "2     8.099124  224.236259  19909.54173     9.275884  333.775777   \n",
      "3     8.316766  214.373394  22018.41744     8.059332  356.886136   \n",
      "4     9.092223  181.101509  17978.98634     6.546600  310.135738   \n",
      "...        ...         ...          ...          ...         ...   \n",
      "3271  4.668102  193.681736  47580.99160     7.166639  359.948574   \n",
      "3272  7.808856  193.553212  17329.80216     8.061362  333.775777   \n",
      "3273  9.419510  175.762646  33155.57822     7.350233  333.775777   \n",
      "3274  5.126763  230.603758  11983.86938     6.303357  333.775777   \n",
      "3275  7.874671  195.102299  17404.17706     7.509306  333.775777   \n",
      "\n",
      "      Conductivity  Organic_carbon  Trihalomethanes  Turbidity  Potability  \n",
      "0       564.308654       10.379783        86.990970   2.963135           0  \n",
      "1       592.885359       15.180013        56.329076   4.500656           0  \n",
      "2       418.606213       16.868637        66.420093   3.055934           0  \n",
      "3       363.266516       18.436525       100.341674   4.628771           0  \n",
      "4       398.410813       11.558279        31.997993   4.075075           0  \n",
      "...            ...             ...              ...        ...         ...  \n",
      "3271    526.424171       13.894419        66.687695   4.435821           1  \n",
      "3272    392.449580       19.903225        66.396293   2.798243           1  \n",
      "3273    432.044783       11.039070        69.845400   3.298875           1  \n",
      "3274    402.883113       11.168946        77.488213   4.708658           1  \n",
      "3275    327.459761       16.140368        78.698446   2.309149           1  \n",
      "\n",
      "[3276 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "# Reference: https://www.geeksforgeeks.org/pandas-how-to-shuffle-a-dataframe-rows/amp/\n",
    "\n",
    "print(\"\\n UNSHUFFLED DATAFRAME:\")\n",
    "print(waterquality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "a5f94124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " SHUFFLED DATAFRAME:\n",
      "            pH    Hardness        Solids  Chloramines    Sulfates  \\\n",
      "1818  7.277144  194.880861  18270.105060     6.121931  344.879754   \n",
      "1681  7.460148  221.343210  25480.918410     6.926943  333.775777   \n",
      "1357  7.893132  201.433250  20526.049710     5.628776  299.018236   \n",
      "244   6.321259  207.257710   8532.139517     5.987877  286.489280   \n",
      "2299  7.606067  248.041453  14609.976880     6.356555  322.356572   \n",
      "...        ...         ...           ...          ...         ...   \n",
      "24    5.400302  140.739062  17266.593420    10.056852  328.358241   \n",
      "1856  7.233538  231.560832  16621.494790     6.073041  333.775777   \n",
      "2861  9.318614  317.338124  24497.873940     7.597452  357.167217   \n",
      "1919  8.458797  241.768340  29317.142440     5.783275  313.885548   \n",
      "2120  7.080795  219.074646  31044.738180     6.772327  263.900537   \n",
      "\n",
      "      Conductivity  Organic_carbon  Trihalomethanes  Turbidity  Potability  \n",
      "1818    402.664582       15.459752        76.987232   4.931354           0  \n",
      "1681    438.619635       13.089086        58.083993   2.413629           0  \n",
      "1357    303.878793       15.254653        71.542306   3.302212           0  \n",
      "244     491.765313       10.546886        74.502808   4.501457           0  \n",
      "2299    275.317146       11.706095        94.775244   4.581477           0  \n",
      "...            ...             ...              ...        ...         ...  \n",
      "24      472.874073       11.256381        56.931906   4.824786           0  \n",
      "1856    475.821645        8.688715        35.833969   3.679015           0  \n",
      "2861    476.510384       12.032377        68.599830   4.642719           1  \n",
      "1919    328.579429       18.296001        84.510985   3.827431           1  \n",
      "2120    411.471151       16.208928        70.852672   1.984615           0  \n",
      "\n",
      "[3276 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "waterquality = waterquality.sample(frac = 1)\n",
    "print(\"\\n SHUFFLED DATAFRAME:\")\n",
    "print(waterquality)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d038205f",
   "metadata": {},
   "source": [
    "##### 4.2.2. Predictor and response <a class=\"anchor\" id=\"section_4_2_2\"></a>\n",
    "\n",
    "The data assigned to the X and y variable to generate a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "2d14e47f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = waterquality[['pH','Hardness','Solids','Chloramines','Sulfates','Conductivity',\n",
    "                 'Organic_carbon','Trihalomethanes','Turbidity','Potability']].astype('float32')\n",
    "\n",
    "y = waterquality[['Potability']].astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "12ba732b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            pH    Hardness        Solids  Chloramines    Sulfates  \\\n",
      "1818  7.277144  194.880859  18270.105469     6.121932  344.879761   \n",
      "1681  7.460148  221.343216  25480.917969     6.926943  333.775787   \n",
      "1357  7.893132  201.433243  20526.048828     5.628776  299.018250   \n",
      "244   6.321259  207.257706   8532.139648     5.987876  286.489288   \n",
      "2299  7.606067  248.041458  14609.976562     6.356555  322.356567   \n",
      "...        ...         ...           ...          ...         ...   \n",
      "24    5.400302  140.739059  17266.593750    10.056852  328.358246   \n",
      "1856  7.233539  231.560837  16621.494141     6.073041  333.775787   \n",
      "2861  9.318614  317.338135  24497.873047     7.597452  357.167206   \n",
      "1919  8.458797  241.768341  29317.142578     5.783276  313.885559   \n",
      "2120  7.080794  219.074646  31044.738281     6.772327  263.900543   \n",
      "\n",
      "      Conductivity  Organic_carbon  Trihalomethanes  Turbidity  Potability  \n",
      "1818    402.664581       15.459751        76.987228   4.931354         0.0  \n",
      "1681    438.619629       13.089086        58.083992   2.413629         0.0  \n",
      "1357    303.878784       15.254653        71.542305   3.302212         0.0  \n",
      "244     491.765320       10.546886        74.502808   4.501457         0.0  \n",
      "2299    275.317139       11.706095        94.775246   4.581478         0.0  \n",
      "...            ...             ...              ...        ...         ...  \n",
      "24      472.874084       11.256381        56.931908   4.824786         0.0  \n",
      "1856    475.821655        8.688715        35.833969   3.679015         0.0  \n",
      "2861    476.510376       12.032377        68.599831   4.642719         1.0  \n",
      "1919    328.579437       18.296001        84.510986   3.827431         1.0  \n",
      "2120    411.471161       16.208927        70.852669   1.984615         0.0  \n",
      "\n",
      "[3276 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189f15fa",
   "metadata": {},
   "source": [
    "##### 4.2.3. Test and train split <a class=\"anchor\" id=\"section_4_2_3\"></a>\n",
    "\n",
    "Split the dataset into testing and training.\n",
    "\n",
    "80% for training, and 20% for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "56f2dd73",
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers = waterquality.iloc[:,:-1]\n",
    "features = waterquality.iloc[:, -1].values\n",
    "\n",
    "numbers = numbers.astype(np.float32)\n",
    "numbers = numbers.to_numpy()\n",
    "features = features.astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "d771b45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(numbers, features, test_size = 0.20, train_size = 0.80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4087dd",
   "metadata": {},
   "source": [
    "##### 4.2.4. Feature scaling  <a class=\"anchor\" id=\"section_4_2_4\"></a>\n",
    "\n",
    "Introduce feature scaling to minimise noise in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "a502a43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardization since the dataset has been made to have a normal distribution\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_standard = scaler.fit_transform(X_train)\n",
    "X_test_standard = scaler.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d0cbba",
   "metadata": {},
   "source": [
    "##### 4.2.5. SMOTE  <a class=\"anchor\" id=\"section_4_2_5\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "943e4d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_smote_train, y_smote_train = SMOTE().fit_resample(X_train_standard, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa98158",
   "metadata": {},
   "source": [
    "### Chapter 5: Building a MLP<a class=\"anchor\" id=\"chapter05\"></a>\n",
    "\n",
    "### 5.1. Layers and parameters <a class=\"anchor\" id=\"section_5_1\"></a>\n",
    "\n",
    "We assign a value to each layer of the neural network: input, output, and the hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "401e65d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer = 9 # (number of features)\n",
    "hidden_layer = 3 # (layers due to the size of the dataset)\n",
    "output_layer = 2 # (number of classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682f6347",
   "metadata": {},
   "source": [
    "Create a function and pass the above parameters through."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "6f26122d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultilayerPerceptron(nn.Module):\n",
    "    def __init__(self,\n",
    "        input_dim = input_layer,\n",
    "        hidden_dim = hidden_layer,\n",
    "        output_dim= output_layer,\n",
    "        dropout=0.5,\n",
    "    ):\n",
    "        \n",
    "        super(MultilayerPerceptron, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout) \n",
    "        \n",
    "        self.linear1 = nn.Linear(input_layer, hidden_layer)\n",
    "        self.linear2 = nn.Linear(hidden_layer, output_layer)\n",
    "        \n",
    "    def __getitem__(self,input_dim, hidden_dim,output_dim):\n",
    "        \n",
    "        return input_dim, hidden_dim, output_dim\n",
    "    \n",
    "    def forward(self, inputs, **kwargs):\n",
    "        inputs = F.relu(self.linear1(inputs)) # pass output of hidden layer through a relu activation function\n",
    "        inputs = self.dropout(inputs) # apply dropout\n",
    "        inputs = F.softmax(self.linear2(inputs), dim = -1)\n",
    "         \n",
    "        return inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "21af73f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size by default is set at 128\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "net = NeuralNetClassifier(\n",
    "    MultilayerPerceptron,\n",
    "    max_epochs=30,\n",
    "    lr=0.1,\n",
    "    device = device,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530bde0b",
   "metadata": {},
   "source": [
    "### 5.2. Training results <a class=\"anchor\" id=\"section_5_2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "e48f3a55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m0.7350\u001b[0m       \u001b[32m0.4930\u001b[0m        \u001b[35m0.7251\u001b[0m  0.2066\n",
      "      2        \u001b[36m0.7081\u001b[0m       \u001b[32m0.4977\u001b[0m        \u001b[35m0.7125\u001b[0m  0.2009\n",
      "      3        \u001b[36m0.7011\u001b[0m       \u001b[32m0.5055\u001b[0m        \u001b[35m0.7065\u001b[0m  0.2006\n",
      "      4        \u001b[36m0.6958\u001b[0m       \u001b[32m0.5149\u001b[0m        \u001b[35m0.7036\u001b[0m  0.5325\n",
      "      5        \u001b[36m0.6953\u001b[0m       0.5117        \u001b[35m0.7009\u001b[0m  0.2011\n",
      "      6        \u001b[36m0.6929\u001b[0m       \u001b[32m0.5211\u001b[0m        \u001b[35m0.6993\u001b[0m  0.2654\n",
      "      7        \u001b[36m0.6896\u001b[0m       \u001b[32m0.5227\u001b[0m        \u001b[35m0.6982\u001b[0m  0.3394\n",
      "      8        0.6924       \u001b[32m0.5243\u001b[0m        \u001b[35m0.6975\u001b[0m  0.1935\n",
      "      9        0.6942       \u001b[32m0.5258\u001b[0m        \u001b[35m0.6972\u001b[0m  0.2123\n",
      "     10        0.6914       \u001b[32m0.5290\u001b[0m        \u001b[35m0.6954\u001b[0m  0.3318\n",
      "     11        0.6901       0.5274        \u001b[35m0.6950\u001b[0m  0.2510\n",
      "     12        \u001b[36m0.6883\u001b[0m       0.5290        0.6953  0.2127\n",
      "     13        0.6899       0.5274        \u001b[35m0.6950\u001b[0m  0.2422\n",
      "     14        0.6926       \u001b[32m0.5305\u001b[0m        \u001b[35m0.6946\u001b[0m  0.2435\n",
      "     15        \u001b[36m0.6855\u001b[0m       \u001b[32m0.5336\u001b[0m        \u001b[35m0.6932\u001b[0m  0.2334\n",
      "     16        0.6885       0.5305        \u001b[35m0.6928\u001b[0m  0.2486\n",
      "     17        0.6911       0.5321        0.6929  0.2609\n",
      "     18        0.6861       0.5336        \u001b[35m0.6928\u001b[0m  0.2436\n",
      "     19        0.6909       0.5336        0.6930  0.2083\n",
      "     20        0.6890       0.5305        \u001b[35m0.6924\u001b[0m  0.2084\n",
      "     21        0.6890       0.5336        \u001b[35m0.6924\u001b[0m  0.2099\n",
      "     22        0.6882       \u001b[32m0.5352\u001b[0m        0.6930  0.2029\n",
      "     23        0.6897       \u001b[32m0.5383\u001b[0m        0.6926  0.2109\n",
      "     24        0.6866       \u001b[32m0.5399\u001b[0m        \u001b[35m0.6922\u001b[0m  0.2134\n",
      "     25        0.6872       \u001b[32m0.5415\u001b[0m        \u001b[35m0.6915\u001b[0m  0.2104\n",
      "     26        0.6882       0.5383        0.6925  0.2082\n",
      "     27        0.6857       \u001b[32m0.5430\u001b[0m        \u001b[35m0.6911\u001b[0m  0.2110\n",
      "     28        0.6903       0.5399        0.6915  0.2005\n",
      "     29        \u001b[36m0.6820\u001b[0m       \u001b[32m0.5446\u001b[0m        \u001b[35m0.6905\u001b[0m  0.2209\n",
      "     30        0.6850       0.5446        \u001b[35m0.6904\u001b[0m  0.2086\n"
     ]
    }
   ],
   "source": [
    "net.fit(X_smote_train, y_smote_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "4966f283",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = net.predict(X_test_standard)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3116077d",
   "metadata": {},
   "source": [
    "print accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "503a7332",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.45884146341463417"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1d8f52",
   "metadata": {},
   "source": [
    "the results of the training are mediocre... now we will attempt to improve the results through tuning parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28608fba",
   "metadata": {},
   "source": [
    "### 5.3. Hyperparameter tuning <a class=\"anchor\" id=\"section_5_3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "75ef185c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultilayerPerceptron(nn.Module):\n",
    "    def __init__(self,\n",
    "        input_dim = input_layer,\n",
    "        hidden_dim = hidden_layer,\n",
    "        output_dim= output_layer,\n",
    "        dropout=0.5,\n",
    "    ):\n",
    "        \n",
    "        super(MultilayerPerceptron, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout) \n",
    "        \n",
    "        self.linear1 = nn.Linear(input_layer, hidden_layer)\n",
    "        self.linear2 = nn.Linear(hidden_layer, output_layer)\n",
    "        \n",
    "    def __getitem__(self,input_dim, hidden_dim,output_dim):\n",
    "        \n",
    "        return input_dim, hidden_dim, output_dim\n",
    "    \n",
    "    def forward(self, inputs, **kwargs):\n",
    "        inputs = F.relu(self.linear1(inputs)) # pass output of hidden layer through a relu activation function\n",
    "        inputs = self.dropout(inputs) # apply dropout\n",
    "        # removing the softmax activation function\n",
    "        \n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "3c03ec79",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "net = NeuralNetClassifier(\n",
    "    MultilayerPerceptron,\n",
    "    callbacks = [EarlyStopping()],\n",
    "    criterion = nn.CrossEntropyLoss,\n",
    "    device = device, \n",
    ")\n",
    "\n",
    "parameters = {\n",
    "    'lr': [0.001, 0.01, 0.1],\n",
    "    'batch_size': [150,175,200],\n",
    "    'optimizer': [optim.Adam, optim.SGD],\n",
    "    'max_epochs': [40, 60], \n",
    "    'module__dropout': [0.5],\n",
    "    'optimizer__momentum': [1.0]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "3f31ab5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 36 candidates, totalling 360 fits\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.2564\u001b[0m       \u001b[32m0.3284\u001b[0m        \u001b[35m1.1862\u001b[0m  0.1158\n",
      "      2        \u001b[36m1.2161\u001b[0m       \u001b[32m0.3369\u001b[0m        \u001b[35m1.1488\u001b[0m  0.1281\n",
      "      3        \u001b[36m1.1904\u001b[0m       \u001b[32m0.4131\u001b[0m        \u001b[35m1.0979\u001b[0m  0.1484\n",
      "      4        \u001b[36m1.0987\u001b[0m       \u001b[32m0.5021\u001b[0m        \u001b[35m1.0485\u001b[0m  0.1519\n",
      "      5        \u001b[36m1.0721\u001b[0m       \u001b[32m0.5720\u001b[0m        \u001b[35m1.0230\u001b[0m  0.1338\n",
      "      6        \u001b[36m1.0650\u001b[0m       \u001b[32m0.5911\u001b[0m        \u001b[35m1.0117\u001b[0m  0.1379\n",
      "      7        \u001b[36m1.0238\u001b[0m       \u001b[32m0.5996\u001b[0m        \u001b[35m0.9960\u001b[0m  0.1402\n",
      "      8        1.0297       \u001b[32m0.6186\u001b[0m        \u001b[35m0.9793\u001b[0m  0.1384\n",
      "      9        \u001b[36m1.0231\u001b[0m       \u001b[32m0.6208\u001b[0m        \u001b[35m0.9626\u001b[0m  0.1303\n",
      "     10        \u001b[36m1.0142\u001b[0m       \u001b[32m0.6229\u001b[0m        \u001b[35m0.9490\u001b[0m  0.1398\n",
      "     11        \u001b[36m1.0049\u001b[0m       0.6229        \u001b[35m0.9399\u001b[0m  0.1411\n",
      "     12        1.0415       0.6144        \u001b[35m0.9354\u001b[0m  0.1299\n",
      "     13        1.0431       0.6165        \u001b[35m0.9346\u001b[0m  0.1401\n",
      "     14        1.0591       0.6123        0.9355  0.1422\n",
      "     15        1.0661       0.6038        0.9368  0.1381\n",
      "     16        1.0833       0.6038        0.9370  0.1407\n",
      "     17        1.0497       0.6017        0.9363  0.1400\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.2182\u001b[0m       \u001b[32m0.3835\u001b[0m        \u001b[35m1.1044\u001b[0m  0.1437\n",
      "      2        \u001b[36m1.1828\u001b[0m       \u001b[32m0.4153\u001b[0m        \u001b[35m1.0784\u001b[0m  0.1497\n",
      "      3        \u001b[36m1.1555\u001b[0m       \u001b[32m0.4597\u001b[0m        \u001b[35m1.0427\u001b[0m  0.1423\n",
      "      4        \u001b[36m1.0925\u001b[0m       \u001b[32m0.5466\u001b[0m        \u001b[35m1.0100\u001b[0m  0.1419\n",
      "      5        \u001b[36m1.0536\u001b[0m       \u001b[32m0.5657\u001b[0m        \u001b[35m0.9906\u001b[0m  0.1507\n",
      "      6        \u001b[36m1.0367\u001b[0m       \u001b[32m0.5847\u001b[0m        \u001b[35m0.9795\u001b[0m  0.1298\n",
      "      7        \u001b[36m1.0291\u001b[0m       \u001b[32m0.5996\u001b[0m        \u001b[35m0.9703\u001b[0m  0.1501\n",
      "      8        \u001b[36m1.0271\u001b[0m       \u001b[32m0.6186\u001b[0m        \u001b[35m0.9629\u001b[0m  0.1503\n",
      "      9        \u001b[36m1.0170\u001b[0m       0.6102        \u001b[35m0.9594\u001b[0m  0.1307\n",
      "     10        1.0234       0.5953        \u001b[35m0.9594\u001b[0m  0.1587\n",
      "     11        1.0255       0.5932        0.9611  0.1231\n",
      "     12        1.0676       0.5847        0.9648  0.1495\n",
      "     13        1.0605       0.5826        0.9690  0.1294\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.1656\u001b[0m       \u001b[32m0.3962\u001b[0m        \u001b[35m1.1329\u001b[0m  0.2563\n",
      "      2        \u001b[36m1.1353\u001b[0m       \u001b[32m0.4237\u001b[0m        \u001b[35m1.1120\u001b[0m  0.1503\n",
      "      3        \u001b[36m1.1241\u001b[0m       \u001b[32m0.5064\u001b[0m        \u001b[35m1.0847\u001b[0m  0.1418\n",
      "      4        \u001b[36m1.0796\u001b[0m       \u001b[32m0.5593\u001b[0m        \u001b[35m1.0601\u001b[0m  0.1410\n",
      "      5        \u001b[36m1.0657\u001b[0m       \u001b[32m0.5614\u001b[0m        \u001b[35m1.0459\u001b[0m  0.1601\n",
      "      6        \u001b[36m1.0559\u001b[0m       0.5572        \u001b[35m1.0353\u001b[0m  0.1497\n",
      "      7        \u001b[36m1.0385\u001b[0m       \u001b[32m0.5720\u001b[0m        \u001b[35m1.0236\u001b[0m  0.1397\n",
      "      8        1.0403       0.5657        \u001b[35m1.0117\u001b[0m  0.1566\n",
      "      9        1.0555       \u001b[32m0.5805\u001b[0m        \u001b[35m1.0002\u001b[0m  0.1298\n",
      "     10        \u001b[36m1.0292\u001b[0m       \u001b[32m0.5869\u001b[0m        \u001b[35m0.9892\u001b[0m  0.1697\n",
      "     11        1.0433       0.5826        \u001b[35m0.9808\u001b[0m  0.1395\n",
      "     12        1.0548       0.5869        \u001b[35m0.9740\u001b[0m  0.1594\n",
      "     13        1.0570       \u001b[32m0.5890\u001b[0m        \u001b[35m0.9688\u001b[0m  0.1302\n",
      "     14        1.0465       0.5890        \u001b[35m0.9654\u001b[0m  0.1514\n",
      "     15        1.0499       0.5890        \u001b[35m0.9623\u001b[0m  0.1422\n",
      "     16        1.0614       \u001b[32m0.5932\u001b[0m        \u001b[35m0.9593\u001b[0m  0.1417\n",
      "     17        1.0766       \u001b[32m0.6081\u001b[0m        \u001b[35m0.9576\u001b[0m  0.1427\n",
      "     18        1.0746       0.5911        \u001b[35m0.9573\u001b[0m  0.1418\n",
      "     19        1.0574       0.5975        0.9582  0.1474\n",
      "     20        1.0605       0.5911        0.9599  0.1405\n",
      "     21        1.0595       0.5763        0.9627  0.1495\n",
      "     22        1.0414       0.5699        0.9658  0.1609\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.1518\u001b[0m       \u001b[32m0.4852\u001b[0m        \u001b[35m1.0935\u001b[0m  0.1757\n",
      "      2        \u001b[36m1.1478\u001b[0m       \u001b[32m0.5233\u001b[0m        \u001b[35m1.0800\u001b[0m  0.1504\n",
      "      3        \u001b[36m1.1254\u001b[0m       \u001b[32m0.5636\u001b[0m        \u001b[35m1.0636\u001b[0m  0.1929\n",
      "      4        \u001b[36m1.0933\u001b[0m       \u001b[32m0.5953\u001b[0m        \u001b[35m1.0519\u001b[0m  0.1399\n",
      "      5        \u001b[36m1.0817\u001b[0m       \u001b[32m0.6123\u001b[0m        \u001b[35m1.0469\u001b[0m  0.2001\n",
      "      6        1.0840       0.6102        \u001b[35m1.0422\u001b[0m  0.1905\n",
      "      7        \u001b[36m1.0681\u001b[0m       0.6102        \u001b[35m1.0370\u001b[0m  0.1904\n",
      "      8        1.0707       \u001b[32m0.6165\u001b[0m        \u001b[35m1.0303\u001b[0m  0.1945\n",
      "      9        \u001b[36m1.0593\u001b[0m       0.6081        \u001b[35m1.0231\u001b[0m  0.1904\n",
      "     10        1.0629       0.5975        \u001b[35m1.0162\u001b[0m  0.1798\n",
      "     11        \u001b[36m1.0471\u001b[0m       0.5911        \u001b[35m1.0096\u001b[0m  0.1992\n",
      "     12        1.0475       0.5911        \u001b[35m1.0045\u001b[0m  0.2013\n",
      "     13        1.0523       0.5932        \u001b[35m1.0019\u001b[0m  0.2920\n",
      "     14        1.0542       0.5911        \u001b[35m0.9987\u001b[0m  0.1896\n",
      "     15        \u001b[36m1.0428\u001b[0m       0.5953        \u001b[35m0.9960\u001b[0m  0.1810\n",
      "     16        1.0786       0.5975        \u001b[35m0.9936\u001b[0m  0.2029\n",
      "     17        1.0546       0.5996        \u001b[35m0.9920\u001b[0m  0.1825\n",
      "     18        1.0630       0.5953        \u001b[35m0.9904\u001b[0m  0.1842\n",
      "     19        1.0793       0.5975        \u001b[35m0.9898\u001b[0m  0.2213\n",
      "     20        1.0785       0.6017        0.9898  0.1501\n",
      "     21        1.0903       0.5911        0.9904  0.2197\n",
      "     22        1.0626       0.5890        0.9916  0.1596\n",
      "     23        1.0717       0.5805        0.9927  0.2003\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.2405\u001b[0m       \u001b[32m0.2691\u001b[0m        \u001b[35m1.1776\u001b[0m  0.1593\n",
      "      2        \u001b[36m1.2114\u001b[0m       \u001b[32m0.3220\u001b[0m        \u001b[35m1.1413\u001b[0m  0.1332\n",
      "      3        \u001b[36m1.1630\u001b[0m       \u001b[32m0.4364\u001b[0m        \u001b[35m1.0948\u001b[0m  0.1699\n",
      "      4        \u001b[36m1.0927\u001b[0m       \u001b[32m0.5424\u001b[0m        \u001b[35m1.0595\u001b[0m  0.1301\n",
      "      5        \u001b[36m1.0666\u001b[0m       \u001b[32m0.5720\u001b[0m        \u001b[35m1.0524\u001b[0m  0.1566\n",
      "      6        1.0709       \u001b[32m0.5932\u001b[0m        \u001b[35m1.0479\u001b[0m  0.1514\n",
      "      7        \u001b[36m1.0665\u001b[0m       \u001b[32m0.6081\u001b[0m        \u001b[35m1.0439\u001b[0m  0.1397\n",
      "      8        1.0762       0.6081        \u001b[35m1.0403\u001b[0m  0.1718\n",
      "      9        1.0691       0.6081        \u001b[35m1.0373\u001b[0m  0.1395\n",
      "     10        \u001b[36m1.0563\u001b[0m       \u001b[32m0.6102\u001b[0m        \u001b[35m1.0336\u001b[0m  0.1727\n",
      "     11        \u001b[36m1.0469\u001b[0m       \u001b[32m0.6165\u001b[0m        \u001b[35m1.0283\u001b[0m  0.1553\n",
      "     12        \u001b[36m1.0443\u001b[0m       0.6165        \u001b[35m1.0212\u001b[0m  0.1367\n",
      "     13        1.0446       \u001b[32m0.6186\u001b[0m        \u001b[35m1.0125\u001b[0m  0.1848\n",
      "     14        1.0444       0.6123        \u001b[35m1.0025\u001b[0m  0.1396\n",
      "     15        \u001b[36m1.0417\u001b[0m       0.6123        \u001b[35m0.9923\u001b[0m  0.1984\n",
      "     16        \u001b[36m1.0341\u001b[0m       0.6059        \u001b[35m0.9808\u001b[0m  0.2147\n",
      "     17        \u001b[36m1.0312\u001b[0m       0.6038        \u001b[35m0.9706\u001b[0m  0.1414\n",
      "     18        1.0567       0.5996        \u001b[35m0.9616\u001b[0m  0.1420\n",
      "     19        1.0715       0.6017        \u001b[35m0.9541\u001b[0m  0.1321\n",
      "     20        1.0606       0.5975        \u001b[35m0.9471\u001b[0m  0.1301\n",
      "     21        1.0435       0.5975        \u001b[35m0.9394\u001b[0m  0.1680\n",
      "     22        1.0557       0.6038        \u001b[35m0.9311\u001b[0m  0.1297\n",
      "     23        1.0879       0.6017        \u001b[35m0.9232\u001b[0m  0.1393\n",
      "     24        1.0503       0.5996        \u001b[35m0.9162\u001b[0m  0.1724\n",
      "     25        1.0800       0.6081        \u001b[35m0.9113\u001b[0m  0.1367\n",
      "     26        \u001b[36m1.0307\u001b[0m       \u001b[32m0.6208\u001b[0m        \u001b[35m0.9095\u001b[0m  0.1336\n",
      "     27        1.0308       \u001b[32m0.6250\u001b[0m        0.9126  0.1606\n",
      "     28        \u001b[36m1.0156\u001b[0m       0.6229        0.9209  0.1302\n",
      "     29        1.0273       0.6165        0.9341  0.1205\n",
      "     30        1.0162       0.6144        0.9503  0.1206\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.2654\u001b[0m       \u001b[32m0.3051\u001b[0m        \u001b[35m1.2091\u001b[0m  0.1232\n",
      "      2        \u001b[36m1.2250\u001b[0m       \u001b[32m0.3347\u001b[0m        \u001b[35m1.1714\u001b[0m  0.1209\n",
      "      3        \u001b[36m1.1764\u001b[0m       \u001b[32m0.4089\u001b[0m        \u001b[35m1.1218\u001b[0m  0.1201\n",
      "      4        \u001b[36m1.1239\u001b[0m       \u001b[32m0.5127\u001b[0m        \u001b[35m1.0793\u001b[0m  0.1204\n",
      "      5        \u001b[36m1.0803\u001b[0m       \u001b[32m0.5678\u001b[0m        \u001b[35m1.0650\u001b[0m  0.1204\n",
      "      6        \u001b[36m1.0734\u001b[0m       0.5678        \u001b[35m1.0589\u001b[0m  0.1301\n",
      "      7        \u001b[36m1.0593\u001b[0m       \u001b[32m0.5784\u001b[0m        \u001b[35m1.0513\u001b[0m  0.1412\n",
      "      8        1.0609       \u001b[32m0.5847\u001b[0m        \u001b[35m1.0417\u001b[0m  0.1286\n",
      "      9        \u001b[36m1.0528\u001b[0m       \u001b[32m0.5953\u001b[0m        \u001b[35m1.0329\u001b[0m  0.1301\n",
      "     10        1.0688       0.5890        \u001b[35m1.0239\u001b[0m  0.1323\n",
      "     11        1.0616       0.5805        \u001b[35m1.0146\u001b[0m  0.1383\n",
      "     12        1.0532       0.5742        \u001b[35m1.0049\u001b[0m  0.1300\n",
      "     13        1.0616       0.5763        \u001b[35m0.9948\u001b[0m  0.1206\n",
      "     14        \u001b[36m1.0508\u001b[0m       0.5678        \u001b[35m0.9854\u001b[0m  0.1305\n",
      "     15        1.0585       0.5720        \u001b[35m0.9770\u001b[0m  0.1312\n",
      "     16        1.0647       0.5784        \u001b[35m0.9688\u001b[0m  0.1284\n",
      "     17        1.0683       0.5890        \u001b[35m0.9611\u001b[0m  0.1412\n",
      "     18        \u001b[36m1.0496\u001b[0m       \u001b[32m0.6081\u001b[0m        \u001b[35m0.9538\u001b[0m  0.1304\n",
      "     19        1.0543       \u001b[32m0.6102\u001b[0m        \u001b[35m0.9480\u001b[0m  0.1300\n",
      "     20        1.0808       \u001b[32m0.6123\u001b[0m        \u001b[35m0.9441\u001b[0m  0.1398\n",
      "     21        1.0604       0.6059        \u001b[35m0.9409\u001b[0m  0.1308\n",
      "     22        1.0696       0.6081        \u001b[35m0.9386\u001b[0m  0.1301\n",
      "     23        1.0731       \u001b[32m0.6165\u001b[0m        \u001b[35m0.9369\u001b[0m  0.1384\n",
      "     24        1.0741       0.6165        \u001b[35m0.9355\u001b[0m  0.1277\n",
      "     25        1.0732       \u001b[32m0.6186\u001b[0m        \u001b[35m0.9352\u001b[0m  0.1317\n",
      "     26        1.0574       0.6144        \u001b[35m0.9351\u001b[0m  0.1197\n",
      "     27        1.0777       0.6123        0.9352  0.1512\n",
      "     28        1.0683       0.6144        0.9363  0.1282\n",
      "     29        1.0545       \u001b[32m0.6250\u001b[0m        0.9371  0.1402\n",
      "     30        \u001b[36m1.0417\u001b[0m       0.6229        0.9389  0.1304\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.2375\u001b[0m       \u001b[32m0.2394\u001b[0m        \u001b[35m1.1777\u001b[0m  0.1345\n",
      "      2        \u001b[36m1.2096\u001b[0m       \u001b[32m0.2797\u001b[0m        \u001b[35m1.1430\u001b[0m  0.1289\n",
      "      3        \u001b[36m1.1633\u001b[0m       \u001b[32m0.3898\u001b[0m        \u001b[35m1.0968\u001b[0m  0.1505\n",
      "      4        \u001b[36m1.1050\u001b[0m       \u001b[32m0.5275\u001b[0m        \u001b[35m1.0609\u001b[0m  0.1308\n",
      "      5        \u001b[36m1.0823\u001b[0m       \u001b[32m0.5636\u001b[0m        \u001b[35m1.0516\u001b[0m  0.1305\n",
      "      6        1.0839       \u001b[32m0.5699\u001b[0m        \u001b[35m1.0449\u001b[0m  0.1302\n",
      "      7        \u001b[36m1.0606\u001b[0m       \u001b[32m0.6123\u001b[0m        \u001b[35m1.0382\u001b[0m  0.2118\n",
      "      8        1.0758       \u001b[32m0.6165\u001b[0m        \u001b[35m1.0320\u001b[0m  0.1484\n",
      "      9        1.0622       0.6102        \u001b[35m1.0262\u001b[0m  0.1300\n",
      "     10        1.0640       0.6165        \u001b[35m1.0201\u001b[0m  0.1537\n",
      "     11        1.0617       0.5975        \u001b[35m1.0123\u001b[0m  0.1494\n",
      "     12        \u001b[36m1.0482\u001b[0m       0.5975        \u001b[35m1.0029\u001b[0m  0.1614\n",
      "     13        1.0655       0.5932        \u001b[35m0.9930\u001b[0m  0.1609\n",
      "     14        \u001b[36m1.0464\u001b[0m       0.5975        \u001b[35m0.9825\u001b[0m  0.1298\n",
      "     15        \u001b[36m1.0433\u001b[0m       0.5996        \u001b[35m0.9717\u001b[0m  0.1703\n",
      "     16        1.0524       0.5996        \u001b[35m0.9597\u001b[0m  0.1405\n",
      "     17        1.0559       0.5932        \u001b[35m0.9469\u001b[0m  0.1526\n",
      "     18        1.0503       0.5932        \u001b[35m0.9364\u001b[0m  0.1403\n",
      "     19        1.0583       0.5869        \u001b[35m0.9287\u001b[0m  0.1784\n",
      "     20        1.0498       0.5805        \u001b[35m0.9230\u001b[0m  0.1502\n",
      "     21        1.0730       0.5869        \u001b[35m0.9189\u001b[0m  0.1711\n",
      "     22        1.0523       0.5932        \u001b[35m0.9168\u001b[0m  0.1403\n",
      "     23        1.0756       0.5953        \u001b[35m0.9158\u001b[0m  0.1900\n",
      "     24        1.0483       0.6038        \u001b[35m0.9157\u001b[0m  0.1406\n",
      "     25        1.0665       0.6081        0.9176  0.1809\n",
      "     26        \u001b[36m1.0433\u001b[0m       0.6123        0.9208  0.1587\n",
      "     27        1.0476       0.6144        0.9259  0.1634\n",
      "     28        \u001b[36m1.0283\u001b[0m       0.6165        0.9334  0.1602\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.2903\u001b[0m       \u001b[32m0.3093\u001b[0m        \u001b[35m1.2025\u001b[0m  0.1908\n",
      "      2        1.3005       \u001b[32m0.3263\u001b[0m        \u001b[35m1.1669\u001b[0m  0.1500\n",
      "      3        \u001b[36m1.2070\u001b[0m       \u001b[32m0.3877\u001b[0m        \u001b[35m1.1163\u001b[0m  0.1701\n",
      "      4        \u001b[36m1.1349\u001b[0m       \u001b[32m0.4407\u001b[0m        \u001b[35m1.0674\u001b[0m  0.1291\n",
      "      5        \u001b[36m1.0855\u001b[0m       \u001b[32m0.5064\u001b[0m        \u001b[35m1.0428\u001b[0m  0.1455\n",
      "      6        \u001b[36m1.0696\u001b[0m       \u001b[32m0.5699\u001b[0m        \u001b[35m1.0372\u001b[0m  0.1484\n",
      "      7        \u001b[36m1.0642\u001b[0m       \u001b[32m0.6038\u001b[0m        \u001b[35m1.0357\u001b[0m  0.1385\n",
      "      8        1.0682       \u001b[32m0.6081\u001b[0m        1.0362  0.1502\n",
      "      9        \u001b[36m1.0633\u001b[0m       0.5975        \u001b[35m1.0357\u001b[0m  0.1217\n",
      "     10        1.0759       0.5975        \u001b[35m1.0347\u001b[0m  0.1602\n",
      "     11        \u001b[36m1.0571\u001b[0m       0.5953        \u001b[35m1.0339\u001b[0m  0.1220\n",
      "     12        1.0769       0.5932        \u001b[35m1.0336\u001b[0m  0.1583\n",
      "     13        1.0836       0.5847        \u001b[35m1.0333\u001b[0m  0.1359\n",
      "     14        1.0856       0.5869        \u001b[35m1.0328\u001b[0m  0.2307\n",
      "     15        1.0831       0.5890        \u001b[35m1.0318\u001b[0m  0.1432\n",
      "     16        1.1002       0.5847        \u001b[35m1.0308\u001b[0m  0.1297\n",
      "     17        1.1128       0.5805        \u001b[35m1.0291\u001b[0m  0.1606\n",
      "     18        1.1088       0.5869        \u001b[35m1.0270\u001b[0m  0.1306\n",
      "     19        1.1114       0.5869        \u001b[35m1.0245\u001b[0m  0.1602\n",
      "     20        1.0705       0.5890        \u001b[35m1.0219\u001b[0m  0.1514\n",
      "     21        1.0734       0.5911        \u001b[35m1.0198\u001b[0m  0.1400\n",
      "     22        1.0610       0.5953        \u001b[35m1.0176\u001b[0m  0.1510\n",
      "     23        1.0676       0.6017        \u001b[35m1.0162\u001b[0m  0.1280\n",
      "     24        1.0768       0.6038        \u001b[35m1.0153\u001b[0m  0.1696\n",
      "     25        1.0629       0.5996        \u001b[35m1.0148\u001b[0m  0.1363\n",
      "     26        \u001b[36m1.0567\u001b[0m       0.5953        \u001b[35m1.0142\u001b[0m  0.1353\n",
      "     27        \u001b[36m1.0556\u001b[0m       0.5953        \u001b[35m1.0131\u001b[0m  0.1585\n",
      "     28        1.0564       0.5996        \u001b[35m1.0113\u001b[0m  0.1305\n",
      "     29        \u001b[36m1.0474\u001b[0m       0.6038        \u001b[35m1.0091\u001b[0m  0.1608\n",
      "     30        1.0769       0.6059        \u001b[35m1.0072\u001b[0m  0.1500\n",
      "     31        1.0690       \u001b[32m0.6123\u001b[0m        \u001b[35m1.0056\u001b[0m  0.1386\n",
      "     32        1.0797       \u001b[32m0.6144\u001b[0m        \u001b[35m1.0043\u001b[0m  0.1619\n",
      "     33        1.0956       \u001b[32m0.6165\u001b[0m        \u001b[35m1.0031\u001b[0m  0.1200\n",
      "     34        1.0969       0.6165        \u001b[35m1.0021\u001b[0m  0.1700\n",
      "     35        1.0936       0.6165        \u001b[35m1.0014\u001b[0m  0.1526\n",
      "     36        1.1037       \u001b[32m0.6208\u001b[0m        \u001b[35m1.0014\u001b[0m  0.1378\n",
      "     37        1.1055       0.6144        1.0016  0.1595\n",
      "     38        1.0894       0.6144        1.0025  0.1399\n",
      "     39        1.0698       0.6186        1.0047  0.1297\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.1476\u001b[0m       \u001b[32m0.3919\u001b[0m        \u001b[35m1.1021\u001b[0m  0.1207\n",
      "      2        1.1586       \u001b[32m0.4174\u001b[0m        \u001b[35m1.0822\u001b[0m  0.1615\n",
      "      3        \u001b[36m1.1108\u001b[0m       \u001b[32m0.4407\u001b[0m        \u001b[35m1.0555\u001b[0m  0.1314\n",
      "      4        \u001b[36m1.0900\u001b[0m       \u001b[32m0.4958\u001b[0m        \u001b[35m1.0325\u001b[0m  0.1299\n",
      "      5        \u001b[36m1.0681\u001b[0m       \u001b[32m0.5254\u001b[0m        \u001b[35m1.0240\u001b[0m  0.1520\n",
      "      6        \u001b[36m1.0591\u001b[0m       \u001b[32m0.5699\u001b[0m        \u001b[35m1.0193\u001b[0m  0.1301\n",
      "      7        \u001b[36m1.0337\u001b[0m       0.5699        \u001b[35m1.0137\u001b[0m  0.1228\n",
      "      8        1.0345       \u001b[32m0.5953\u001b[0m        \u001b[35m1.0061\u001b[0m  0.1408\n",
      "      9        \u001b[36m1.0161\u001b[0m       \u001b[32m0.6081\u001b[0m        \u001b[35m0.9962\u001b[0m  0.1532\n",
      "     10        \u001b[36m1.0087\u001b[0m       0.5975        \u001b[35m0.9865\u001b[0m  0.1305\n",
      "     11        1.0191       0.5953        \u001b[35m0.9787\u001b[0m  0.1303\n",
      "     12        1.0194       0.5932        \u001b[35m0.9735\u001b[0m  0.1307\n",
      "     13        1.0545       0.5975        \u001b[35m0.9698\u001b[0m  0.1226\n",
      "     14        1.0715       0.5847        \u001b[35m0.9670\u001b[0m  0.1399\n",
      "     15        1.0537       0.5826        \u001b[35m0.9634\u001b[0m  0.1204\n",
      "     16        1.0731       0.5826        \u001b[35m0.9589\u001b[0m  0.1205\n",
      "     17        1.0960       0.5911        \u001b[35m0.9529\u001b[0m  0.1280\n",
      "     18        1.0866       0.6038        \u001b[35m0.9453\u001b[0m  0.1305\n",
      "     19        1.0904       \u001b[32m0.6123\u001b[0m        \u001b[35m0.9369\u001b[0m  0.1228\n",
      "     20        1.0678       0.6081        \u001b[35m0.9291\u001b[0m  0.1320\n",
      "     21        1.0294       \u001b[32m0.6165\u001b[0m        \u001b[35m0.9237\u001b[0m  0.1305\n",
      "     22        1.0421       \u001b[32m0.6186\u001b[0m        \u001b[35m0.9227\u001b[0m  0.1289\n",
      "     23        1.0279       \u001b[32m0.6229\u001b[0m        0.9278  0.1388\n",
      "     24        1.0167       0.6229        0.9393  0.1309\n",
      "     25        1.0336       0.6186        0.9547  0.1226\n",
      "     26        1.0307       0.6186        0.9691  0.1400\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.1954\u001b[0m       \u001b[32m0.2691\u001b[0m        \u001b[35m1.1670\u001b[0m  0.1292\n",
      "      2        \u001b[36m1.1725\u001b[0m       \u001b[32m0.3284\u001b[0m        \u001b[35m1.1385\u001b[0m  0.1306\n",
      "      3        \u001b[36m1.1380\u001b[0m       \u001b[32m0.4131\u001b[0m        \u001b[35m1.1035\u001b[0m  0.1303\n",
      "      4        \u001b[36m1.1009\u001b[0m       \u001b[32m0.5064\u001b[0m        \u001b[35m1.0822\u001b[0m  0.1196\n",
      "      5        \u001b[36m1.0800\u001b[0m       \u001b[32m0.5212\u001b[0m        \u001b[35m1.0747\u001b[0m  0.1296\n",
      "      6        \u001b[36m1.0709\u001b[0m       \u001b[32m0.5360\u001b[0m        \u001b[35m1.0671\u001b[0m  0.1303\n",
      "      7        1.0768       \u001b[32m0.5487\u001b[0m        \u001b[35m1.0597\u001b[0m  0.1202\n",
      "      8        1.0777       0.5487        \u001b[35m1.0536\u001b[0m  0.1324\n",
      "      9        \u001b[36m1.0671\u001b[0m       \u001b[32m0.5530\u001b[0m        \u001b[35m1.0478\u001b[0m  0.1223\n",
      "     10        1.0734       \u001b[32m0.5551\u001b[0m        \u001b[35m1.0425\u001b[0m  0.1483\n",
      "     11        1.0755       \u001b[32m0.5572\u001b[0m        \u001b[35m1.0374\u001b[0m  0.1303\n",
      "     12        1.0690       0.5487        \u001b[35m1.0315\u001b[0m  0.1481\n",
      "     13        1.0691       0.5445        \u001b[35m1.0245\u001b[0m  0.1408\n",
      "     14        \u001b[36m1.0621\u001b[0m       0.5445        \u001b[35m1.0171\u001b[0m  0.1202\n",
      "     15        1.0715       0.5487        \u001b[35m1.0089\u001b[0m  0.1384\n",
      "     16        1.0780       0.5487        \u001b[35m1.0000\u001b[0m  0.1284\n",
      "     17        1.0796       0.5572        \u001b[35m0.9905\u001b[0m  0.1402\n",
      "     18        1.0926       \u001b[32m0.5593\u001b[0m        \u001b[35m0.9808\u001b[0m  0.1304\n",
      "     19        1.0858       0.5530        \u001b[35m0.9707\u001b[0m  0.1407\n",
      "     20        1.0698       \u001b[32m0.5742\u001b[0m        \u001b[35m0.9625\u001b[0m  0.1225\n",
      "     21        \u001b[36m1.0487\u001b[0m       \u001b[32m0.5869\u001b[0m        \u001b[35m0.9547\u001b[0m  0.1833\n",
      "     22        \u001b[36m1.0452\u001b[0m       0.5869        \u001b[35m0.9487\u001b[0m  0.1402\n",
      "     23        \u001b[36m1.0375\u001b[0m       \u001b[32m0.5953\u001b[0m        \u001b[35m0.9455\u001b[0m  0.1714\n",
      "     24        1.0378       \u001b[32m0.6102\u001b[0m        \u001b[35m0.9434\u001b[0m  0.1586\n",
      "     25        \u001b[36m1.0299\u001b[0m       \u001b[32m0.6271\u001b[0m        \u001b[35m0.9414\u001b[0m  0.1588\n",
      "     26        1.0528       0.6271        0.9418  0.1689\n",
      "     27        1.0310       0.6208        0.9439  0.1400\n",
      "     28        1.0404       0.6229        0.9481  0.1702\n",
      "     29        1.0597       0.6250        0.9536  0.1505\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.1765\u001b[0m       \u001b[32m0.4237\u001b[0m        \u001b[35m1.0902\u001b[0m  0.1472\n",
      "      2        \u001b[36m1.1554\u001b[0m       \u001b[32m0.4534\u001b[0m        \u001b[35m1.0670\u001b[0m  0.1500\n",
      "      3        \u001b[36m1.1256\u001b[0m       \u001b[32m0.4958\u001b[0m        \u001b[35m1.0360\u001b[0m  0.2640\n",
      "      4        \u001b[36m1.0720\u001b[0m       \u001b[32m0.5720\u001b[0m        \u001b[35m1.0095\u001b[0m  0.1319\n",
      "      5        \u001b[36m1.0605\u001b[0m       \u001b[32m0.5953\u001b[0m        \u001b[35m0.9990\u001b[0m  0.1708\n",
      "      6        \u001b[36m1.0461\u001b[0m       \u001b[32m0.5996\u001b[0m        \u001b[35m0.9966\u001b[0m  0.1323\n",
      "      7        \u001b[36m1.0376\u001b[0m       0.5847        \u001b[35m0.9952\u001b[0m  0.2037\n",
      "      8        \u001b[36m1.0303\u001b[0m       0.5911        \u001b[35m0.9929\u001b[0m  0.1737\n",
      "      9        1.0345       0.5890        \u001b[35m0.9896\u001b[0m  0.1693\n",
      "     10        \u001b[36m1.0223\u001b[0m       0.5784        \u001b[35m0.9855\u001b[0m  0.1912\n",
      "     11        1.0231       0.5805        \u001b[35m0.9796\u001b[0m  0.1513\n",
      "     12        \u001b[36m1.0211\u001b[0m       0.5636        \u001b[35m0.9736\u001b[0m  0.1482\n",
      "     13        1.0382       0.5678        \u001b[35m0.9676\u001b[0m  0.1597\n",
      "     14        1.0367       0.5699        \u001b[35m0.9620\u001b[0m  0.1639\n",
      "     15        1.0391       0.5742        \u001b[35m0.9564\u001b[0m  0.1569\n",
      "     16        1.0577       0.5826        \u001b[35m0.9503\u001b[0m  0.1348\n",
      "     17        1.0545       0.5869        \u001b[35m0.9432\u001b[0m  0.1577\n",
      "     18        1.0441       0.5975        \u001b[35m0.9352\u001b[0m  0.1936\n",
      "     19        1.0290       \u001b[32m0.6081\u001b[0m        \u001b[35m0.9279\u001b[0m  0.1608\n",
      "     20        1.0392       \u001b[32m0.6208\u001b[0m        \u001b[35m0.9217\u001b[0m  0.1277\n",
      "     21        1.0392       \u001b[32m0.6229\u001b[0m        \u001b[35m0.9166\u001b[0m  0.1897\n",
      "     22        1.0406       \u001b[32m0.6398\u001b[0m        \u001b[35m0.9134\u001b[0m  0.2211\n",
      "     23        \u001b[36m1.0091\u001b[0m       0.6377        \u001b[35m0.9127\u001b[0m  0.1754\n",
      "     24        1.0118       0.6314        0.9144  0.1463\n",
      "     25        1.0302       0.6271        0.9188  0.1582\n",
      "     26        1.0361       0.6271        0.9258  0.1279\n",
      "     27        1.0325       0.6271        0.9341  0.1648\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.2094\u001b[0m       \u001b[32m0.3686\u001b[0m        \u001b[35m1.1328\u001b[0m  0.1305\n",
      "      2        \u001b[36m1.1814\u001b[0m       \u001b[32m0.4025\u001b[0m        \u001b[35m1.1043\u001b[0m  0.1800\n",
      "      3        \u001b[36m1.1338\u001b[0m       \u001b[32m0.4534\u001b[0m        \u001b[35m1.0655\u001b[0m  0.2515\n",
      "      4        \u001b[36m1.1138\u001b[0m       \u001b[32m0.4979\u001b[0m        \u001b[35m1.0275\u001b[0m  0.1396\n",
      "      5        \u001b[36m1.0778\u001b[0m       \u001b[32m0.5297\u001b[0m        \u001b[35m1.0035\u001b[0m  0.1301\n",
      "      6        \u001b[36m1.0438\u001b[0m       \u001b[32m0.5593\u001b[0m        \u001b[35m0.9918\u001b[0m  0.1416\n",
      "      7        \u001b[36m1.0425\u001b[0m       \u001b[32m0.5805\u001b[0m        \u001b[35m0.9788\u001b[0m  0.1532\n",
      "      8        \u001b[36m1.0043\u001b[0m       \u001b[32m0.6038\u001b[0m        \u001b[35m0.9645\u001b[0m  0.1706\n",
      "      9        1.0193       \u001b[32m0.6059\u001b[0m        \u001b[35m0.9543\u001b[0m  0.1858\n",
      "     10        1.0168       0.6059        \u001b[35m0.9512\u001b[0m  0.1869\n",
      "     11        1.0272       0.5953        0.9532  0.1366\n",
      "     12        1.0419       0.5932        0.9581  0.1691\n",
      "     13        1.0525       0.5826        0.9636  0.1494\n",
      "     14        1.0529       0.5784        0.9689  0.1875\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.3015\u001b[0m       \u001b[32m0.4068\u001b[0m        \u001b[35m1.1579\u001b[0m  0.1520\n",
      "      2        \u001b[36m1.2680\u001b[0m       \u001b[32m0.4301\u001b[0m        \u001b[35m1.1258\u001b[0m  0.2103\n",
      "      3        \u001b[36m1.2206\u001b[0m       \u001b[32m0.4958\u001b[0m        \u001b[35m1.0816\u001b[0m  0.1336\n",
      "      4        \u001b[36m1.1797\u001b[0m       \u001b[32m0.5572\u001b[0m        \u001b[35m1.0398\u001b[0m  0.2325\n",
      "      5        \u001b[36m1.1025\u001b[0m       \u001b[32m0.5763\u001b[0m        \u001b[35m1.0230\u001b[0m  0.2295\n",
      "      6        \u001b[36m1.0812\u001b[0m       0.5720        \u001b[35m1.0209\u001b[0m  0.1720\n",
      "      7        \u001b[36m1.0765\u001b[0m       \u001b[32m0.5911\u001b[0m        \u001b[35m1.0198\u001b[0m  0.1786\n",
      "      8        \u001b[36m1.0540\u001b[0m       0.5911        \u001b[35m1.0195\u001b[0m  0.1302\n",
      "      9        \u001b[36m1.0391\u001b[0m       \u001b[32m0.6017\u001b[0m        \u001b[35m1.0169\u001b[0m  0.1722\n",
      "     10        1.0413       0.6017        \u001b[35m1.0110\u001b[0m  0.2334\n",
      "     11        \u001b[36m1.0339\u001b[0m       0.5953        \u001b[35m1.0061\u001b[0m  0.2164\n",
      "     12        1.0439       0.5847        \u001b[35m1.0039\u001b[0m  0.2190\n",
      "     13        1.0362       0.5763        \u001b[35m1.0034\u001b[0m  0.1899\n",
      "     14        1.0627       0.5720        1.0042  0.1752\n",
      "     15        1.0843       0.5784        1.0062  0.1548\n",
      "     16        1.0958       0.5720        1.0083  0.1205\n",
      "     17        1.0905       0.5784        1.0097  0.1320\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.2313\u001b[0m       \u001b[32m0.3771\u001b[0m        \u001b[35m1.1366\u001b[0m  0.1366\n",
      "      2        \u001b[36m1.2040\u001b[0m       \u001b[32m0.4004\u001b[0m        \u001b[35m1.1032\u001b[0m  0.1275\n",
      "      3        \u001b[36m1.1494\u001b[0m       \u001b[32m0.4407\u001b[0m        \u001b[35m1.0588\u001b[0m  0.1302\n",
      "      4        \u001b[36m1.1131\u001b[0m       \u001b[32m0.5064\u001b[0m        \u001b[35m1.0169\u001b[0m  0.1306\n",
      "      5        \u001b[36m1.0619\u001b[0m       \u001b[32m0.5678\u001b[0m        \u001b[35m0.9965\u001b[0m  0.1281\n",
      "      6        \u001b[36m1.0406\u001b[0m       \u001b[32m0.5911\u001b[0m        \u001b[35m0.9891\u001b[0m  0.1232\n",
      "      7        \u001b[36m1.0358\u001b[0m       \u001b[32m0.5932\u001b[0m        \u001b[35m0.9830\u001b[0m  0.1232\n",
      "      8        1.0384       \u001b[32m0.5975\u001b[0m        \u001b[35m0.9786\u001b[0m  0.1430\n",
      "      9        \u001b[36m1.0280\u001b[0m       0.5932        \u001b[35m0.9738\u001b[0m  0.1589\n",
      "     10        1.0310       0.5869        \u001b[35m0.9708\u001b[0m  0.1232\n",
      "     11        \u001b[36m1.0190\u001b[0m       0.5763        0.9710  0.1301\n",
      "     12        1.0328       0.5784        0.9741  0.1440\n",
      "     13        1.0643       0.5699        0.9789  0.1391\n",
      "     14        1.0520       0.5657        0.9846  0.1569\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.1289\u001b[0m       \u001b[32m0.4576\u001b[0m        \u001b[35m1.1164\u001b[0m  0.1278\n",
      "      2        \u001b[36m1.1245\u001b[0m       \u001b[32m0.4725\u001b[0m        \u001b[35m1.1088\u001b[0m  0.1371\n",
      "      3        \u001b[36m1.1152\u001b[0m       \u001b[32m0.4979\u001b[0m        \u001b[35m1.0972\u001b[0m  0.1470\n",
      "      4        \u001b[36m1.1020\u001b[0m       \u001b[32m0.5254\u001b[0m        \u001b[35m1.0841\u001b[0m  0.1349\n",
      "      5        \u001b[36m1.0868\u001b[0m       \u001b[32m0.5275\u001b[0m        \u001b[35m1.0718\u001b[0m  0.1406\n",
      "      6        \u001b[36m1.0802\u001b[0m       \u001b[32m0.5614\u001b[0m        \u001b[35m1.0595\u001b[0m  0.1305\n",
      "      7        \u001b[36m1.0685\u001b[0m       \u001b[32m0.5805\u001b[0m        \u001b[35m1.0458\u001b[0m  0.1202\n",
      "      8        \u001b[36m1.0538\u001b[0m       \u001b[32m0.5932\u001b[0m        \u001b[35m1.0326\u001b[0m  0.1490\n",
      "      9        \u001b[36m1.0502\u001b[0m       \u001b[32m0.5975\u001b[0m        \u001b[35m1.0203\u001b[0m  0.1818\n",
      "     10        1.0578       \u001b[32m0.6081\u001b[0m        \u001b[35m1.0097\u001b[0m  0.1987\n",
      "     11        \u001b[36m1.0453\u001b[0m       \u001b[32m0.6144\u001b[0m        \u001b[35m1.0004\u001b[0m  0.2205\n",
      "     12        1.0578       0.6144        \u001b[35m0.9918\u001b[0m  0.2075\n",
      "     13        1.0610       \u001b[32m0.6208\u001b[0m        \u001b[35m0.9848\u001b[0m  0.1401\n",
      "     14        1.0709       0.6165        \u001b[35m0.9794\u001b[0m  0.1381\n",
      "     15        1.0754       0.6144        \u001b[35m0.9755\u001b[0m  0.1420\n",
      "     16        1.0799       0.6038        \u001b[35m0.9720\u001b[0m  0.2146\n",
      "     17        1.0735       0.5996        \u001b[35m0.9699\u001b[0m  0.1496\n",
      "     18        1.0619       0.6059        \u001b[35m0.9687\u001b[0m  0.2005\n",
      "     19        1.0752       0.6038        0.9689  0.1696\n",
      "     20        1.0679       0.6038        0.9703  0.1993\n",
      "     21        1.0585       0.6017        0.9728  0.1707\n",
      "     22        \u001b[36m1.0387\u001b[0m       0.5911        0.9764  0.1458\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.1947\u001b[0m       \u001b[32m0.3475\u001b[0m        \u001b[35m1.1306\u001b[0m  0.1485\n",
      "      2        \u001b[36m1.1659\u001b[0m       \u001b[32m0.3962\u001b[0m        \u001b[35m1.1082\u001b[0m  0.1402\n",
      "      3        \u001b[36m1.1371\u001b[0m       \u001b[32m0.4873\u001b[0m        \u001b[35m1.0788\u001b[0m  0.1607\n",
      "      4        \u001b[36m1.1159\u001b[0m       \u001b[32m0.5487\u001b[0m        \u001b[35m1.0497\u001b[0m  0.1300\n",
      "      5        \u001b[36m1.0910\u001b[0m       \u001b[32m0.5678\u001b[0m        \u001b[35m1.0275\u001b[0m  0.1705\n",
      "      6        \u001b[36m1.0568\u001b[0m       \u001b[32m0.5763\u001b[0m        \u001b[35m1.0113\u001b[0m  0.1386\n",
      "      7        \u001b[36m1.0358\u001b[0m       \u001b[32m0.6038\u001b[0m        \u001b[35m0.9955\u001b[0m  0.1602\n",
      "      8        \u001b[36m1.0331\u001b[0m       \u001b[32m0.6186\u001b[0m        \u001b[35m0.9803\u001b[0m  0.1407\n",
      "      9        1.0554       0.6165        \u001b[35m0.9673\u001b[0m  0.1323\n",
      "     10        \u001b[36m1.0222\u001b[0m       0.6081        \u001b[35m0.9573\u001b[0m  0.1418\n",
      "     11        1.0348       0.6165        \u001b[35m0.9510\u001b[0m  0.1330\n",
      "     12        1.0260       0.6144        \u001b[35m0.9475\u001b[0m  0.1401\n",
      "     13        1.0291       0.6123        \u001b[35m0.9458\u001b[0m  0.1407\n",
      "     14        1.0521       0.6059        0.9468  0.1311\n",
      "     15        1.0683       0.5996        0.9498  0.1298\n",
      "     16        1.0869       0.5953        0.9527  0.2113\n",
      "     17        1.0630       0.5996        0.9563  0.1297\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.1918\u001b[0m       \u001b[32m0.4131\u001b[0m        \u001b[35m1.1333\u001b[0m  0.1531\n",
      "      2        \u001b[36m1.1804\u001b[0m       \u001b[32m0.4597\u001b[0m        \u001b[35m1.1139\u001b[0m  0.1401\n",
      "      3        \u001b[36m1.1438\u001b[0m       \u001b[32m0.4958\u001b[0m        \u001b[35m1.0882\u001b[0m  0.1404\n",
      "      4        \u001b[36m1.1090\u001b[0m       \u001b[32m0.5233\u001b[0m        \u001b[35m1.0657\u001b[0m  0.1417\n",
      "      5        \u001b[36m1.0852\u001b[0m       \u001b[32m0.5339\u001b[0m        \u001b[35m1.0543\u001b[0m  0.1298\n",
      "      6        \u001b[36m1.0625\u001b[0m       \u001b[32m0.5508\u001b[0m        \u001b[35m1.0468\u001b[0m  0.1584\n",
      "      7        \u001b[36m1.0557\u001b[0m       \u001b[32m0.5826\u001b[0m        \u001b[35m1.0397\u001b[0m  0.1300\n",
      "      8        \u001b[36m1.0401\u001b[0m       \u001b[32m0.5975\u001b[0m        \u001b[35m1.0345\u001b[0m  0.1604\n",
      "      9        1.0497       0.5911        \u001b[35m1.0302\u001b[0m  0.1605\n",
      "     10        1.0511       0.5847        \u001b[35m1.0279\u001b[0m  0.1311\n",
      "     11        1.0634       0.5699        \u001b[35m1.0251\u001b[0m  0.1480\n",
      "     12        1.0771       0.5678        \u001b[35m1.0219\u001b[0m  0.1302\n",
      "     13        1.0844       0.5699        \u001b[35m1.0169\u001b[0m  0.1602\n",
      "     14        1.0884       0.5699        \u001b[35m1.0088\u001b[0m  0.1203\n",
      "     15        1.0899       0.5763        \u001b[35m0.9988\u001b[0m  0.1588\n",
      "     16        1.0894       0.5826        \u001b[35m0.9866\u001b[0m  0.1381\n",
      "     17        1.0562       0.5890        \u001b[35m0.9732\u001b[0m  0.1305\n",
      "     18        1.0428       \u001b[32m0.6038\u001b[0m        \u001b[35m0.9605\u001b[0m  0.1497\n",
      "     19        1.0454       0.5996        \u001b[35m0.9506\u001b[0m  0.1302\n",
      "     20        1.0462       0.6017        \u001b[35m0.9442\u001b[0m  0.1609\n",
      "     21        1.0589       0.6017        \u001b[35m0.9411\u001b[0m  0.1299\n",
      "     22        1.0433       \u001b[32m0.6144\u001b[0m        \u001b[35m0.9399\u001b[0m  0.1415\n",
      "     23        1.0642       0.6123        0.9407  0.1625\n",
      "     24        1.0792       \u001b[32m0.6208\u001b[0m        0.9428  0.1301\n",
      "     25        1.0812       0.6208        0.9462  0.1724\n",
      "     26        1.0661       0.6208        0.9523  0.1316\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.2411\u001b[0m       \u001b[32m0.3729\u001b[0m        \u001b[35m1.1745\u001b[0m  0.1412\n",
      "      2        \u001b[36m1.1968\u001b[0m       \u001b[32m0.3962\u001b[0m        \u001b[35m1.1474\u001b[0m  0.1296\n",
      "      3        \u001b[36m1.1760\u001b[0m       \u001b[32m0.4470\u001b[0m        \u001b[35m1.1108\u001b[0m  0.1283\n",
      "      4        \u001b[36m1.1321\u001b[0m       \u001b[32m0.5042\u001b[0m        \u001b[35m1.0758\u001b[0m  0.1710\n",
      "      5        \u001b[36m1.0973\u001b[0m       \u001b[32m0.5530\u001b[0m        \u001b[35m1.0567\u001b[0m  0.1199\n",
      "      6        \u001b[36m1.0766\u001b[0m       \u001b[32m0.5699\u001b[0m        \u001b[35m1.0510\u001b[0m  0.1412\n",
      "      7        1.0851       \u001b[32m0.5763\u001b[0m        \u001b[35m1.0469\u001b[0m  0.1472\n",
      "      8        \u001b[36m1.0729\u001b[0m       \u001b[32m0.5975\u001b[0m        \u001b[35m1.0446\u001b[0m  0.1231\n",
      "      9        1.0744       0.5953        \u001b[35m1.0433\u001b[0m  0.1682\n",
      "     10        \u001b[36m1.0689\u001b[0m       \u001b[32m0.6038\u001b[0m        \u001b[35m1.0430\u001b[0m  0.1524\n",
      "     11        1.0791       0.5975        1.0437  0.1378\n",
      "     12        1.0826       0.5742        1.0441  0.1603\n",
      "     13        1.0821       0.5763        1.0440  0.1277\n",
      "     14        1.0879       0.5763        1.0437  0.1384\n",
      "     15        1.0923       0.5784        \u001b[35m1.0425\u001b[0m  0.1654\n",
      "     16        1.0909       0.5911        \u001b[35m1.0410\u001b[0m  0.1298\n",
      "     17        1.1064       0.5847        \u001b[35m1.0391\u001b[0m  0.1378\n",
      "     18        1.1104       0.5847        \u001b[35m1.0365\u001b[0m  0.1702\n",
      "     19        1.1011       0.5742        \u001b[35m1.0327\u001b[0m  0.1379\n",
      "     20        1.0986       0.5699        \u001b[35m1.0278\u001b[0m  0.1707\n",
      "     21        1.0828       0.5805        \u001b[35m1.0232\u001b[0m  0.1507\n",
      "     22        1.0933       0.5805        \u001b[35m1.0196\u001b[0m  0.1211\n",
      "     23        1.0796       0.5847        \u001b[35m1.0162\u001b[0m  0.1307\n",
      "     24        1.0971       0.5847        \u001b[35m1.0137\u001b[0m  0.1307\n",
      "     25        1.1010       0.5826        \u001b[35m1.0113\u001b[0m  0.1236\n",
      "     26        1.1032       0.5784        \u001b[35m1.0095\u001b[0m  0.1202\n",
      "     27        1.0870       0.5805        \u001b[35m1.0081\u001b[0m  0.1206\n",
      "     28        1.0795       0.5826        \u001b[35m1.0057\u001b[0m  0.1281\n",
      "     29        \u001b[36m1.0620\u001b[0m       0.5869        \u001b[35m1.0041\u001b[0m  0.1305\n",
      "     30        1.0646       0.5953        \u001b[35m1.0015\u001b[0m  0.1221\n",
      "     31        \u001b[36m1.0510\u001b[0m       0.5975        \u001b[35m0.9981\u001b[0m  0.1307\n",
      "     32        \u001b[36m1.0493\u001b[0m       0.5996        \u001b[35m0.9935\u001b[0m  0.1225\n",
      "     33        \u001b[36m1.0283\u001b[0m       0.5996        \u001b[35m0.9880\u001b[0m  0.1199\n",
      "     34        \u001b[36m1.0254\u001b[0m       \u001b[32m0.6059\u001b[0m        \u001b[35m0.9835\u001b[0m  0.1301\n",
      "     35        1.0431       \u001b[32m0.6144\u001b[0m        \u001b[35m0.9811\u001b[0m  0.1307\n",
      "     36        1.0312       \u001b[32m0.6186\u001b[0m        \u001b[35m0.9799\u001b[0m  0.1198\n",
      "     37        1.0362       0.6186        0.9801  0.1310\n",
      "     38        1.0578       \u001b[32m0.6208\u001b[0m        0.9815  0.1230\n",
      "     39        1.0470       0.6208        0.9852  0.1208\n",
      "     40        1.0585       0.6123        0.9887  0.1318\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.1856\u001b[0m       \u001b[32m0.3453\u001b[0m        \u001b[35m1.1290\u001b[0m  0.1329\n",
      "      2        \u001b[36m1.1561\u001b[0m       \u001b[32m0.3665\u001b[0m        \u001b[35m1.1099\u001b[0m  0.1300\n",
      "      3        \u001b[36m1.1288\u001b[0m       \u001b[32m0.4301\u001b[0m        \u001b[35m1.0852\u001b[0m  0.1381\n",
      "      4        \u001b[36m1.1018\u001b[0m       \u001b[32m0.5000\u001b[0m        \u001b[35m1.0648\u001b[0m  0.1203\n",
      "      5        \u001b[36m1.0820\u001b[0m       \u001b[32m0.5551\u001b[0m        \u001b[35m1.0556\u001b[0m  0.1586\n",
      "      6        \u001b[36m1.0775\u001b[0m       \u001b[32m0.5720\u001b[0m        \u001b[35m1.0520\u001b[0m  0.1587\n",
      "      7        \u001b[36m1.0637\u001b[0m       \u001b[32m0.5869\u001b[0m        \u001b[35m1.0489\u001b[0m  0.1413\n",
      "      8        1.0698       0.5784        \u001b[35m1.0442\u001b[0m  0.1512\n",
      "      9        1.0674       0.5869        \u001b[35m1.0387\u001b[0m  0.1586\n",
      "     10        \u001b[36m1.0529\u001b[0m       \u001b[32m0.5996\u001b[0m        \u001b[35m1.0339\u001b[0m  0.1389\n",
      "     11        1.0694       0.5932        \u001b[35m1.0299\u001b[0m  0.1609\n",
      "     12        1.0661       0.5932        \u001b[35m1.0268\u001b[0m  0.1606\n",
      "     13        1.0852       0.5890        \u001b[35m1.0245\u001b[0m  0.2617\n",
      "     14        1.0778       0.5826        \u001b[35m1.0226\u001b[0m  0.1399\n",
      "     15        1.0829       0.5805        \u001b[35m1.0201\u001b[0m  0.1609\n",
      "     16        1.0790       0.5720        \u001b[35m1.0157\u001b[0m  0.1606\n",
      "     17        1.0668       0.5636        \u001b[35m1.0103\u001b[0m  0.1321\n",
      "     18        1.0558       0.5551        \u001b[35m1.0042\u001b[0m  0.1607\n",
      "     19        1.0540       0.5551        \u001b[35m0.9957\u001b[0m  0.1505\n",
      "     20        1.0859       0.5487        \u001b[35m0.9863\u001b[0m  0.1407\n",
      "     21        \u001b[36m1.0424\u001b[0m       0.5636        \u001b[35m0.9768\u001b[0m  0.1605\n",
      "     22        1.0603       0.5784        \u001b[35m0.9673\u001b[0m  0.1385\n",
      "     23        1.0586       0.5826        \u001b[35m0.9595\u001b[0m  0.1708\n",
      "     24        1.0675       0.5742        \u001b[35m0.9537\u001b[0m  0.1505\n",
      "     25        1.0565       0.5720        \u001b[35m0.9508\u001b[0m  0.1505\n",
      "     26        1.0563       0.5826        \u001b[35m0.9494\u001b[0m  0.1390\n",
      "     27        1.0562       0.5911        \u001b[35m0.9492\u001b[0m  0.1305\n",
      "     28        1.0432       0.5932        0.9493  0.1402\n",
      "     29        1.0643       \u001b[32m0.6038\u001b[0m        0.9509  0.1222\n",
      "     30        1.0662       \u001b[32m0.6186\u001b[0m        0.9545  0.1308\n",
      "     31        1.0543       0.6144        0.9594  0.1402\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.1727\u001b[0m       \u001b[32m0.3729\u001b[0m        \u001b[35m1.1164\u001b[0m  0.1333\n",
      "      2        \u001b[36m1.1599\u001b[0m       \u001b[32m0.4068\u001b[0m        \u001b[35m1.0901\u001b[0m  0.1325\n",
      "      3        \u001b[36m1.1241\u001b[0m       \u001b[32m0.4767\u001b[0m        \u001b[35m1.0567\u001b[0m  0.1381\n",
      "      4        \u001b[36m1.0722\u001b[0m       \u001b[32m0.5593\u001b[0m        \u001b[35m1.0329\u001b[0m  0.1302\n",
      "      5        \u001b[36m1.0597\u001b[0m       \u001b[32m0.5847\u001b[0m        \u001b[35m1.0266\u001b[0m  0.1396\n",
      "      6        \u001b[36m1.0551\u001b[0m       \u001b[32m0.5911\u001b[0m        \u001b[35m1.0208\u001b[0m  0.1325\n",
      "      7        \u001b[36m1.0474\u001b[0m       \u001b[32m0.6038\u001b[0m        \u001b[35m1.0140\u001b[0m  0.1425\n",
      "      8        \u001b[36m1.0417\u001b[0m       \u001b[32m0.6102\u001b[0m        \u001b[35m1.0051\u001b[0m  0.1313\n",
      "      9        \u001b[36m1.0186\u001b[0m       \u001b[32m0.6144\u001b[0m        \u001b[35m0.9956\u001b[0m  0.1406\n",
      "     10        1.0344       \u001b[32m0.6165\u001b[0m        \u001b[35m0.9845\u001b[0m  0.1316\n",
      "     11        1.0249       0.6123        \u001b[35m0.9730\u001b[0m  0.1405\n",
      "     12        1.0228       0.6102        \u001b[35m0.9636\u001b[0m  0.1405\n",
      "     13        1.0435       0.6081        \u001b[35m0.9564\u001b[0m  0.1414\n",
      "     14        1.0513       0.6017        \u001b[35m0.9516\u001b[0m  0.1384\n",
      "     15        1.0438       0.5996        \u001b[35m0.9481\u001b[0m  0.1498\n",
      "     16        1.0641       0.6038        \u001b[35m0.9448\u001b[0m  0.1381\n",
      "     17        1.0541       0.6059        \u001b[35m0.9408\u001b[0m  0.1601\n",
      "     18        1.0753       0.6059        \u001b[35m0.9368\u001b[0m  0.1327\n",
      "     19        1.0695       0.6144        \u001b[35m0.9331\u001b[0m  0.1711\n",
      "     20        1.0592       0.6123        \u001b[35m0.9311\u001b[0m  0.1299\n",
      "     21        1.0493       \u001b[32m0.6335\u001b[0m        0.9317  0.1482\n",
      "     22        1.0515       \u001b[32m0.6356\u001b[0m        0.9353  0.1607\n",
      "     23        1.0371       0.6165        0.9422  0.2546\n",
      "     24        1.0376       0.6102        0.9522  0.1388\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.2133\u001b[0m       \u001b[32m0.4640\u001b[0m        \u001b[35m1.0480\u001b[0m  0.1132\n",
      "      2        \u001b[36m1.0518\u001b[0m       \u001b[32m0.6144\u001b[0m        \u001b[35m1.0044\u001b[0m  0.1400\n",
      "      3        \u001b[36m1.0347\u001b[0m       \u001b[32m0.6208\u001b[0m        \u001b[35m0.9781\u001b[0m  0.1498\n",
      "      4        1.0420       0.6059        \u001b[35m0.9588\u001b[0m  0.1305\n",
      "      5        1.0674       0.6038        \u001b[35m0.9564\u001b[0m  0.1604\n",
      "      6        1.0700       0.6123        \u001b[35m0.9556\u001b[0m  0.1280\n",
      "      7        1.0591       0.6144        0.9566  0.1608\n",
      "      8        1.0394       \u001b[32m0.6271\u001b[0m        0.9686  0.1420\n",
      "      9        \u001b[36m1.0330\u001b[0m       0.6271        0.9753  0.1405\n",
      "     10        1.0590       0.6208        0.9809  0.1819\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.1112\u001b[0m       \u001b[32m0.4555\u001b[0m        \u001b[35m1.0480\u001b[0m  0.1539\n",
      "      2        \u001b[36m1.0563\u001b[0m       \u001b[32m0.5784\u001b[0m        \u001b[35m1.0086\u001b[0m  0.1198\n",
      "      3        \u001b[36m1.0400\u001b[0m       \u001b[32m0.6250\u001b[0m        \u001b[35m0.9835\u001b[0m  0.1679\n",
      "      4        1.0411       0.6102        \u001b[35m0.9484\u001b[0m  0.1396\n",
      "      5        \u001b[36m1.0336\u001b[0m       0.6081        \u001b[35m0.9260\u001b[0m  0.1303\n",
      "      6        1.0556       0.5720        0.9445  0.1601\n",
      "      7        1.0664       0.5826        0.9655  0.1276\n",
      "      8        1.0573       0.5720        0.9755  0.1688\n",
      "      9        1.0519       0.6017        0.9806  0.1434\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.1579\u001b[0m       \u001b[32m0.4767\u001b[0m        \u001b[35m1.0325\u001b[0m  0.1532\n",
      "      2        \u001b[36m1.0518\u001b[0m       \u001b[32m0.5763\u001b[0m        \u001b[35m0.9936\u001b[0m  0.1251\n",
      "      3        \u001b[36m1.0272\u001b[0m       \u001b[32m0.5847\u001b[0m        \u001b[35m0.9717\u001b[0m  0.1599\n",
      "      4        \u001b[36m1.0233\u001b[0m       \u001b[32m0.5975\u001b[0m        \u001b[35m0.9438\u001b[0m  0.1408\n",
      "      5        1.0327       \u001b[32m0.5996\u001b[0m        \u001b[35m0.9208\u001b[0m  0.1299\n",
      "      6        1.0620       \u001b[32m0.6165\u001b[0m        \u001b[35m0.9166\u001b[0m  0.1701\n",
      "      7        1.0699       0.6038        0.9352  0.1199\n",
      "      8        1.0393       0.5911        0.9634  0.1991\n",
      "      9        1.0351       0.5869        0.9805  0.1937\n",
      "     10        1.0480       0.6017        0.9876  0.1277\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.1765\u001b[0m       \u001b[32m0.4788\u001b[0m        \u001b[35m1.0933\u001b[0m  0.1252\n",
      "      2        \u001b[36m1.0982\u001b[0m       \u001b[32m0.5847\u001b[0m        \u001b[35m1.0591\u001b[0m  0.1207\n",
      "      3        \u001b[36m1.0758\u001b[0m       \u001b[32m0.6123\u001b[0m        \u001b[35m1.0499\u001b[0m  0.1202\n",
      "      4        \u001b[36m1.0666\u001b[0m       0.5869        \u001b[35m1.0425\u001b[0m  0.1200\n",
      "      5        1.0819       0.5763        \u001b[35m1.0329\u001b[0m  0.1209\n",
      "      6        1.0915       0.5657        \u001b[35m1.0133\u001b[0m  0.1209\n",
      "      7        1.0806       0.5551        \u001b[35m0.9845\u001b[0m  0.1204\n",
      "      8        \u001b[36m1.0476\u001b[0m       0.5805        \u001b[35m0.9733\u001b[0m  0.1207\n",
      "      9        1.0715       0.5805        0.9873  0.1203\n",
      "     10        1.0802       0.5953        0.9961  0.1201\n",
      "     11        1.0545       0.6123        1.0019  0.1298\n",
      "     12        \u001b[36m1.0336\u001b[0m       0.6123        1.0057  0.1305\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.2071\u001b[0m       \u001b[32m0.4597\u001b[0m        \u001b[35m1.0879\u001b[0m  0.1234\n",
      "      2        \u001b[36m1.0686\u001b[0m       \u001b[32m0.5720\u001b[0m        \u001b[35m1.0331\u001b[0m  0.1296\n",
      "      3        \u001b[36m1.0549\u001b[0m       \u001b[32m0.5869\u001b[0m        \u001b[35m1.0115\u001b[0m  0.1225\n",
      "      4        \u001b[36m1.0511\u001b[0m       0.5805        \u001b[35m1.0020\u001b[0m  0.1203\n",
      "      5        1.0637       \u001b[32m0.5953\u001b[0m        \u001b[35m0.9917\u001b[0m  0.1407\n",
      "      6        1.0594       0.5932        \u001b[35m0.9855\u001b[0m  0.1204\n",
      "      7        1.0695       \u001b[32m0.5996\u001b[0m        0.9866  0.1407\n",
      "      8        1.0580       0.5996        \u001b[35m0.9837\u001b[0m  0.1605\n",
      "      9        1.0696       \u001b[32m0.6144\u001b[0m        \u001b[35m0.9805\u001b[0m  0.1486\n",
      "     10        \u001b[36m1.0457\u001b[0m       0.6081        0.9817  0.1489\n",
      "     11        1.0517       0.6017        0.9927  0.1406\n",
      "     12        \u001b[36m1.0397\u001b[0m       0.5763        1.0113  0.1511\n",
      "     13        1.0507       0.5572        1.0213  0.1515\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.2340\u001b[0m       \u001b[32m0.4386\u001b[0m        \u001b[35m1.0760\u001b[0m  0.1605\n",
      "      2        \u001b[36m1.0866\u001b[0m       \u001b[32m0.5763\u001b[0m        \u001b[35m1.0151\u001b[0m  0.1505\n",
      "      3        \u001b[36m1.0493\u001b[0m       \u001b[32m0.6165\u001b[0m        \u001b[35m0.9947\u001b[0m  0.1405\n",
      "      4        1.0545       0.6144        \u001b[35m0.9793\u001b[0m  0.1642\n",
      "      5        1.0682       0.5953        \u001b[35m0.9646\u001b[0m  0.1583\n",
      "      6        1.0802       0.5911        \u001b[35m0.9603\u001b[0m  0.1509\n",
      "      7        1.0767       0.5826        0.9609  0.1782\n",
      "      8        1.0748       0.5932        0.9673  0.1432\n",
      "      9        1.0563       0.6165        0.9884  0.1699\n",
      "     10        1.0900       \u001b[32m0.6271\u001b[0m        1.0177  0.1837\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.1967\u001b[0m       \u001b[32m0.4280\u001b[0m        \u001b[35m1.0794\u001b[0m  0.1607\n",
      "      2        \u001b[36m1.0822\u001b[0m       \u001b[32m0.5530\u001b[0m        \u001b[35m1.0256\u001b[0m  0.1914\n",
      "      3        \u001b[36m1.0611\u001b[0m       \u001b[32m0.5869\u001b[0m        \u001b[35m1.0129\u001b[0m  0.1428\n",
      "      4        \u001b[36m1.0569\u001b[0m       0.5720        \u001b[35m0.9934\u001b[0m  0.1299\n",
      "      5        1.0624       0.5805        \u001b[35m0.9693\u001b[0m  0.1662\n",
      "      6        \u001b[36m1.0536\u001b[0m       \u001b[32m0.5932\u001b[0m        \u001b[35m0.9547\u001b[0m  0.1225\n",
      "      7        1.0883       \u001b[32m0.6059\u001b[0m        \u001b[35m0.9466\u001b[0m  0.1882\n",
      "      8        1.0625       \u001b[32m0.6081\u001b[0m        0.9543  0.1532\n",
      "      9        1.0758       0.6059        0.9715  0.1380\n",
      "     10        1.0715       \u001b[32m0.6250\u001b[0m        0.9946  0.1479\n",
      "     11        1.0838       0.6208        1.0194  0.1290\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.1738\u001b[0m       \u001b[32m0.4576\u001b[0m        \u001b[35m1.0572\u001b[0m  0.1417\n",
      "      2        \u001b[36m1.0798\u001b[0m       \u001b[32m0.5657\u001b[0m        \u001b[35m1.0273\u001b[0m  0.1414\n",
      "      3        \u001b[36m1.0527\u001b[0m       0.5572        \u001b[35m1.0161\u001b[0m  0.1517\n",
      "      4        1.0953       0.5593        \u001b[35m1.0124\u001b[0m  0.1460\n",
      "      5        1.0857       0.5657        \u001b[35m1.0032\u001b[0m  0.1321\n",
      "      6        1.0778       \u001b[32m0.5911\u001b[0m        \u001b[35m0.9939\u001b[0m  0.1252\n",
      "      7        \u001b[36m1.0478\u001b[0m       \u001b[32m0.6144\u001b[0m        \u001b[35m0.9883\u001b[0m  0.1261\n",
      "      8        \u001b[36m1.0435\u001b[0m       \u001b[32m0.6208\u001b[0m        0.9956  0.1242\n",
      "      9        \u001b[36m1.0331\u001b[0m       0.6186        0.9955  0.1261\n",
      "     10        1.0461       0.6186        \u001b[35m0.9877\u001b[0m  0.1281\n",
      "     11        1.0535       0.6144        0.9880  0.1331\n",
      "     12        1.0535       0.6186        0.9991  0.1438\n",
      "     13        1.0443       0.6144        1.0260  0.1492\n",
      "     14        1.0459       0.6123        1.0443  0.1268\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.1989\u001b[0m       \u001b[32m0.4852\u001b[0m        \u001b[35m1.0454\u001b[0m  0.1218\n",
      "      2        \u001b[36m1.0686\u001b[0m       \u001b[32m0.5890\u001b[0m        \u001b[35m1.0138\u001b[0m  0.1387\n",
      "      3        \u001b[36m1.0563\u001b[0m       \u001b[32m0.5953\u001b[0m        \u001b[35m1.0103\u001b[0m  0.1239\n",
      "      4        1.0752       0.5720        \u001b[35m1.0064\u001b[0m  0.1312\n",
      "      5        1.0690       0.5699        1.0086  0.1624\n",
      "      6        1.1004       0.5699        1.0195  0.1297\n",
      "      7        1.1015       0.5699        1.0288  0.1412\n",
      "      8        1.0954       0.5805        1.0328  0.1416\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.1843\u001b[0m       \u001b[32m0.5169\u001b[0m        \u001b[35m1.0518\u001b[0m  0.1521\n",
      "      2        \u001b[36m1.0694\u001b[0m       \u001b[32m0.5932\u001b[0m        \u001b[35m1.0091\u001b[0m  0.1315\n",
      "      3        \u001b[36m1.0349\u001b[0m       \u001b[32m0.6144\u001b[0m        \u001b[35m0.9822\u001b[0m  0.1415\n",
      "      4        \u001b[36m1.0282\u001b[0m       0.6081        \u001b[35m0.9678\u001b[0m  0.1375\n",
      "      5        1.0552       0.5678        0.9762  0.1383\n",
      "      6        1.0926       0.5826        0.9728  0.1470\n",
      "      7        1.0701       0.5847        \u001b[35m0.9675\u001b[0m  0.1363\n",
      "      8        1.0593       0.6102        0.9808  0.1210\n",
      "      9        1.0496       \u001b[32m0.6250\u001b[0m        0.9968  0.1207\n",
      "     10        1.0545       0.6102        1.0027  0.1211\n",
      "     11        1.0571       0.6208        0.9956  0.1292\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.2374\u001b[0m       \u001b[32m0.4703\u001b[0m        \u001b[35m1.0683\u001b[0m  0.1162\n",
      "      2        \u001b[36m1.0729\u001b[0m       \u001b[32m0.6102\u001b[0m        \u001b[35m0.9912\u001b[0m  0.1298\n",
      "      3        \u001b[36m1.0468\u001b[0m       0.5720        \u001b[35m0.9762\u001b[0m  0.1431\n",
      "      4        \u001b[36m1.0412\u001b[0m       0.5805        \u001b[35m0.9731\u001b[0m  0.2560\n",
      "      5        1.0731       0.5678        0.9757  0.1707\n",
      "      6        1.0652       0.5826        0.9734  0.1603\n",
      "      7        1.0612       0.5932        0.9732  0.1559\n",
      "      8        1.0650       0.5890        0.9876  0.1608\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.2437\u001b[0m       \u001b[32m0.3729\u001b[0m        \u001b[35m1.1145\u001b[0m  0.1616\n",
      "      2        \u001b[36m1.0948\u001b[0m       \u001b[32m0.5826\u001b[0m        \u001b[35m1.0459\u001b[0m  0.1527\n",
      "      3        \u001b[36m1.0641\u001b[0m       0.5826        \u001b[35m1.0316\u001b[0m  0.1415\n",
      "      4        1.0827       0.5551        \u001b[35m1.0264\u001b[0m  0.1333\n",
      "      5        1.1008       0.5508        \u001b[35m1.0238\u001b[0m  0.1310\n",
      "      6        1.0882       0.5572        \u001b[35m1.0178\u001b[0m  0.1722\n",
      "      7        1.0920       0.5657        \u001b[35m1.0130\u001b[0m  0.1422\n",
      "      8        1.0871       0.5742        \u001b[35m1.0101\u001b[0m  0.1418\n",
      "      9        1.0846       \u001b[32m0.5953\u001b[0m        1.0206  0.1315\n",
      "     10        1.0797       \u001b[32m0.6059\u001b[0m        1.0358  0.1448\n",
      "     11        1.0824       0.5953        1.0459  0.1436\n",
      "     12        1.0842       0.6059        1.0560  0.1208\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.2317\u001b[0m       \u001b[32m0.5106\u001b[0m        \u001b[35m1.0520\u001b[0m  0.1311\n",
      "      2        \u001b[36m1.0838\u001b[0m       \u001b[32m0.5996\u001b[0m        \u001b[35m0.9816\u001b[0m  0.1195\n",
      "      3        \u001b[36m1.0512\u001b[0m       0.5572        0.9848  0.1199\n",
      "      4        1.0729       0.5572        0.9950  0.1291\n",
      "      5        1.0776       0.5466        1.0081  0.1263\n",
      "      6        1.0846       0.5508        1.0102  0.1108\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.1729\u001b[0m       \u001b[32m0.4364\u001b[0m        \u001b[35m1.0608\u001b[0m  0.1227\n",
      "      2        \u001b[36m1.0615\u001b[0m       \u001b[32m0.6081\u001b[0m        \u001b[35m1.0164\u001b[0m  0.1131\n",
      "      3        \u001b[36m1.0279\u001b[0m       0.5953        \u001b[35m0.9940\u001b[0m  0.1340\n",
      "      4        1.0618       0.5826        \u001b[35m0.9813\u001b[0m  0.1419\n",
      "      5        1.0583       0.5720        \u001b[35m0.9700\u001b[0m  0.1828\n",
      "      6        1.0710       0.5996        \u001b[35m0.9555\u001b[0m  0.1175\n",
      "      7        1.0750       \u001b[32m0.6102\u001b[0m        \u001b[35m0.9530\u001b[0m  0.1210\n",
      "      8        1.0572       \u001b[32m0.6165\u001b[0m        0.9731  0.1228\n",
      "      9        1.0524       0.6059        1.0011  0.1211\n",
      "     10        1.0535       0.6059        1.0266  0.1315\n",
      "     11        1.0504       0.6059        1.0437  0.1414\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.1372\u001b[0m       \u001b[32m0.5000\u001b[0m        \u001b[35m1.0384\u001b[0m  0.1334\n",
      "      2        \u001b[36m1.0398\u001b[0m       \u001b[32m0.6165\u001b[0m        \u001b[35m0.9750\u001b[0m  0.1556\n",
      "      3        1.0402       0.6081        \u001b[35m0.9542\u001b[0m  0.1369\n",
      "      4        1.0460       0.5932        \u001b[35m0.9476\u001b[0m  0.1635\n",
      "      5        1.0635       0.5869        0.9561  0.1394\n",
      "      6        1.0783       0.5742        0.9741  0.1528\n",
      "      7        1.0598       0.5932        0.9862  0.1828\n",
      "      8        1.0577       \u001b[32m0.6419\u001b[0m        0.9840  0.1337\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.2316\u001b[0m       \u001b[32m0.4703\u001b[0m        \u001b[35m1.0730\u001b[0m  0.1364\n",
      "      2        \u001b[36m1.0724\u001b[0m       \u001b[32m0.5784\u001b[0m        \u001b[35m1.0130\u001b[0m  0.1687\n",
      "      3        \u001b[36m1.0385\u001b[0m       \u001b[32m0.5911\u001b[0m        \u001b[35m0.9903\u001b[0m  0.2062\n",
      "      4        \u001b[36m1.0378\u001b[0m       0.5890        \u001b[35m0.9591\u001b[0m  0.1334\n",
      "      5        1.0835       \u001b[32m0.5975\u001b[0m        0.9613  0.1540\n",
      "      6        1.0951       0.5847        0.9750  0.1415\n",
      "      7        1.0718       0.5975        0.9897  0.1347\n",
      "      8        1.0468       0.5953        1.0158  0.1418\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.1448\u001b[0m       \u001b[32m0.5191\u001b[0m        \u001b[35m1.0467\u001b[0m  0.3129\n",
      "      2        \u001b[36m1.0753\u001b[0m       \u001b[32m0.6102\u001b[0m        \u001b[35m1.0043\u001b[0m  0.2449\n",
      "      3        \u001b[36m1.0392\u001b[0m       0.6081        \u001b[35m0.9765\u001b[0m  0.1311\n",
      "      4        1.0434       0.6017        \u001b[35m0.9490\u001b[0m  0.1235\n",
      "      5        1.0494       \u001b[32m0.6123\u001b[0m        0.9508  0.1491\n",
      "      6        1.0858       0.6038        0.9599  0.1437\n",
      "      7        1.0764       0.5996        0.9725  0.1124\n",
      "      8        1.0585       0.6081        0.9972  0.1239\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.1822\u001b[0m       \u001b[32m0.4746\u001b[0m        \u001b[35m1.0692\u001b[0m  0.1139\n",
      "      2        \u001b[36m1.0834\u001b[0m       \u001b[32m0.5530\u001b[0m        \u001b[35m1.0399\u001b[0m  0.1210\n",
      "      3        \u001b[36m1.0570\u001b[0m       \u001b[32m0.5805\u001b[0m        \u001b[35m1.0325\u001b[0m  0.1217\n",
      "      4        \u001b[36m1.0375\u001b[0m       0.5784        \u001b[35m1.0103\u001b[0m  0.1239\n",
      "      5        1.0786       0.5805        \u001b[35m0.9853\u001b[0m  0.1375\n",
      "      6        1.0817       \u001b[32m0.5847\u001b[0m        \u001b[35m0.9697\u001b[0m  0.1444\n",
      "      7        1.0809       0.5805        \u001b[35m0.9640\u001b[0m  0.1225\n",
      "      8        1.0693       \u001b[32m0.5953\u001b[0m        0.9693  0.1207\n",
      "      9        1.0753       \u001b[32m0.6144\u001b[0m        0.9972  0.1189\n",
      "     10        1.0579       \u001b[32m0.6250\u001b[0m        1.0332  0.1209\n",
      "     11        1.0767       0.6250        1.0461  0.1208\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.1073\u001b[0m       \u001b[32m0.5445\u001b[0m        \u001b[35m1.0400\u001b[0m  0.1206\n",
      "      2        \u001b[36m1.0585\u001b[0m       \u001b[32m0.5805\u001b[0m        \u001b[35m1.0201\u001b[0m  0.1208\n",
      "      3        \u001b[36m1.0408\u001b[0m       \u001b[32m0.5996\u001b[0m        \u001b[35m0.9868\u001b[0m  0.1392\n",
      "      4        \u001b[36m1.0152\u001b[0m       \u001b[32m0.6123\u001b[0m        \u001b[35m0.9496\u001b[0m  0.1314\n",
      "      5        1.0379       \u001b[32m0.6250\u001b[0m        \u001b[35m0.9315\u001b[0m  0.1188\n",
      "      6        1.0849       0.6229        \u001b[35m0.9177\u001b[0m  0.1204\n",
      "      7        1.0614       0.6102        \u001b[35m0.9160\u001b[0m  0.1131\n",
      "      8        1.0357       0.6186        0.9574  0.1185\n",
      "      9        1.0345       0.5911        1.0107  0.1220\n",
      "     10        1.0647       0.5932        1.0348  0.1214\n",
      "     11        1.0700       0.6038        1.0403  0.1126\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.1786\u001b[0m       \u001b[32m0.4979\u001b[0m        \u001b[35m1.0593\u001b[0m  0.1132\n",
      "      2        \u001b[36m1.0893\u001b[0m       \u001b[32m0.5847\u001b[0m        \u001b[35m1.0245\u001b[0m  0.1317\n",
      "      3        \u001b[36m1.0550\u001b[0m       \u001b[32m0.6208\u001b[0m        \u001b[35m1.0017\u001b[0m  0.1335\n",
      "      4        \u001b[36m1.0489\u001b[0m       0.6081        \u001b[35m0.9764\u001b[0m  0.1184\n",
      "      5        1.0575       0.5890        \u001b[35m0.9673\u001b[0m  0.1212\n",
      "      6        1.0613       0.6017        \u001b[35m0.9513\u001b[0m  0.1207\n",
      "      7        1.0550       0.6017        \u001b[35m0.9429\u001b[0m  0.1214\n",
      "      8        1.0598       0.6208        0.9482  0.1207\n",
      "      9        \u001b[36m1.0483\u001b[0m       \u001b[32m0.6229\u001b[0m        0.9640  0.1211\n",
      "     10        \u001b[36m1.0478\u001b[0m       \u001b[32m0.6292\u001b[0m        0.9804  0.1210\n",
      "     11        1.0727       0.6229        0.9958  0.1238\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.0870\u001b[0m       \u001b[32m0.6017\u001b[0m        \u001b[35m0.9718\u001b[0m  0.1209\n",
      "      2        1.0970       \u001b[32m0.6208\u001b[0m        \u001b[35m0.9599\u001b[0m  0.1228\n",
      "      3        \u001b[36m1.0692\u001b[0m       \u001b[32m0.6271\u001b[0m        1.0280  0.1124\n",
      "      4        1.1075       0.6208        1.0800  0.1207\n",
      "      5        1.1021       0.6250        1.0851  0.1210\n",
      "      6        1.0876       0.6271        1.0926  0.1129\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.1042\u001b[0m       \u001b[32m0.6081\u001b[0m        \u001b[35m1.0077\u001b[0m  0.1208\n",
      "      2        \u001b[36m1.0841\u001b[0m       0.5826        1.0142  0.1208\n",
      "      3        1.0867       0.5932        1.0423  0.1210\n",
      "      4        \u001b[36m1.0779\u001b[0m       0.6059        1.0328  0.1207\n",
      "      5        \u001b[36m1.0774\u001b[0m       \u001b[32m0.6102\u001b[0m        \u001b[35m1.0043\u001b[0m  0.1190\n",
      "      6        1.0786       0.6102        1.0376  0.1210\n",
      "      7        1.1007       \u001b[32m0.6123\u001b[0m        1.0624  0.1213\n",
      "      8        1.0787       0.6123        1.0747  0.1135\n",
      "      9        1.0980       0.6102        1.0748  0.1189\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.1215\u001b[0m       \u001b[32m0.6165\u001b[0m        \u001b[35m1.0107\u001b[0m  0.1145\n",
      "      2        \u001b[36m1.0932\u001b[0m       0.5869        \u001b[35m0.9943\u001b[0m  0.1126\n",
      "      3        \u001b[36m1.0721\u001b[0m       0.6165        1.0185  0.1259\n",
      "      4        1.0825       0.6017        1.0429  0.1210\n",
      "      5        1.0779       0.6123        1.0476  0.1206\n",
      "      6        1.1001       0.6081        1.0515  0.1231\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.1014\u001b[0m       \u001b[32m0.6165\u001b[0m        \u001b[35m1.0184\u001b[0m  0.1186\n",
      "      2        \u001b[36m1.0671\u001b[0m       0.5720        \u001b[35m0.9658\u001b[0m  0.1213\n",
      "      3        1.1085       \u001b[32m0.6271\u001b[0m        0.9674  0.1215\n",
      "      4        \u001b[36m1.0605\u001b[0m       \u001b[32m0.6356\u001b[0m        1.0482  0.1191\n",
      "      5        1.0864       0.6208        1.0720  0.1210\n",
      "      6        1.0963       0.6186        1.0801  0.1213\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.1334\u001b[0m       \u001b[32m0.5826\u001b[0m        \u001b[35m1.0323\u001b[0m  0.1299\n",
      "      2        \u001b[36m1.0897\u001b[0m       \u001b[32m0.6250\u001b[0m        \u001b[35m0.9735\u001b[0m  0.1207\n",
      "      3        \u001b[36m1.0688\u001b[0m       0.5890        0.9971  0.1210\n",
      "      4        1.0757       0.6123        1.0533  0.1202\n",
      "      5        1.0982       0.6081        1.0546  0.1186\n",
      "      6        1.0755       0.6059        1.0300  0.1188\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.1037\u001b[0m       \u001b[32m0.5975\u001b[0m        \u001b[35m1.0402\u001b[0m  0.1126\n",
      "      2        \u001b[36m1.0774\u001b[0m       \u001b[32m0.6123\u001b[0m        \u001b[35m0.9794\u001b[0m  0.1213\n",
      "      3        \u001b[36m1.0713\u001b[0m       0.5826        1.0047  0.1210\n",
      "      4        1.0771       0.6081        1.0305  0.1110\n",
      "      5        1.0741       0.6102        1.0187  0.1189\n",
      "      6        \u001b[36m1.0594\u001b[0m       \u001b[32m0.6144\u001b[0m        1.0154  0.1207\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.1570\u001b[0m       \u001b[32m0.5784\u001b[0m        \u001b[35m1.0016\u001b[0m  0.1109\n",
      "      2        \u001b[36m1.0861\u001b[0m       0.5636        1.0321  0.1214\n",
      "      3        1.1084       \u001b[32m0.5953\u001b[0m        1.0387  0.1125\n",
      "      4        \u001b[36m1.0849\u001b[0m       \u001b[32m0.6038\u001b[0m        1.0453  0.1188\n",
      "      5        \u001b[36m1.0757\u001b[0m       \u001b[32m0.6081\u001b[0m        1.0275  0.1292\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.0775\u001b[0m       \u001b[32m0.5911\u001b[0m        \u001b[35m0.9968\u001b[0m  0.1308\n",
      "      2        \u001b[36m1.0531\u001b[0m       \u001b[32m0.5975\u001b[0m        \u001b[35m0.9722\u001b[0m  0.1314\n",
      "      3        1.0760       \u001b[32m0.6144\u001b[0m        1.0096  0.1308\n",
      "      4        1.0633       0.6059        1.0395  0.1314\n",
      "      5        1.0548       \u001b[32m0.6271\u001b[0m        1.0211  0.1316\n",
      "      6        1.0746       \u001b[32m0.6292\u001b[0m        1.0050  0.1314\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.1215\u001b[0m       \u001b[32m0.5826\u001b[0m        \u001b[35m1.0044\u001b[0m  0.1320\n",
      "      2        \u001b[36m1.0681\u001b[0m       \u001b[32m0.5869\u001b[0m        1.0107  0.1316\n",
      "      3        1.0769       \u001b[32m0.5996\u001b[0m        \u001b[35m0.9798\u001b[0m  0.1313\n",
      "      4        1.0690       \u001b[32m0.6059\u001b[0m        1.0309  0.1345\n",
      "      5        1.0969       \u001b[32m0.6102\u001b[0m        1.0694  0.1309\n",
      "      6        1.0975       \u001b[32m0.6123\u001b[0m        1.0754  0.1314\n",
      "      7        1.0913       0.6102        1.0761  0.1308\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.0780\u001b[0m       \u001b[32m0.6123\u001b[0m        \u001b[35m0.9672\u001b[0m  0.1316\n",
      "      2        \u001b[36m1.0579\u001b[0m       \u001b[32m0.6208\u001b[0m        \u001b[35m0.9364\u001b[0m  0.1425\n",
      "      3        \u001b[36m1.0517\u001b[0m       \u001b[32m0.6250\u001b[0m        1.0124  0.1332\n",
      "      4        1.0807       0.5996        1.0318  0.1294\n",
      "      5        1.0690       0.6250        1.0629  0.1208\n",
      "      6        1.0808       0.6208        1.0342  0.1263\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.1049\u001b[0m       \u001b[32m0.5805\u001b[0m        \u001b[35m1.0264\u001b[0m  0.1208\n",
      "      2        \u001b[36m1.0658\u001b[0m       \u001b[32m0.6017\u001b[0m        \u001b[35m0.9688\u001b[0m  0.1209\n",
      "      3        \u001b[36m1.0312\u001b[0m       \u001b[32m0.6144\u001b[0m        \u001b[35m0.9635\u001b[0m  0.1186\n",
      "      4        1.0570       \u001b[32m0.6250\u001b[0m        1.0059  0.1209\n",
      "      5        1.0666       \u001b[32m0.6462\u001b[0m        1.0310  0.1185\n",
      "      6        1.0927       0.6419        1.0369  0.1204\n",
      "      7        1.0797       0.6377        1.0576  0.1128\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.0896\u001b[0m       \u001b[32m0.6017\u001b[0m        \u001b[35m0.9812\u001b[0m  0.1136\n",
      "      2        \u001b[36m1.0448\u001b[0m       0.5911        \u001b[35m0.9657\u001b[0m  0.1187\n",
      "      3        1.0691       \u001b[32m0.6102\u001b[0m        0.9846  0.1208\n",
      "      4        1.0504       \u001b[32m0.6229\u001b[0m        0.9850  0.1110\n",
      "      5        1.0538       0.6229        1.0188  0.1190\n",
      "      6        1.0748       0.6186        1.0344  0.1209\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.1475\u001b[0m       \u001b[32m0.5953\u001b[0m        \u001b[35m1.0406\u001b[0m  0.1211\n",
      "      2        \u001b[36m1.0811\u001b[0m       \u001b[32m0.5996\u001b[0m        \u001b[35m0.9485\u001b[0m  0.1208\n",
      "      3        \u001b[36m1.0744\u001b[0m       \u001b[32m0.6081\u001b[0m        0.9553  0.1190\n",
      "      4        \u001b[36m1.0471\u001b[0m       \u001b[32m0.6208\u001b[0m        1.0360  0.1204\n",
      "      5        1.0778       0.6165        1.0471  0.1187\n",
      "      6        1.0786       0.6165        1.0561  0.1205\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.0792\u001b[0m       \u001b[32m0.5826\u001b[0m        \u001b[35m0.9980\u001b[0m  0.1200\n",
      "      2        1.0893       \u001b[32m0.6017\u001b[0m        \u001b[35m0.9866\u001b[0m  0.1208\n",
      "      3        \u001b[36m1.0632\u001b[0m       \u001b[32m0.6271\u001b[0m        1.0062  0.1187\n",
      "      4        1.0970       0.6123        1.0473  0.1165\n",
      "      5        1.1162       0.6081        1.0688  0.1206\n",
      "      6        1.1015       0.6165        1.0797  0.1210\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.0789\u001b[0m       \u001b[32m0.6059\u001b[0m        \u001b[35m1.0051\u001b[0m  0.1108\n",
      "      2        \u001b[36m1.0555\u001b[0m       0.6017        \u001b[35m0.9808\u001b[0m  0.1189\n",
      "      3        1.0602       \u001b[32m0.6102\u001b[0m        1.0096  0.1207\n",
      "      4        1.0581       \u001b[32m0.6271\u001b[0m        0.9957  0.1104\n",
      "      5        1.0566       0.6250        1.0196  0.1184\n",
      "      6        1.0745       \u001b[32m0.6292\u001b[0m        1.0455  0.1167\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.0930\u001b[0m       \u001b[32m0.5869\u001b[0m        \u001b[35m1.0329\u001b[0m  0.1213\n",
      "      2        \u001b[36m1.0856\u001b[0m       \u001b[32m0.6059\u001b[0m        \u001b[35m0.9610\u001b[0m  0.1340\n",
      "      3        \u001b[36m1.0572\u001b[0m       \u001b[32m0.6208\u001b[0m        0.9610  0.1225\n",
      "      4        1.0649       \u001b[32m0.6292\u001b[0m        1.0041  0.1128\n",
      "      5        1.0705       0.6271        1.0495  0.1104\n",
      "      6        1.0916       0.6144        1.0820  0.1202\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.1068\u001b[0m       \u001b[32m0.6017\u001b[0m        \u001b[35m0.9805\u001b[0m  0.1112\n",
      "      2        1.1168       0.5381        \u001b[35m0.9703\u001b[0m  0.1208\n",
      "      3        1.1093       0.5360        1.0500  0.1208\n",
      "      4        \u001b[36m1.1049\u001b[0m       0.5890        1.0420  0.1104\n",
      "      5        \u001b[36m1.0791\u001b[0m       \u001b[32m0.6250\u001b[0m        1.0406  0.1187\n",
      "      6        1.0910       0.6229        1.0482  0.1212\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.1259\u001b[0m       \u001b[32m0.5953\u001b[0m        \u001b[35m1.0514\u001b[0m  0.1190\n",
      "      2        \u001b[36m1.0896\u001b[0m       0.5847        \u001b[35m1.0292\u001b[0m  0.1210\n",
      "      3        \u001b[36m1.0833\u001b[0m       0.5742        \u001b[35m0.9810\u001b[0m  0.1133\n",
      "      4        1.0970       \u001b[32m0.6017\u001b[0m        1.0235  0.1106\n",
      "      5        1.1200       0.5890        1.0637  0.1188\n",
      "      6        1.1104       \u001b[32m0.6102\u001b[0m        1.0831  0.1186\n",
      "      7        1.0975       0.6102        1.0930  0.1190\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.1205\u001b[0m       \u001b[32m0.5826\u001b[0m        \u001b[35m1.0357\u001b[0m  0.1208\n",
      "      2        \u001b[36m1.0492\u001b[0m       \u001b[32m0.5847\u001b[0m        \u001b[35m0.9751\u001b[0m  0.1153\n",
      "      3        1.0700       \u001b[32m0.5975\u001b[0m        0.9813  0.1103\n",
      "      4        1.0686       0.5636        1.0508  0.1189\n",
      "      5        1.1160       0.5826        1.0943  0.1189\n",
      "      6        1.1052       \u001b[32m0.6208\u001b[0m        1.0905  0.1188\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.0844\u001b[0m       \u001b[32m0.6165\u001b[0m        \u001b[35m1.0089\u001b[0m  0.1126\n",
      "      2        \u001b[36m1.0461\u001b[0m       0.6017        \u001b[35m0.9116\u001b[0m  0.1187\n",
      "      3        1.0465       0.6165        0.9695  0.1208\n",
      "      4        1.0491       0.6017        0.9878  0.1212\n",
      "      5        1.0794       \u001b[32m0.6292\u001b[0m        1.0021  0.1247\n",
      "      6        1.0670       0.6144        1.0302  0.1176\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.2862\u001b[0m       \u001b[32m0.2945\u001b[0m        \u001b[35m1.1429\u001b[0m  0.1107\n",
      "      2        \u001b[36m1.2356\u001b[0m       \u001b[32m0.3220\u001b[0m        \u001b[35m1.1147\u001b[0m  0.1127\n",
      "      3        \u001b[36m1.1945\u001b[0m       \u001b[32m0.3708\u001b[0m        \u001b[35m1.0737\u001b[0m  0.1103\n",
      "      4        \u001b[36m1.1419\u001b[0m       \u001b[32m0.4492\u001b[0m        \u001b[35m1.0299\u001b[0m  0.1111\n",
      "      5        \u001b[36m1.0925\u001b[0m       \u001b[32m0.5530\u001b[0m        \u001b[35m0.9993\u001b[0m  0.1112\n",
      "      6        \u001b[36m1.0475\u001b[0m       \u001b[32m0.5636\u001b[0m        \u001b[35m0.9916\u001b[0m  0.1105\n",
      "      7        1.0581       \u001b[32m0.5953\u001b[0m        \u001b[35m0.9892\u001b[0m  0.1112\n",
      "      8        1.0606       \u001b[32m0.6017\u001b[0m        \u001b[35m0.9882\u001b[0m  0.1114\n",
      "      9        1.0492       \u001b[32m0.6038\u001b[0m        0.9892  0.1087\n",
      "     10        1.0481       \u001b[32m0.6059\u001b[0m        0.9908  0.1105\n",
      "     11        1.0489       0.5932        0.9922  0.1068\n",
      "     12        1.0662       0.5720        0.9936  0.1114\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.1896\u001b[0m       \u001b[32m0.3898\u001b[0m        \u001b[35m1.1370\u001b[0m  0.1108\n",
      "      2        \u001b[36m1.1557\u001b[0m       \u001b[32m0.4004\u001b[0m        \u001b[35m1.1227\u001b[0m  0.1109\n",
      "      3        \u001b[36m1.1294\u001b[0m       \u001b[32m0.4237\u001b[0m        \u001b[35m1.1031\u001b[0m  0.1091\n",
      "      4        \u001b[36m1.1247\u001b[0m       \u001b[32m0.4746\u001b[0m        \u001b[35m1.0833\u001b[0m  0.1107\n",
      "      5        \u001b[36m1.1033\u001b[0m       \u001b[32m0.5000\u001b[0m        \u001b[35m1.0690\u001b[0m  0.1106\n",
      "      6        \u001b[36m1.0677\u001b[0m       \u001b[32m0.5275\u001b[0m        \u001b[35m1.0618\u001b[0m  0.1190\n",
      "      7        \u001b[36m1.0667\u001b[0m       \u001b[32m0.5339\u001b[0m        \u001b[35m1.0562\u001b[0m  0.1106\n",
      "      8        1.0690       \u001b[32m0.5466\u001b[0m        \u001b[35m1.0525\u001b[0m  0.1105\n",
      "      9        1.0677       \u001b[32m0.5593\u001b[0m        \u001b[35m1.0481\u001b[0m  0.1106\n",
      "     10        \u001b[36m1.0586\u001b[0m       0.5572        \u001b[35m1.0435\u001b[0m  0.1150\n",
      "     11        \u001b[36m1.0512\u001b[0m       0.5530        \u001b[35m1.0387\u001b[0m  0.1108\n",
      "     12        1.0599       0.5424        \u001b[35m1.0338\u001b[0m  0.1109\n",
      "     13        1.0611       0.5487        \u001b[35m1.0291\u001b[0m  0.1104\n",
      "     14        1.0721       0.5487        \u001b[35m1.0240\u001b[0m  0.1130\n",
      "     15        1.0620       0.5593        \u001b[35m1.0190\u001b[0m  0.1106\n",
      "     16        1.0750       0.5530        \u001b[35m1.0139\u001b[0m  0.1294\n",
      "     17        1.0826       0.5508        \u001b[35m1.0098\u001b[0m  0.1311\n",
      "     18        1.0799       0.5530        \u001b[35m1.0066\u001b[0m  0.1232\n",
      "     19        1.0718       0.5572        \u001b[35m1.0039\u001b[0m  0.1260\n",
      "     20        1.0792       0.5593        \u001b[35m1.0007\u001b[0m  0.1214\n",
      "     21        1.0717       \u001b[32m0.5657\u001b[0m        \u001b[35m0.9989\u001b[0m  0.1213\n",
      "     22        1.0787       \u001b[32m0.5720\u001b[0m        \u001b[35m0.9980\u001b[0m  0.1286\n",
      "     23        1.0901       \u001b[32m0.5805\u001b[0m        \u001b[35m0.9974\u001b[0m  0.1204\n",
      "     24        1.0861       \u001b[32m0.5826\u001b[0m        \u001b[35m0.9963\u001b[0m  0.1214\n",
      "     25        1.0673       \u001b[32m0.5869\u001b[0m        \u001b[35m0.9948\u001b[0m  0.1215\n",
      "     26        1.0800       \u001b[32m0.5890\u001b[0m        \u001b[35m0.9929\u001b[0m  0.1300\n",
      "     27        1.0655       0.5890        \u001b[35m0.9908\u001b[0m  0.1316\n",
      "     28        1.0661       \u001b[32m0.5953\u001b[0m        \u001b[35m0.9885\u001b[0m  0.1235\n",
      "     29        1.0590       \u001b[32m0.6017\u001b[0m        \u001b[35m0.9863\u001b[0m  0.1215\n",
      "     30        1.0594       0.5996        \u001b[35m0.9849\u001b[0m  0.1299\n",
      "     31        \u001b[36m1.0472\u001b[0m       0.5996        \u001b[35m0.9839\u001b[0m  0.1213\n",
      "     32        \u001b[36m1.0463\u001b[0m       0.6017        \u001b[35m0.9833\u001b[0m  0.1315\n",
      "     33        \u001b[36m1.0277\u001b[0m       0.5975        0.9842  0.1313\n",
      "     34        1.0349       0.5996        0.9871  0.1290\n",
      "     35        1.0350       0.5996        0.9913  0.1292\n",
      "     36        1.0384       \u001b[32m0.6038\u001b[0m        0.9950  0.1415\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.2468\u001b[0m       \u001b[32m0.3220\u001b[0m        \u001b[35m1.1465\u001b[0m  0.1090\n",
      "      2        \u001b[36m1.2023\u001b[0m       \u001b[32m0.3432\u001b[0m        \u001b[35m1.1232\u001b[0m  0.1210\n",
      "      3        \u001b[36m1.1920\u001b[0m       \u001b[32m0.3771\u001b[0m        \u001b[35m1.0900\u001b[0m  0.1211\n",
      "      4        \u001b[36m1.1351\u001b[0m       \u001b[32m0.4555\u001b[0m        \u001b[35m1.0537\u001b[0m  0.1244\n",
      "      5        \u001b[36m1.1084\u001b[0m       \u001b[32m0.5191\u001b[0m        \u001b[35m1.0241\u001b[0m  0.1220\n",
      "      6        \u001b[36m1.0847\u001b[0m       \u001b[32m0.5614\u001b[0m        \u001b[35m1.0087\u001b[0m  0.1209\n",
      "      7        \u001b[36m1.0611\u001b[0m       \u001b[32m0.5678\u001b[0m        \u001b[35m1.0036\u001b[0m  0.1212\n",
      "      8        \u001b[36m1.0458\u001b[0m       \u001b[32m0.5763\u001b[0m        \u001b[35m1.0000\u001b[0m  0.1228\n",
      "      9        1.0570       \u001b[32m0.6059\u001b[0m        \u001b[35m0.9964\u001b[0m  0.1212\n",
      "     10        1.0511       \u001b[32m0.6102\u001b[0m        \u001b[35m0.9928\u001b[0m  0.1210\n",
      "     11        \u001b[36m1.0419\u001b[0m       0.6102        \u001b[35m0.9897\u001b[0m  0.1208\n",
      "     12        \u001b[36m1.0300\u001b[0m       0.6038        \u001b[35m0.9852\u001b[0m  0.1310\n",
      "     13        1.0533       0.5996        \u001b[35m0.9787\u001b[0m  0.1155\n",
      "     14        1.0424       0.6017        \u001b[35m0.9724\u001b[0m  0.1208\n",
      "     15        \u001b[36m1.0219\u001b[0m       0.5975        \u001b[35m0.9672\u001b[0m  0.1236\n",
      "     16        1.0507       0.5847        \u001b[35m0.9639\u001b[0m  0.1225\n",
      "     17        1.0540       0.5763        \u001b[35m0.9630\u001b[0m  0.1218\n",
      "     18        1.0468       0.5678        0.9646  0.1316\n",
      "     19        1.0612       0.5699        0.9682  0.1300\n",
      "     20        1.0594       0.5678        0.9727  0.1204\n",
      "     21        1.0739       0.5636        0.9774  0.1119\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.2843\u001b[0m       \u001b[32m0.2394\u001b[0m        \u001b[35m1.2280\u001b[0m  0.1114\n",
      "      2        \u001b[36m1.2693\u001b[0m       \u001b[32m0.2521\u001b[0m        \u001b[35m1.1978\u001b[0m  0.1124\n",
      "      3        \u001b[36m1.2162\u001b[0m       \u001b[32m0.3199\u001b[0m        \u001b[35m1.1539\u001b[0m  0.1073\n",
      "      4        \u001b[36m1.1523\u001b[0m       \u001b[32m0.4280\u001b[0m        \u001b[35m1.1052\u001b[0m  0.1110\n",
      "      5        \u001b[36m1.1016\u001b[0m       \u001b[32m0.4767\u001b[0m        \u001b[35m1.0692\u001b[0m  0.1187\n",
      "      6        \u001b[36m1.0790\u001b[0m       \u001b[32m0.5000\u001b[0m        \u001b[35m1.0558\u001b[0m  0.1106\n",
      "      7        \u001b[36m1.0757\u001b[0m       \u001b[32m0.5042\u001b[0m        \u001b[35m1.0457\u001b[0m  0.1124\n",
      "      8        \u001b[36m1.0540\u001b[0m       \u001b[32m0.5339\u001b[0m        \u001b[35m1.0356\u001b[0m  0.1113\n",
      "      9        1.0683       \u001b[32m0.5551\u001b[0m        \u001b[35m1.0263\u001b[0m  0.1110\n",
      "     10        \u001b[36m1.0536\u001b[0m       0.5551        \u001b[35m1.0172\u001b[0m  0.1115\n",
      "     11        1.0565       \u001b[32m0.5636\u001b[0m        \u001b[35m1.0082\u001b[0m  0.1101\n",
      "     12        1.0727       \u001b[32m0.5742\u001b[0m        \u001b[35m1.0004\u001b[0m  0.1110\n",
      "     13        1.0741       0.5720        \u001b[35m0.9925\u001b[0m  0.1111\n",
      "     14        1.0658       \u001b[32m0.5784\u001b[0m        \u001b[35m0.9852\u001b[0m  0.1108\n",
      "     15        1.0815       0.5720        \u001b[35m0.9786\u001b[0m  0.1099\n",
      "     16        1.0838       \u001b[32m0.5805\u001b[0m        \u001b[35m0.9720\u001b[0m  0.1099\n",
      "     17        1.0878       \u001b[32m0.5869\u001b[0m        \u001b[35m0.9664\u001b[0m  0.1030\n",
      "     18        \u001b[36m1.0524\u001b[0m       \u001b[32m0.5953\u001b[0m        \u001b[35m0.9618\u001b[0m  0.1106\n",
      "     19        1.0752       \u001b[32m0.5996\u001b[0m        \u001b[35m0.9580\u001b[0m  0.1104\n",
      "     20        1.0652       0.5890        \u001b[35m0.9553\u001b[0m  0.1126\n",
      "     21        1.0611       0.5932        \u001b[35m0.9539\u001b[0m  0.1691\n",
      "     22        1.0693       0.5953        \u001b[35m0.9530\u001b[0m  0.1653\n",
      "     23        1.0641       0.5975        \u001b[35m0.9525\u001b[0m  0.1609\n",
      "     24        1.0907       \u001b[32m0.6059\u001b[0m        0.9531  0.1102\n",
      "     25        \u001b[36m1.0283\u001b[0m       0.6059        0.9552  0.1116\n",
      "     26        1.0549       0.5996        0.9587  0.1495\n",
      "     27        1.0569       0.5996        0.9638  0.1131\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.1429\u001b[0m       \u001b[32m0.4364\u001b[0m        \u001b[35m1.0931\u001b[0m  0.1475\n",
      "      2        \u001b[36m1.1272\u001b[0m       \u001b[32m0.4534\u001b[0m        \u001b[35m1.0799\u001b[0m  0.1256\n",
      "      3        \u001b[36m1.1029\u001b[0m       \u001b[32m0.4936\u001b[0m        \u001b[35m1.0609\u001b[0m  0.1514\n",
      "      4        \u001b[36m1.0917\u001b[0m       \u001b[32m0.5318\u001b[0m        \u001b[35m1.0404\u001b[0m  0.1229\n",
      "      5        \u001b[36m1.0796\u001b[0m       \u001b[32m0.5699\u001b[0m        \u001b[35m1.0235\u001b[0m  0.1204\n",
      "      6        \u001b[36m1.0660\u001b[0m       \u001b[32m0.5805\u001b[0m        \u001b[35m1.0150\u001b[0m  0.1291\n",
      "      7        \u001b[36m1.0612\u001b[0m       \u001b[32m0.5826\u001b[0m        \u001b[35m1.0090\u001b[0m  0.1437\n",
      "      8        \u001b[36m1.0598\u001b[0m       \u001b[32m0.5911\u001b[0m        \u001b[35m1.0026\u001b[0m  0.1375\n",
      "      9        \u001b[36m1.0461\u001b[0m       \u001b[32m0.5953\u001b[0m        \u001b[35m0.9954\u001b[0m  0.1296\n",
      "     10        1.0583       \u001b[32m0.6102\u001b[0m        \u001b[35m0.9882\u001b[0m  0.1320\n",
      "     11        \u001b[36m1.0425\u001b[0m       0.6081        \u001b[35m0.9812\u001b[0m  0.1313\n",
      "     12        \u001b[36m1.0356\u001b[0m       \u001b[32m0.6165\u001b[0m        \u001b[35m0.9749\u001b[0m  0.1317\n",
      "     13        \u001b[36m1.0307\u001b[0m       0.6059        \u001b[35m0.9696\u001b[0m  0.1209\n",
      "     14        1.0399       0.6059        \u001b[35m0.9647\u001b[0m  0.1321\n",
      "     15        1.0411       0.6059        \u001b[35m0.9612\u001b[0m  0.1109\n",
      "     16        1.0363       \u001b[32m0.6186\u001b[0m        \u001b[35m0.9595\u001b[0m  0.1315\n",
      "     17        \u001b[36m1.0298\u001b[0m       0.6165        \u001b[35m0.9584\u001b[0m  0.1192\n",
      "     18        1.0526       0.6059        \u001b[35m0.9579\u001b[0m  0.1316\n",
      "     19        1.0467       0.6059        0.9580  0.1113\n",
      "     20        1.0516       0.6017        0.9591  0.1187\n",
      "     21        1.0554       0.6081        0.9607  0.1107\n",
      "     22        1.0711       0.6038        0.9618  0.1109\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.2890\u001b[0m       \u001b[32m0.3665\u001b[0m        \u001b[35m1.1664\u001b[0m  0.1211\n",
      "      2        \u001b[36m1.2826\u001b[0m       \u001b[32m0.3983\u001b[0m        \u001b[35m1.1396\u001b[0m  0.1215\n",
      "      3        \u001b[36m1.2519\u001b[0m       \u001b[32m0.4534\u001b[0m        \u001b[35m1.1011\u001b[0m  0.1108\n",
      "      4        \u001b[36m1.1533\u001b[0m       \u001b[32m0.5042\u001b[0m        \u001b[35m1.0597\u001b[0m  0.1186\n",
      "      5        \u001b[36m1.1122\u001b[0m       \u001b[32m0.5424\u001b[0m        \u001b[35m1.0267\u001b[0m  0.1216\n",
      "      6        \u001b[36m1.0868\u001b[0m       \u001b[32m0.5593\u001b[0m        \u001b[35m1.0133\u001b[0m  0.1318\n",
      "      7        \u001b[36m1.0602\u001b[0m       \u001b[32m0.5869\u001b[0m        \u001b[35m1.0084\u001b[0m  0.1821\n",
      "      8        \u001b[36m1.0497\u001b[0m       \u001b[32m0.5890\u001b[0m        \u001b[35m1.0052\u001b[0m  0.1540\n",
      "      9        \u001b[36m1.0286\u001b[0m       \u001b[32m0.5953\u001b[0m        \u001b[35m1.0019\u001b[0m  0.1299\n",
      "     10        1.0472       \u001b[32m0.6081\u001b[0m        \u001b[35m0.9990\u001b[0m  0.1575\n",
      "     11        1.0341       0.6017        \u001b[35m0.9970\u001b[0m  0.1490\n",
      "     12        1.0528       0.5996        \u001b[35m0.9962\u001b[0m  0.1283\n",
      "     13        1.0465       0.5911        \u001b[35m0.9953\u001b[0m  0.1210\n",
      "     14        1.0716       0.6017        \u001b[35m0.9935\u001b[0m  0.1107\n",
      "     15        1.0749       0.5869        \u001b[35m0.9910\u001b[0m  0.1271\n",
      "     16        1.0733       0.5847        \u001b[35m0.9892\u001b[0m  0.1165\n",
      "     17        1.0898       0.5890        \u001b[35m0.9878\u001b[0m  0.1197\n",
      "     18        1.1008       0.5678        \u001b[35m0.9867\u001b[0m  0.1291\n",
      "     19        1.0866       0.5742        \u001b[35m0.9854\u001b[0m  0.1192\n",
      "     20        1.0806       0.5805        \u001b[35m0.9832\u001b[0m  0.1242\n",
      "     21        1.0815       0.5847        \u001b[35m0.9806\u001b[0m  0.1331\n",
      "     22        1.0871       0.5847        \u001b[35m0.9775\u001b[0m  0.1158\n",
      "     23        1.1037       0.5869        \u001b[35m0.9747\u001b[0m  0.1214\n",
      "     24        1.1018       0.5932        \u001b[35m0.9726\u001b[0m  0.1239\n",
      "     25        1.1034       0.5996        \u001b[35m0.9721\u001b[0m  0.1110\n",
      "     26        1.0739       0.6081        0.9736  0.1303\n",
      "     27        1.0882       0.6081        0.9768  0.1213\n",
      "     28        1.0841       \u001b[32m0.6102\u001b[0m        0.9817  0.1500\n",
      "     29        1.0928       \u001b[32m0.6186\u001b[0m        0.9876  0.1466\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.2727\u001b[0m       \u001b[32m0.3432\u001b[0m        \u001b[35m1.1821\u001b[0m  0.1417\n",
      "      2        \u001b[36m1.2352\u001b[0m       \u001b[32m0.3665\u001b[0m        \u001b[35m1.1583\u001b[0m  0.1696\n",
      "      3        \u001b[36m1.1958\u001b[0m       \u001b[32m0.4131\u001b[0m        \u001b[35m1.1244\u001b[0m  0.1865\n",
      "      4        \u001b[36m1.1616\u001b[0m       \u001b[32m0.4534\u001b[0m        \u001b[35m1.0881\u001b[0m  0.1417\n",
      "      5        \u001b[36m1.1089\u001b[0m       \u001b[32m0.4915\u001b[0m        \u001b[35m1.0582\u001b[0m  0.1315\n",
      "      6        \u001b[36m1.0907\u001b[0m       \u001b[32m0.5233\u001b[0m        \u001b[35m1.0430\u001b[0m  0.1420\n",
      "      7        \u001b[36m1.0639\u001b[0m       \u001b[32m0.5254\u001b[0m        \u001b[35m1.0359\u001b[0m  0.1411\n",
      "      8        1.0672       \u001b[32m0.5424\u001b[0m        \u001b[35m1.0300\u001b[0m  0.1317\n",
      "      9        \u001b[36m1.0602\u001b[0m       \u001b[32m0.5911\u001b[0m        \u001b[35m1.0259\u001b[0m  0.1452\n",
      "     10        1.0710       \u001b[32m0.6017\u001b[0m        \u001b[35m1.0196\u001b[0m  0.1533\n",
      "     11        1.0718       0.5996        \u001b[35m1.0132\u001b[0m  0.2136\n",
      "     12        1.0604       0.5953        \u001b[35m1.0078\u001b[0m  0.2087\n",
      "     13        1.0691       0.5847        \u001b[35m1.0031\u001b[0m  0.2074\n",
      "     14        1.0607       0.5805        \u001b[35m0.9983\u001b[0m  0.1534\n",
      "     15        1.0625       0.5742        \u001b[35m0.9946\u001b[0m  0.1584\n",
      "     16        \u001b[36m1.0601\u001b[0m       0.5678        \u001b[35m0.9922\u001b[0m  0.1472\n",
      "     17        1.0676       0.5742        \u001b[35m0.9903\u001b[0m  0.1473\n",
      "     18        1.0768       0.5699        \u001b[35m0.9887\u001b[0m  0.1897\n",
      "     19        \u001b[36m1.0596\u001b[0m       0.5742        0.9888  0.3904\n",
      "     20        \u001b[36m1.0594\u001b[0m       0.5742        0.9901  0.1450\n",
      "     21        1.0909       0.5784        0.9918  0.1391\n",
      "     22        \u001b[36m1.0587\u001b[0m       0.5826        0.9941  0.1202\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.1912\u001b[0m       \u001b[32m0.3983\u001b[0m        \u001b[35m1.1239\u001b[0m  0.1947\n",
      "      2        \u001b[36m1.1826\u001b[0m       \u001b[32m0.4174\u001b[0m        \u001b[35m1.1051\u001b[0m  0.1321\n",
      "      3        \u001b[36m1.1413\u001b[0m       \u001b[32m0.4513\u001b[0m        \u001b[35m1.0773\u001b[0m  0.1262\n",
      "      4        \u001b[36m1.1100\u001b[0m       \u001b[32m0.4915\u001b[0m        \u001b[35m1.0456\u001b[0m  0.1261\n",
      "      5        \u001b[36m1.0824\u001b[0m       \u001b[32m0.5508\u001b[0m        \u001b[35m1.0191\u001b[0m  0.1301\n",
      "      6        \u001b[36m1.0425\u001b[0m       \u001b[32m0.5826\u001b[0m        \u001b[35m1.0040\u001b[0m  0.1212\n",
      "      7        \u001b[36m1.0285\u001b[0m       \u001b[32m0.5953\u001b[0m        \u001b[35m0.9940\u001b[0m  0.1262\n",
      "      8        \u001b[36m1.0221\u001b[0m       \u001b[32m0.6123\u001b[0m        \u001b[35m0.9845\u001b[0m  0.1281\n",
      "      9        \u001b[36m1.0172\u001b[0m       \u001b[32m0.6186\u001b[0m        \u001b[35m0.9747\u001b[0m  0.1311\n",
      "     10        1.0302       0.6102        \u001b[35m0.9629\u001b[0m  0.1490\n",
      "     11        1.0199       0.6144        \u001b[35m0.9519\u001b[0m  0.1659\n",
      "     12        \u001b[36m1.0166\u001b[0m       0.6059        \u001b[35m0.9428\u001b[0m  0.1271\n",
      "     13        1.0379       0.5975        \u001b[35m0.9356\u001b[0m  0.1212\n",
      "     14        \u001b[36m1.0159\u001b[0m       0.5953        \u001b[35m0.9308\u001b[0m  0.1162\n",
      "     15        1.0562       0.5975        \u001b[35m0.9277\u001b[0m  0.1130\n",
      "     16        1.0541       0.5953        \u001b[35m0.9261\u001b[0m  0.1133\n",
      "     17        1.0342       0.5953        \u001b[35m0.9252\u001b[0m  0.1128\n",
      "     18        1.0477       0.5975        \u001b[35m0.9250\u001b[0m  0.1188\n",
      "     19        1.0751       0.5953        0.9256  0.1110\n",
      "     20        1.0749       0.5932        0.9268  0.1193\n",
      "     21        1.0714       0.5996        0.9287  0.1110\n",
      "     22        1.0635       0.6017        0.9311  0.1275\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.1790\u001b[0m       \u001b[32m0.3941\u001b[0m        \u001b[35m1.1164\u001b[0m  0.1210\n",
      "      2        \u001b[36m1.1701\u001b[0m       \u001b[32m0.4407\u001b[0m        \u001b[35m1.0968\u001b[0m  0.1214\n",
      "      3        \u001b[36m1.1193\u001b[0m       \u001b[32m0.4915\u001b[0m        \u001b[35m1.0687\u001b[0m  0.1152\n",
      "      4        \u001b[36m1.0840\u001b[0m       \u001b[32m0.5360\u001b[0m        \u001b[35m1.0400\u001b[0m  0.1128\n",
      "      5        \u001b[36m1.0536\u001b[0m       \u001b[32m0.5763\u001b[0m        \u001b[35m1.0206\u001b[0m  0.1951\n",
      "      6        \u001b[36m1.0327\u001b[0m       0.5763        \u001b[35m1.0129\u001b[0m  0.1406\n",
      "      7        1.0474       \u001b[32m0.5805\u001b[0m        \u001b[35m1.0054\u001b[0m  0.1214\n",
      "      8        \u001b[36m1.0291\u001b[0m       0.5784        \u001b[35m0.9961\u001b[0m  0.1219\n",
      "      9        1.0428       \u001b[32m0.5932\u001b[0m        \u001b[35m0.9853\u001b[0m  0.1599\n",
      "     10        \u001b[36m1.0216\u001b[0m       \u001b[32m0.6017\u001b[0m        \u001b[35m0.9745\u001b[0m  0.1191\n",
      "     11        \u001b[36m1.0065\u001b[0m       \u001b[32m0.6059\u001b[0m        \u001b[35m0.9646\u001b[0m  0.1172\n",
      "     12        1.0207       \u001b[32m0.6102\u001b[0m        \u001b[35m0.9571\u001b[0m  0.1247\n",
      "     13        1.0121       \u001b[32m0.6144\u001b[0m        \u001b[35m0.9516\u001b[0m  0.1142\n",
      "     14        1.0201       \u001b[32m0.6186\u001b[0m        \u001b[35m0.9476\u001b[0m  0.1216\n",
      "     15        1.0157       \u001b[32m0.6208\u001b[0m        \u001b[35m0.9443\u001b[0m  0.1310\n",
      "     16        1.0420       0.6165        \u001b[35m0.9410\u001b[0m  0.1310\n",
      "     17        1.0500       0.6165        \u001b[35m0.9379\u001b[0m  0.1312\n",
      "     18        1.0649       0.6208        \u001b[35m0.9344\u001b[0m  0.1363\n",
      "     19        1.0808       0.6165        \u001b[35m0.9307\u001b[0m  0.1291\n",
      "     20        1.0777       0.6186        \u001b[35m0.9265\u001b[0m  0.1778\n",
      "     21        1.0521       0.6144        \u001b[35m0.9221\u001b[0m  0.1202\n",
      "     22        1.0334       0.6208        \u001b[35m0.9181\u001b[0m  0.1142\n",
      "     23        1.0459       \u001b[32m0.6271\u001b[0m        \u001b[35m0.9153\u001b[0m  0.1132\n",
      "     24        1.0394       \u001b[32m0.6292\u001b[0m        \u001b[35m0.9136\u001b[0m  0.1202\n",
      "     25        1.0443       \u001b[32m0.6398\u001b[0m        0.9151  0.1212\n",
      "     26        1.0188       0.6356        0.9204  0.1162\n",
      "     27        \u001b[36m1.0062\u001b[0m       0.6377        0.9296  0.1172\n",
      "     28        1.0200       0.6377        0.9408  0.1132\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.2672\u001b[0m       \u001b[32m0.2606\u001b[0m        \u001b[35m1.1937\u001b[0m  0.1162\n",
      "      2        \u001b[36m1.2417\u001b[0m       \u001b[32m0.2775\u001b[0m        \u001b[35m1.1663\u001b[0m  0.1172\n",
      "      3        \u001b[36m1.1947\u001b[0m       \u001b[32m0.3305\u001b[0m        \u001b[35m1.1280\u001b[0m  0.1142\n",
      "      4        \u001b[36m1.1469\u001b[0m       \u001b[32m0.3983\u001b[0m        \u001b[35m1.0879\u001b[0m  0.1142\n",
      "      5        \u001b[36m1.0950\u001b[0m       \u001b[32m0.5021\u001b[0m        \u001b[35m1.0600\u001b[0m  0.1192\n",
      "      6        \u001b[36m1.0697\u001b[0m       \u001b[32m0.5275\u001b[0m        \u001b[35m1.0534\u001b[0m  0.1252\n",
      "      7        1.0762       \u001b[32m0.5445\u001b[0m        \u001b[35m1.0515\u001b[0m  0.1162\n",
      "      8        \u001b[36m1.0695\u001b[0m       \u001b[32m0.5636\u001b[0m        \u001b[35m1.0498\u001b[0m  0.1162\n",
      "      9        \u001b[36m1.0602\u001b[0m       \u001b[32m0.6038\u001b[0m        \u001b[35m1.0493\u001b[0m  0.1158\n",
      "     10        1.0607       \u001b[32m0.6102\u001b[0m        \u001b[35m1.0485\u001b[0m  0.1214\n",
      "     11        1.0649       0.5996        \u001b[35m1.0473\u001b[0m  0.1107\n",
      "     12        1.0726       0.6038        \u001b[35m1.0458\u001b[0m  0.1209\n",
      "     13        \u001b[36m1.0585\u001b[0m       0.6059        \u001b[35m1.0444\u001b[0m  0.1212\n",
      "     14        1.0734       0.6038        \u001b[35m1.0427\u001b[0m  0.1373\n",
      "     15        1.0830       0.5953        \u001b[35m1.0408\u001b[0m  0.1321\n",
      "     16        1.0840       0.5911        \u001b[35m1.0385\u001b[0m  0.1262\n",
      "     17        1.0854       0.5911        \u001b[35m1.0364\u001b[0m  0.1301\n",
      "     18        1.0853       0.5847        \u001b[35m1.0347\u001b[0m  0.1226\n",
      "     19        1.0750       0.5847        \u001b[35m1.0331\u001b[0m  0.1224\n",
      "     20        1.0906       0.5847        \u001b[35m1.0317\u001b[0m  0.1321\n",
      "     21        1.0777       0.5826        \u001b[35m1.0303\u001b[0m  0.1194\n",
      "     22        1.0864       0.5847        \u001b[35m1.0292\u001b[0m  0.1109\n",
      "     23        1.0705       0.5932        \u001b[35m1.0283\u001b[0m  0.1108\n",
      "     24        1.0673       0.5911        \u001b[35m1.0273\u001b[0m  0.1249\n",
      "     25        \u001b[36m1.0554\u001b[0m       0.5953        \u001b[35m1.0258\u001b[0m  0.1315\n",
      "     26        1.0665       0.5890        \u001b[35m1.0238\u001b[0m  0.1150\n",
      "     27        1.0555       0.5911        \u001b[35m1.0206\u001b[0m  0.1105\n",
      "     28        1.0603       0.5869        \u001b[35m1.0176\u001b[0m  0.1130\n",
      "     29        1.0711       0.5932        \u001b[35m1.0152\u001b[0m  0.1133\n",
      "     30        \u001b[36m1.0509\u001b[0m       0.5975        \u001b[35m1.0127\u001b[0m  0.1210\n",
      "     31        1.0686       0.6017        \u001b[35m1.0092\u001b[0m  0.1132\n",
      "     32        \u001b[36m1.0446\u001b[0m       0.6081        \u001b[35m1.0038\u001b[0m  0.1212\n",
      "     33        1.0527       0.6102        \u001b[35m0.9983\u001b[0m  0.1289\n",
      "     34        1.0538       0.6081        \u001b[35m0.9922\u001b[0m  0.1190\n",
      "     35        1.0670       0.6059        \u001b[35m0.9854\u001b[0m  0.1191\n",
      "     36        1.0543       0.6081        \u001b[35m0.9786\u001b[0m  0.1211\n",
      "     37        1.0474       0.6059        \u001b[35m0.9717\u001b[0m  0.1212\n",
      "     38        1.0491       0.6102        \u001b[35m0.9663\u001b[0m  0.1314\n",
      "     39        1.0454       0.6059        \u001b[35m0.9625\u001b[0m  0.1236\n",
      "     40        1.0476       0.6102        \u001b[35m0.9599\u001b[0m  0.1214\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.2244\u001b[0m       \u001b[32m0.4004\u001b[0m        \u001b[35m1.1081\u001b[0m  0.1235\n",
      "      2        \u001b[36m1.1913\u001b[0m       \u001b[32m0.4153\u001b[0m        \u001b[35m1.0900\u001b[0m  0.1309\n",
      "      3        \u001b[36m1.1560\u001b[0m       \u001b[32m0.4640\u001b[0m        \u001b[35m1.0643\u001b[0m  0.1316\n",
      "      4        \u001b[36m1.1233\u001b[0m       \u001b[32m0.5318\u001b[0m        \u001b[35m1.0379\u001b[0m  0.1234\n",
      "      5        \u001b[36m1.0893\u001b[0m       \u001b[32m0.5826\u001b[0m        \u001b[35m1.0184\u001b[0m  0.1295\n",
      "      6        \u001b[36m1.0851\u001b[0m       \u001b[32m0.6081\u001b[0m        \u001b[35m1.0106\u001b[0m  0.1315\n",
      "      7        \u001b[36m1.0641\u001b[0m       \u001b[32m0.6165\u001b[0m        \u001b[35m1.0053\u001b[0m  0.1213\n",
      "      8        \u001b[36m1.0584\u001b[0m       \u001b[32m0.6208\u001b[0m        \u001b[35m1.0012\u001b[0m  0.1316\n",
      "      9        \u001b[36m1.0468\u001b[0m       \u001b[32m0.6271\u001b[0m        \u001b[35m0.9979\u001b[0m  0.1235\n",
      "     10        \u001b[36m1.0409\u001b[0m       0.5996        \u001b[35m0.9946\u001b[0m  0.1316\n",
      "     11        1.0411       0.5911        \u001b[35m0.9928\u001b[0m  0.1313\n",
      "     12        1.0603       0.5911        \u001b[35m0.9926\u001b[0m  0.1234\n",
      "     13        1.0427       0.5805        \u001b[35m0.9925\u001b[0m  0.1234\n",
      "     14        1.0558       0.5699        \u001b[35m0.9922\u001b[0m  0.1318\n",
      "     15        1.0518       0.5678        0.9925  0.1311\n",
      "     16        1.0562       0.5572        0.9934  0.1315\n",
      "     17        1.0634       0.5508        0.9951  0.1234\n",
      "     18        1.0923       0.5551        0.9967  0.1395\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.2241\u001b[0m       \u001b[32m0.3114\u001b[0m        \u001b[35m1.1698\u001b[0m  0.1104\n",
      "      2        1.2318       \u001b[32m0.3411\u001b[0m        \u001b[35m1.1459\u001b[0m  0.1191\n",
      "      3        \u001b[36m1.1973\u001b[0m       \u001b[32m0.4301\u001b[0m        \u001b[35m1.1123\u001b[0m  0.1114\n",
      "      4        \u001b[36m1.1565\u001b[0m       \u001b[32m0.5042\u001b[0m        \u001b[35m1.0764\u001b[0m  0.1113\n",
      "      5        \u001b[36m1.1089\u001b[0m       \u001b[32m0.5508\u001b[0m        \u001b[35m1.0475\u001b[0m  0.1109\n",
      "      6        \u001b[36m1.0865\u001b[0m       \u001b[32m0.5636\u001b[0m        \u001b[35m1.0343\u001b[0m  0.1112\n",
      "      7        \u001b[36m1.0674\u001b[0m       0.5614        \u001b[35m1.0291\u001b[0m  0.1111\n",
      "      8        \u001b[36m1.0499\u001b[0m       \u001b[32m0.5720\u001b[0m        \u001b[35m1.0234\u001b[0m  0.1125\n",
      "      9        1.0579       \u001b[32m0.5869\u001b[0m        \u001b[35m1.0159\u001b[0m  0.1194\n",
      "     10        \u001b[36m1.0461\u001b[0m       \u001b[32m0.6059\u001b[0m        \u001b[35m1.0070\u001b[0m  0.1110\n",
      "     11        \u001b[36m1.0412\u001b[0m       \u001b[32m0.6081\u001b[0m        \u001b[35m0.9967\u001b[0m  0.1109\n",
      "     12        \u001b[36m1.0309\u001b[0m       \u001b[32m0.6123\u001b[0m        \u001b[35m0.9843\u001b[0m  0.1112\n",
      "     13        1.0426       0.6123        \u001b[35m0.9711\u001b[0m  0.1187\n",
      "     14        \u001b[36m1.0309\u001b[0m       0.6038        \u001b[35m0.9579\u001b[0m  0.1110\n",
      "     15        \u001b[36m1.0157\u001b[0m       0.6017        \u001b[35m0.9460\u001b[0m  0.1170\n",
      "     16        1.0260       0.5975        \u001b[35m0.9369\u001b[0m  0.1121\n",
      "     17        1.0476       0.6059        \u001b[35m0.9309\u001b[0m  0.1111\n",
      "     18        1.0516       0.6038        \u001b[35m0.9275\u001b[0m  0.1128\n",
      "     19        1.0576       0.6059        \u001b[35m0.9257\u001b[0m  0.1111\n",
      "     20        1.0463       0.6017        \u001b[35m0.9254\u001b[0m  0.1109\n",
      "     21        1.0817       0.5953        0.9256  0.1127\n",
      "     22        1.0888       0.5890        0.9262  0.1214\n",
      "     23        1.0560       0.5890        0.9270  0.1211\n",
      "     24        1.0807       0.5911        0.9277  0.1207\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.1572\u001b[0m       \u001b[32m0.4470\u001b[0m        \u001b[35m1.0543\u001b[0m  0.1114\n",
      "      2        \u001b[36m1.1330\u001b[0m       \u001b[32m0.4640\u001b[0m        \u001b[35m1.0479\u001b[0m  0.1108\n",
      "      3        \u001b[36m1.1037\u001b[0m       \u001b[32m0.4788\u001b[0m        \u001b[35m1.0376\u001b[0m  0.1114\n",
      "      4        \u001b[36m1.0999\u001b[0m       \u001b[32m0.5127\u001b[0m        \u001b[35m1.0264\u001b[0m  0.1192\n",
      "      5        \u001b[36m1.0889\u001b[0m       \u001b[32m0.5678\u001b[0m        \u001b[35m1.0173\u001b[0m  0.1158\n",
      "      6        \u001b[36m1.0819\u001b[0m       \u001b[32m0.5847\u001b[0m        \u001b[35m1.0114\u001b[0m  0.1187\n",
      "      7        \u001b[36m1.0586\u001b[0m       \u001b[32m0.5975\u001b[0m        \u001b[35m1.0085\u001b[0m  0.1118\n",
      "      8        \u001b[36m1.0574\u001b[0m       \u001b[32m0.6123\u001b[0m        \u001b[35m1.0074\u001b[0m  0.1108\n",
      "      9        \u001b[36m1.0534\u001b[0m       \u001b[32m0.6165\u001b[0m        \u001b[35m1.0073\u001b[0m  0.1108\n",
      "     10        1.0614       0.6059        \u001b[35m1.0071\u001b[0m  0.1897\n",
      "     11        \u001b[36m1.0484\u001b[0m       0.6102        \u001b[35m1.0066\u001b[0m  0.1255\n",
      "     12        \u001b[36m1.0413\u001b[0m       0.6144        \u001b[35m1.0064\u001b[0m  0.1200\n",
      "     13        1.0478       0.6144        \u001b[35m1.0059\u001b[0m  0.1107\n",
      "     14        1.0416       0.6081        \u001b[35m1.0058\u001b[0m  0.1107\n",
      "     15        1.0440       0.5890        \u001b[35m1.0053\u001b[0m  0.1114\n",
      "     16        1.0550       0.5805        1.0058  0.1135\n",
      "     17        1.0471       0.5742        \u001b[35m1.0050\u001b[0m  0.1188\n",
      "     18        1.0544       0.5742        \u001b[35m1.0033\u001b[0m  0.1110\n",
      "     19        1.0625       0.5784        \u001b[35m1.0011\u001b[0m  0.1219\n",
      "     20        1.0611       0.5699        \u001b[35m0.9986\u001b[0m  0.1293\n",
      "     21        1.0694       0.5720        \u001b[35m0.9961\u001b[0m  0.1190\n",
      "     22        1.0873       0.5742        \u001b[35m0.9930\u001b[0m  0.1118\n",
      "     23        1.0821       0.5763        \u001b[35m0.9894\u001b[0m  0.1123\n",
      "     24        1.0576       0.5826        \u001b[35m0.9851\u001b[0m  0.1104\n",
      "     25        1.0961       0.5890        \u001b[35m0.9806\u001b[0m  0.1082\n",
      "     26        1.0645       0.5932        \u001b[35m0.9770\u001b[0m  0.1110\n",
      "     27        1.0660       0.5975        \u001b[35m0.9741\u001b[0m  0.1107\n",
      "     28        1.0442       0.6059        \u001b[35m0.9712\u001b[0m  0.1228\n",
      "     29        1.0558       0.6081        \u001b[35m0.9697\u001b[0m  0.1103\n",
      "     30        1.0526       0.6081        \u001b[35m0.9687\u001b[0m  0.1117\n",
      "     31        1.0436       0.6123        \u001b[35m0.9687\u001b[0m  0.1314\n",
      "     32        1.0519       0.6123        0.9696  0.1138\n",
      "     33        1.0652       0.6123        0.9713  0.1139\n",
      "     34        1.0541       0.6144        0.9744  0.1213\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.1987\u001b[0m       \u001b[32m0.3708\u001b[0m        \u001b[35m1.1196\u001b[0m  0.1322\n",
      "      2        \u001b[36m1.1819\u001b[0m       \u001b[32m0.3941\u001b[0m        \u001b[35m1.0994\u001b[0m  0.1211\n",
      "      3        \u001b[36m1.1669\u001b[0m       \u001b[32m0.4237\u001b[0m        \u001b[35m1.0696\u001b[0m  0.1109\n",
      "      4        \u001b[36m1.1165\u001b[0m       \u001b[32m0.4767\u001b[0m        \u001b[35m1.0357\u001b[0m  0.1105\n",
      "      5        \u001b[36m1.0950\u001b[0m       \u001b[32m0.5191\u001b[0m        \u001b[35m1.0044\u001b[0m  0.1110\n",
      "      6        \u001b[36m1.0311\u001b[0m       \u001b[32m0.5678\u001b[0m        \u001b[35m0.9876\u001b[0m  0.1097\n",
      "      7        1.0383       \u001b[32m0.5911\u001b[0m        \u001b[35m0.9803\u001b[0m  0.1108\n",
      "      8        1.0557       \u001b[32m0.6017\u001b[0m        \u001b[35m0.9751\u001b[0m  0.1214\n",
      "      9        \u001b[36m1.0228\u001b[0m       \u001b[32m0.6123\u001b[0m        \u001b[35m0.9697\u001b[0m  0.1221\n",
      "     10        1.0319       0.6038        \u001b[35m0.9643\u001b[0m  0.1208\n",
      "     11        1.0385       0.6038        \u001b[35m0.9606\u001b[0m  0.1108\n",
      "     12        1.0343       0.6059        \u001b[35m0.9565\u001b[0m  0.1211\n",
      "     13        1.0361       0.5996        \u001b[35m0.9539\u001b[0m  0.1462\n",
      "     14        1.0242       0.5911        \u001b[35m0.9536\u001b[0m  0.1580\n",
      "     15        1.0324       0.6017        0.9555  0.1203\n",
      "     16        1.0408       0.5975        0.9594  0.1116\n",
      "     17        1.0513       0.5890        0.9648  0.1201\n",
      "     18        1.0733       0.5847        0.9709  0.1055\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.1376\u001b[0m       \u001b[32m0.4915\u001b[0m        \u001b[35m1.0557\u001b[0m  0.1108\n",
      "      2        \u001b[36m1.1147\u001b[0m       \u001b[32m0.5085\u001b[0m        \u001b[35m1.0464\u001b[0m  0.1294\n",
      "      3        1.1203       \u001b[32m0.5297\u001b[0m        \u001b[35m1.0330\u001b[0m  0.1303\n",
      "      4        \u001b[36m1.0990\u001b[0m       \u001b[32m0.5593\u001b[0m        \u001b[35m1.0172\u001b[0m  0.1587\n",
      "      5        \u001b[36m1.0755\u001b[0m       \u001b[32m0.5869\u001b[0m        \u001b[35m1.0013\u001b[0m  0.1460\n",
      "      6        \u001b[36m1.0636\u001b[0m       \u001b[32m0.6123\u001b[0m        \u001b[35m0.9887\u001b[0m  0.1430\n",
      "      7        \u001b[36m1.0319\u001b[0m       \u001b[32m0.6165\u001b[0m        \u001b[35m0.9804\u001b[0m  0.1331\n",
      "      8        \u001b[36m1.0270\u001b[0m       0.6102        \u001b[35m0.9741\u001b[0m  0.1150\n",
      "      9        1.0299       0.6102        \u001b[35m0.9694\u001b[0m  0.1113\n",
      "     10        1.0373       0.6102        \u001b[35m0.9656\u001b[0m  0.1206\n",
      "     11        \u001b[36m1.0200\u001b[0m       0.6123        \u001b[35m0.9629\u001b[0m  0.1110\n",
      "     12        1.0372       0.6165        \u001b[35m0.9616\u001b[0m  0.2027\n",
      "     13        1.0412       \u001b[32m0.6186\u001b[0m        \u001b[35m0.9612\u001b[0m  0.1305\n",
      "     14        1.0416       0.6165        0.9615  0.1316\n",
      "     15        1.0290       0.6165        0.9630  0.1131\n",
      "     16        1.0501       0.6059        0.9652  0.1108\n",
      "     17        1.0408       0.5996        0.9680  0.1202\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.1950\u001b[0m       \u001b[32m0.4513\u001b[0m        \u001b[35m1.0888\u001b[0m  0.1133\n",
      "      2        \u001b[36m1.1711\u001b[0m       \u001b[32m0.4597\u001b[0m        \u001b[35m1.0767\u001b[0m  0.1131\n",
      "      3        \u001b[36m1.1422\u001b[0m       \u001b[32m0.4767\u001b[0m        \u001b[35m1.0590\u001b[0m  0.1307\n",
      "      4        \u001b[36m1.1198\u001b[0m       \u001b[32m0.5127\u001b[0m        \u001b[35m1.0390\u001b[0m  0.1293\n",
      "      5        \u001b[36m1.0897\u001b[0m       \u001b[32m0.5466\u001b[0m        \u001b[35m1.0218\u001b[0m  0.1314\n",
      "      6        \u001b[36m1.0451\u001b[0m       \u001b[32m0.5508\u001b[0m        \u001b[35m1.0114\u001b[0m  0.1313\n",
      "      7        1.0493       \u001b[32m0.5657\u001b[0m        \u001b[35m1.0055\u001b[0m  0.1313\n",
      "      8        1.0565       \u001b[32m0.5763\u001b[0m        \u001b[35m1.0004\u001b[0m  0.1217\n",
      "      9        1.0469       \u001b[32m0.5932\u001b[0m        \u001b[35m0.9956\u001b[0m  0.1214\n",
      "     10        \u001b[36m1.0270\u001b[0m       \u001b[32m0.6038\u001b[0m        \u001b[35m0.9916\u001b[0m  0.1212\n",
      "     11        \u001b[36m1.0156\u001b[0m       \u001b[32m0.6123\u001b[0m        \u001b[35m0.9871\u001b[0m  0.1208\n",
      "     12        1.0362       \u001b[32m0.6186\u001b[0m        \u001b[35m0.9822\u001b[0m  0.1220\n",
      "     13        1.0292       0.6123        \u001b[35m0.9784\u001b[0m  0.1309\n",
      "     14        \u001b[36m1.0081\u001b[0m       0.6102        \u001b[35m0.9751\u001b[0m  0.1291\n",
      "     15        1.0219       0.6081        \u001b[35m0.9717\u001b[0m  0.1306\n",
      "     16        1.0334       0.6017        \u001b[35m0.9700\u001b[0m  0.1113\n",
      "     17        1.0451       0.5911        \u001b[35m0.9681\u001b[0m  0.1211\n",
      "     18        1.0429       0.5890        \u001b[35m0.9659\u001b[0m  0.1234\n",
      "     19        1.0486       0.5869        \u001b[35m0.9637\u001b[0m  0.1178\n",
      "     20        1.0519       0.5890        \u001b[35m0.9620\u001b[0m  0.1292\n",
      "     21        1.0645       0.5953        \u001b[35m0.9598\u001b[0m  0.1292\n",
      "     22        1.0816       0.5890        \u001b[35m0.9574\u001b[0m  0.1362\n",
      "     23        1.0466       0.5911        \u001b[35m0.9550\u001b[0m  0.1296\n",
      "     24        1.0796       0.5953        \u001b[35m0.9525\u001b[0m  0.1313\n",
      "     25        1.0851       0.6017        \u001b[35m0.9498\u001b[0m  0.1317\n",
      "     26        1.0716       0.5975        \u001b[35m0.9472\u001b[0m  0.1314\n",
      "     27        1.0563       0.5975        \u001b[35m0.9446\u001b[0m  0.1235\n",
      "     28        1.0571       0.6017        \u001b[35m0.9423\u001b[0m  0.1110\n",
      "     29        1.0507       0.6081        \u001b[35m0.9408\u001b[0m  0.1132\n",
      "     30        1.0553       0.6102        \u001b[35m0.9405\u001b[0m  0.1147\n",
      "     31        1.0903       0.5975        0.9418  0.1131\n",
      "     32        1.0701       0.6017        0.9444  0.1109\n",
      "     33        1.0448       0.6081        0.9472  0.1107\n",
      "     34        1.0431       0.6144        0.9503  0.1111\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.2607\u001b[0m       \u001b[32m0.3750\u001b[0m        \u001b[35m1.1460\u001b[0m  0.1107\n",
      "      2        \u001b[36m1.2108\u001b[0m       \u001b[32m0.3941\u001b[0m        \u001b[35m1.1220\u001b[0m  0.1108\n",
      "      3        \u001b[36m1.1828\u001b[0m       \u001b[32m0.4258\u001b[0m        \u001b[35m1.0883\u001b[0m  0.1109\n",
      "      4        \u001b[36m1.1524\u001b[0m       \u001b[32m0.4576\u001b[0m        \u001b[35m1.0509\u001b[0m  0.1212\n",
      "      5        \u001b[36m1.0898\u001b[0m       \u001b[32m0.4852\u001b[0m        \u001b[35m1.0196\u001b[0m  0.1134\n",
      "      6        \u001b[36m1.0772\u001b[0m       \u001b[32m0.5000\u001b[0m        \u001b[35m1.0058\u001b[0m  0.1191\n",
      "      7        \u001b[36m1.0492\u001b[0m       \u001b[32m0.5275\u001b[0m        \u001b[35m0.9991\u001b[0m  0.1105\n",
      "      8        1.0557       \u001b[32m0.5551\u001b[0m        \u001b[35m0.9940\u001b[0m  0.1179\n",
      "      9        \u001b[36m1.0482\u001b[0m       \u001b[32m0.5890\u001b[0m        \u001b[35m0.9910\u001b[0m  0.1116\n",
      "     10        \u001b[36m1.0384\u001b[0m       \u001b[32m0.6081\u001b[0m        \u001b[35m0.9898\u001b[0m  0.1111\n",
      "     11        \u001b[36m1.0246\u001b[0m       \u001b[32m0.6250\u001b[0m        \u001b[35m0.9885\u001b[0m  0.1128\n",
      "     12        1.0298       \u001b[32m0.6271\u001b[0m        \u001b[35m0.9875\u001b[0m  0.1218\n",
      "     13        1.0389       0.6208        \u001b[35m0.9874\u001b[0m  0.1115\n",
      "     14        1.0394       0.6144        0.9876  0.1195\n",
      "     15        1.0319       0.6081        0.9876  0.1105\n",
      "     16        1.0426       0.5890        0.9882  0.1104\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.1305\u001b[0m       \u001b[32m0.4619\u001b[0m        \u001b[35m1.0745\u001b[0m  0.1108\n",
      "      2        \u001b[36m1.1267\u001b[0m       \u001b[32m0.4873\u001b[0m        \u001b[35m1.0671\u001b[0m  0.1107\n",
      "      3        \u001b[36m1.1267\u001b[0m       \u001b[32m0.5085\u001b[0m        \u001b[35m1.0566\u001b[0m  0.1133\n",
      "      4        \u001b[36m1.0926\u001b[0m       \u001b[32m0.5297\u001b[0m        \u001b[35m1.0467\u001b[0m  0.1209\n",
      "      5        \u001b[36m1.0887\u001b[0m       \u001b[32m0.5466\u001b[0m        \u001b[35m1.0413\u001b[0m  0.1130\n",
      "      6        \u001b[36m1.0842\u001b[0m       \u001b[32m0.5551\u001b[0m        \u001b[35m1.0401\u001b[0m  0.1188\n",
      "      7        \u001b[36m1.0813\u001b[0m       \u001b[32m0.5614\u001b[0m        1.0403  0.1109\n",
      "      8        \u001b[36m1.0569\u001b[0m       \u001b[32m0.5678\u001b[0m        1.0408  0.1108\n",
      "      9        1.0617       \u001b[32m0.5847\u001b[0m        1.0411  0.1109\n",
      "     10        1.0574       \u001b[32m0.6102\u001b[0m        1.0409  0.1108\n",
      "     11        \u001b[36m1.0311\u001b[0m       0.6081        \u001b[35m1.0390\u001b[0m  0.1125\n",
      "     12        1.0512       0.6038        \u001b[35m1.0349\u001b[0m  0.1208\n",
      "     13        1.0553       0.6017        \u001b[35m1.0304\u001b[0m  0.1109\n",
      "     14        1.0404       0.5996        \u001b[35m1.0256\u001b[0m  0.1214\n",
      "     15        1.0398       0.5911        \u001b[35m1.0199\u001b[0m  0.1132\n",
      "     16        \u001b[36m1.0245\u001b[0m       0.5826        \u001b[35m1.0137\u001b[0m  0.1151\n",
      "     17        1.0436       0.5847        \u001b[35m1.0068\u001b[0m  0.1104\n",
      "     18        1.0560       0.5911        \u001b[35m1.0017\u001b[0m  0.1130\n",
      "     19        1.0564       0.5890        \u001b[35m0.9980\u001b[0m  0.1212\n",
      "     20        1.0798       0.5826        \u001b[35m0.9941\u001b[0m  0.1129\n",
      "     21        1.0700       0.5847        \u001b[35m0.9904\u001b[0m  0.1162\n",
      "     22        1.0910       0.5869        \u001b[35m0.9864\u001b[0m  0.1110\n",
      "     23        1.0720       0.5869        \u001b[35m0.9827\u001b[0m  0.1105\n",
      "     24        1.0760       0.5826        \u001b[35m0.9793\u001b[0m  0.1101\n",
      "     25        1.0545       0.5826        \u001b[35m0.9765\u001b[0m  0.1126\n",
      "     26        1.0700       0.5826        \u001b[35m0.9743\u001b[0m  0.1191\n",
      "     27        1.0574       0.5763        \u001b[35m0.9729\u001b[0m  0.1111\n",
      "     28        1.0466       0.5742        0.9729  0.1105\n",
      "     29        1.0642       0.5826        0.9744  0.1123\n",
      "     30        1.0275       0.5847        0.9774  0.1215\n",
      "     31        1.0598       0.5869        0.9824  0.1215\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.1829\u001b[0m       \u001b[32m0.4047\u001b[0m        \u001b[35m1.1044\u001b[0m  0.1121\n",
      "      2        \u001b[36m1.1749\u001b[0m       \u001b[32m0.4343\u001b[0m        \u001b[35m1.0881\u001b[0m  0.1211\n",
      "      3        \u001b[36m1.1321\u001b[0m       \u001b[32m0.4661\u001b[0m        \u001b[35m1.0658\u001b[0m  0.1215\n",
      "      4        \u001b[36m1.1104\u001b[0m       \u001b[32m0.5212\u001b[0m        \u001b[35m1.0436\u001b[0m  0.1190\n",
      "      5        \u001b[36m1.0866\u001b[0m       \u001b[32m0.5784\u001b[0m        \u001b[35m1.0296\u001b[0m  0.1206\n",
      "      6        \u001b[36m1.0567\u001b[0m       \u001b[32m0.5805\u001b[0m        \u001b[35m1.0255\u001b[0m  0.1638\n",
      "      7        \u001b[36m1.0504\u001b[0m       \u001b[32m0.5826\u001b[0m        \u001b[35m1.0226\u001b[0m  0.1500\n",
      "      8        \u001b[36m1.0475\u001b[0m       \u001b[32m0.6123\u001b[0m        \u001b[35m1.0185\u001b[0m  0.1192\n",
      "      9        1.0478       \u001b[32m0.6165\u001b[0m        \u001b[35m1.0140\u001b[0m  0.1219\n",
      "     10        \u001b[36m1.0351\u001b[0m       0.6123        \u001b[35m1.0071\u001b[0m  0.1109\n",
      "     11        \u001b[36m1.0295\u001b[0m       \u001b[32m0.6250\u001b[0m        \u001b[35m0.9987\u001b[0m  0.1112\n",
      "     12        \u001b[36m1.0260\u001b[0m       0.6081        \u001b[35m0.9885\u001b[0m  0.1211\n",
      "     13        1.0266       0.5953        \u001b[35m0.9784\u001b[0m  0.1109\n",
      "     14        \u001b[36m1.0124\u001b[0m       0.5932        \u001b[35m0.9699\u001b[0m  0.1105\n",
      "     15        1.0425       0.5932        \u001b[35m0.9630\u001b[0m  0.1131\n",
      "     16        1.0575       0.5911        \u001b[35m0.9579\u001b[0m  0.1127\n",
      "     17        1.0440       0.5911        \u001b[35m0.9540\u001b[0m  0.1132\n",
      "     18        1.0415       0.5911        \u001b[35m0.9507\u001b[0m  0.1211\n",
      "     19        1.0574       0.5911        \u001b[35m0.9475\u001b[0m  0.1213\n",
      "     20        1.0675       0.5932        \u001b[35m0.9442\u001b[0m  0.1208\n",
      "     21        1.0767       0.5953        \u001b[35m0.9410\u001b[0m  0.1117\n",
      "     22        1.0844       0.6017        \u001b[35m0.9382\u001b[0m  0.1108\n",
      "     23        1.0653       0.6059        \u001b[35m0.9357\u001b[0m  0.1111\n",
      "     24        1.0524       0.6038        \u001b[35m0.9339\u001b[0m  0.1209\n",
      "     25        1.0643       0.6144        \u001b[35m0.9336\u001b[0m  0.1320\n",
      "     26        1.0227       0.6102        0.9345  0.1236\n",
      "     27        1.0481       0.6165        0.9371  0.1131\n",
      "     28        1.0301       0.6123        0.9414  0.1212\n",
      "     29        1.0299       0.6144        0.9482  0.1197\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.2883\u001b[0m       \u001b[32m0.2585\u001b[0m        \u001b[35m1.1956\u001b[0m  0.1111\n",
      "      2        \u001b[36m1.2611\u001b[0m       \u001b[32m0.2839\u001b[0m        \u001b[35m1.1666\u001b[0m  0.1215\n",
      "      3        \u001b[36m1.2166\u001b[0m       \u001b[32m0.3347\u001b[0m        \u001b[35m1.1254\u001b[0m  0.1191\n",
      "      4        \u001b[36m1.1490\u001b[0m       \u001b[32m0.4237\u001b[0m        \u001b[35m1.0804\u001b[0m  0.1209\n",
      "      5        \u001b[36m1.1049\u001b[0m       \u001b[32m0.5148\u001b[0m        \u001b[35m1.0430\u001b[0m  0.1201\n",
      "      6        \u001b[36m1.0717\u001b[0m       \u001b[32m0.5466\u001b[0m        \u001b[35m1.0262\u001b[0m  0.1188\n",
      "      7        \u001b[36m1.0549\u001b[0m       \u001b[32m0.5614\u001b[0m        \u001b[35m1.0201\u001b[0m  0.1113\n",
      "      8        \u001b[36m1.0526\u001b[0m       \u001b[32m0.5805\u001b[0m        \u001b[35m1.0153\u001b[0m  0.1109\n",
      "      9        \u001b[36m1.0509\u001b[0m       \u001b[32m0.6038\u001b[0m        \u001b[35m1.0105\u001b[0m  0.1108\n",
      "     10        1.0518       \u001b[32m0.6102\u001b[0m        \u001b[35m1.0045\u001b[0m  0.1109\n",
      "     11        \u001b[36m1.0425\u001b[0m       0.6081        \u001b[35m0.9978\u001b[0m  0.1209\n",
      "     12        1.0478       0.6059        \u001b[35m0.9895\u001b[0m  0.1213\n",
      "     13        \u001b[36m1.0336\u001b[0m       0.6059        \u001b[35m0.9797\u001b[0m  0.1130\n",
      "     14        1.0362       0.6038        \u001b[35m0.9707\u001b[0m  0.1214\n",
      "     15        1.0352       0.6038        \u001b[35m0.9623\u001b[0m  0.1190\n",
      "     16        1.0531       0.6038        \u001b[35m0.9555\u001b[0m  0.1233\n",
      "     17        1.0376       0.5953        \u001b[35m0.9514\u001b[0m  0.1122\n",
      "     18        1.0577       0.5953        \u001b[35m0.9499\u001b[0m  0.1191\n",
      "     19        1.0748       0.5996        0.9507  0.1210\n",
      "     20        1.0577       0.5932        0.9533  0.1212\n",
      "     21        1.0797       0.5763        0.9568  0.1236\n",
      "     22        1.0663       0.5742        0.9602  0.1312\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.2259\u001b[0m       \u001b[32m0.4364\u001b[0m        \u001b[35m1.1028\u001b[0m  0.1212\n",
      "      2        \u001b[36m1.1040\u001b[0m       \u001b[32m0.6038\u001b[0m        \u001b[35m1.0213\u001b[0m  0.1215\n",
      "      3        \u001b[36m1.0568\u001b[0m       0.5996        \u001b[35m1.0025\u001b[0m  0.1214\n",
      "      4        \u001b[36m1.0347\u001b[0m       0.5975        \u001b[35m0.9798\u001b[0m  0.1216\n",
      "      5        \u001b[36m1.0336\u001b[0m       0.5826        \u001b[35m0.9567\u001b[0m  0.1213\n",
      "      6        \u001b[36m1.0157\u001b[0m       0.5847        \u001b[35m0.9481\u001b[0m  0.1315\n",
      "      7        1.0657       0.6038        \u001b[35m0.9441\u001b[0m  0.1318\n",
      "      8        1.0912       \u001b[32m0.6059\u001b[0m        \u001b[35m0.9360\u001b[0m  0.1320\n",
      "      9        1.0777       \u001b[32m0.6123\u001b[0m        \u001b[35m0.9256\u001b[0m  0.1235\n",
      "     10        1.0581       \u001b[32m0.6398\u001b[0m        0.9262  0.1313\n",
      "     11        1.0415       \u001b[32m0.6441\u001b[0m        0.9515  0.1317\n",
      "     12        1.0366       \u001b[32m0.6483\u001b[0m        0.9764  0.1216\n",
      "     13        1.0587       0.6377        0.9915  0.1295\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.1905\u001b[0m       \u001b[32m0.4386\u001b[0m        \u001b[35m1.1141\u001b[0m  0.1216\n",
      "      2        \u001b[36m1.0989\u001b[0m       \u001b[32m0.5466\u001b[0m        \u001b[35m1.0577\u001b[0m  0.1216\n",
      "      3        \u001b[36m1.0583\u001b[0m       \u001b[32m0.5953\u001b[0m        \u001b[35m1.0512\u001b[0m  0.1316\n",
      "      4        \u001b[36m1.0432\u001b[0m       0.5890        \u001b[35m1.0406\u001b[0m  0.1214\n",
      "      5        1.0624       0.5678        \u001b[35m1.0310\u001b[0m  0.1136\n",
      "      6        1.0584       0.5614        \u001b[35m1.0189\u001b[0m  0.1108\n",
      "      7        1.0663       0.5699        \u001b[35m0.9954\u001b[0m  0.1190\n",
      "      8        1.0768       0.5699        \u001b[35m0.9696\u001b[0m  0.1121\n",
      "      9        1.0482       0.5784        \u001b[35m0.9545\u001b[0m  0.1108\n",
      "     10        1.0718       0.5911        \u001b[35m0.9510\u001b[0m  0.1110\n",
      "     11        1.0674       \u001b[32m0.6038\u001b[0m        0.9626  0.1108\n",
      "     12        1.0522       0.6038        0.9871  0.1313\n",
      "     13        1.0470       \u001b[32m0.6081\u001b[0m        1.0124  0.1291\n",
      "     14        1.0653       \u001b[32m0.6123\u001b[0m        1.0276  0.1210\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.1398\u001b[0m       \u001b[32m0.5106\u001b[0m        \u001b[35m1.0893\u001b[0m  0.1114\n",
      "      2        \u001b[36m1.0983\u001b[0m       \u001b[32m0.5636\u001b[0m        \u001b[35m1.0588\u001b[0m  0.1208\n",
      "      3        \u001b[36m1.0693\u001b[0m       \u001b[32m0.6186\u001b[0m        \u001b[35m1.0551\u001b[0m  0.1189\n",
      "      4        \u001b[36m1.0688\u001b[0m       0.5911        \u001b[35m1.0434\u001b[0m  0.1216\n",
      "      5        \u001b[36m1.0657\u001b[0m       0.5678        \u001b[35m1.0311\u001b[0m  0.1316\n",
      "      6        \u001b[36m1.0635\u001b[0m       0.5784        \u001b[35m1.0133\u001b[0m  0.1129\n",
      "      7        \u001b[36m1.0553\u001b[0m       0.5784        \u001b[35m0.9854\u001b[0m  0.1315\n",
      "      8        1.0577       0.5911        \u001b[35m0.9599\u001b[0m  0.1219\n",
      "      9        1.0715       0.6059        \u001b[35m0.9469\u001b[0m  0.1110\n",
      "     10        1.0889       0.6102        0.9481  0.1107\n",
      "     11        1.0860       0.6144        0.9538  0.1109\n",
      "     12        \u001b[36m1.0396\u001b[0m       \u001b[32m0.6208\u001b[0m        0.9696  0.1209\n",
      "     13        1.0444       \u001b[32m0.6229\u001b[0m        0.9993  0.1107\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.1789\u001b[0m       \u001b[32m0.5275\u001b[0m        \u001b[35m1.0565\u001b[0m  0.1107\n",
      "      2        \u001b[36m1.0772\u001b[0m       \u001b[32m0.6165\u001b[0m        \u001b[35m1.0155\u001b[0m  0.1196\n",
      "      3        \u001b[36m1.0387\u001b[0m       0.5890        \u001b[35m1.0045\u001b[0m  0.1111\n",
      "      4        \u001b[36m1.0350\u001b[0m       0.5720        \u001b[35m0.9883\u001b[0m  0.1107\n",
      "      5        1.0418       0.5911        \u001b[35m0.9721\u001b[0m  0.1190\n",
      "      6        1.0715       0.5911        \u001b[35m0.9561\u001b[0m  0.1110\n",
      "      7        1.0488       0.6081        \u001b[35m0.9397\u001b[0m  0.1117\n",
      "      8        1.0704       \u001b[32m0.6250\u001b[0m        \u001b[35m0.9362\u001b[0m  0.1110\n",
      "      9        1.0512       \u001b[32m0.6292\u001b[0m        0.9567  0.1113\n",
      "     10        1.0651       0.6292        0.9873  0.1211\n",
      "     11        1.0505       0.6229        1.0161  0.1132\n",
      "     12        1.0713       0.6144        1.0325  0.1109\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.2249\u001b[0m       \u001b[32m0.3559\u001b[0m        \u001b[35m1.1277\u001b[0m  0.1186\n",
      "      2        \u001b[36m1.1042\u001b[0m       \u001b[32m0.5466\u001b[0m        \u001b[35m1.0592\u001b[0m  0.1209\n",
      "      3        \u001b[36m1.0710\u001b[0m       \u001b[32m0.5487\u001b[0m        \u001b[35m1.0434\u001b[0m  0.1211\n",
      "      4        1.0739       0.5466        \u001b[35m1.0256\u001b[0m  0.1187\n",
      "      5        1.0772       \u001b[32m0.5530\u001b[0m        \u001b[35m1.0119\u001b[0m  0.1212\n",
      "      6        1.0806       \u001b[32m0.5699\u001b[0m        \u001b[35m1.0011\u001b[0m  0.1119\n",
      "      7        1.0737       \u001b[32m0.5847\u001b[0m        \u001b[35m0.9876\u001b[0m  0.1110\n",
      "      8        1.0797       \u001b[32m0.6081\u001b[0m        \u001b[35m0.9702\u001b[0m  0.1114\n",
      "      9        \u001b[36m1.0535\u001b[0m       \u001b[32m0.6144\u001b[0m        \u001b[35m0.9599\u001b[0m  0.1114\n",
      "     10        1.0641       0.6059        0.9665  0.1190\n",
      "     11        1.0577       0.5996        0.9811  0.1215\n",
      "     12        1.0604       0.6059        0.9949  0.1215\n",
      "     13        1.0593       0.6102        1.0057  0.1235\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.2205\u001b[0m       \u001b[32m0.4746\u001b[0m        \u001b[35m1.0709\u001b[0m  0.1108\n",
      "      2        \u001b[36m1.0932\u001b[0m       \u001b[32m0.5996\u001b[0m        \u001b[35m0.9888\u001b[0m  0.1111\n",
      "      3        \u001b[36m1.0209\u001b[0m       \u001b[32m0.6123\u001b[0m        \u001b[35m0.9680\u001b[0m  0.1110\n",
      "      4        1.0424       0.6017        \u001b[35m0.9566\u001b[0m  0.1110\n",
      "      5        1.0543       0.6102        \u001b[35m0.9560\u001b[0m  0.1131\n",
      "      6        1.0598       0.5996        0.9578  0.1189\n",
      "      7        1.0659       \u001b[32m0.6165\u001b[0m        0.9583  0.1107\n",
      "      8        1.0636       0.6144        0.9599  0.1211\n",
      "      9        1.0414       0.6038        0.9682  0.1215\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.1861\u001b[0m       \u001b[32m0.4619\u001b[0m        \u001b[35m1.1005\u001b[0m  0.1127\n",
      "      2        \u001b[36m1.1048\u001b[0m       \u001b[32m0.5890\u001b[0m        \u001b[35m1.0567\u001b[0m  0.1210\n",
      "      3        \u001b[36m1.0780\u001b[0m       \u001b[32m0.5953\u001b[0m        \u001b[35m1.0415\u001b[0m  0.1188\n",
      "      4        \u001b[36m1.0742\u001b[0m       \u001b[32m0.6038\u001b[0m        \u001b[35m1.0302\u001b[0m  0.1107\n",
      "      5        1.0797       0.5657        \u001b[35m1.0264\u001b[0m  0.1110\n",
      "      6        \u001b[36m1.0676\u001b[0m       0.5530        \u001b[35m1.0254\u001b[0m  0.1109\n",
      "      7        1.0800       0.5487        1.0272  0.1111\n",
      "      8        1.0876       0.5466        1.0279  0.1219\n",
      "      9        1.0962       0.5614        \u001b[35m1.0242\u001b[0m  0.1195\n",
      "     10        1.1040       0.5678        \u001b[35m1.0235\u001b[0m  0.1108\n",
      "     11        1.0783       0.5657        1.0277  0.1110\n",
      "     12        \u001b[36m1.0599\u001b[0m       0.5742        1.0331  0.1115\n",
      "     13        \u001b[36m1.0562\u001b[0m       0.5911        1.0271  0.1111\n",
      "     14        1.0650       0.6038        1.0277  0.1111\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.1814\u001b[0m       \u001b[32m0.4449\u001b[0m        \u001b[35m1.0668\u001b[0m  0.1109\n",
      "      2        \u001b[36m1.0914\u001b[0m       \u001b[32m0.5911\u001b[0m        \u001b[35m0.9935\u001b[0m  0.1193\n",
      "      3        \u001b[36m1.0323\u001b[0m       \u001b[32m0.6250\u001b[0m        \u001b[35m0.9610\u001b[0m  0.1105\n",
      "      4        \u001b[36m1.0120\u001b[0m       0.6208        \u001b[35m0.9490\u001b[0m  0.1107\n",
      "      5        1.0452       0.5911        0.9562  0.1125\n",
      "      6        1.0910       0.5953        0.9604  0.1110\n",
      "      7        1.0707       0.6081        0.9569  0.1108\n",
      "      8        1.0488       0.5996        0.9551  0.1107\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.3340\u001b[0m       \u001b[32m0.3432\u001b[0m        \u001b[35m1.1531\u001b[0m  0.1130\n",
      "      2        \u001b[36m1.1314\u001b[0m       \u001b[32m0.4831\u001b[0m        \u001b[35m1.0668\u001b[0m  0.1211\n",
      "      3        \u001b[36m1.0762\u001b[0m       \u001b[32m0.5784\u001b[0m        \u001b[35m1.0559\u001b[0m  0.1108\n",
      "      4        1.0888       0.5551        \u001b[35m1.0518\u001b[0m  0.1212\n",
      "      5        1.0952       \u001b[32m0.5805\u001b[0m        \u001b[35m1.0483\u001b[0m  0.1109\n",
      "      6        1.1051       \u001b[32m0.5911\u001b[0m        \u001b[35m1.0379\u001b[0m  0.1109\n",
      "      7        1.0765       \u001b[32m0.6017\u001b[0m        \u001b[35m1.0161\u001b[0m  0.1105\n",
      "      8        \u001b[36m1.0711\u001b[0m       \u001b[32m0.6102\u001b[0m        \u001b[35m0.9819\u001b[0m  0.1113\n",
      "      9        1.0760       \u001b[32m0.6144\u001b[0m        \u001b[35m0.9563\u001b[0m  0.1186\n",
      "     10        1.0750       \u001b[32m0.6165\u001b[0m        0.9563  0.1108\n",
      "     11        \u001b[36m1.0543\u001b[0m       0.6144        0.9717  0.1112\n",
      "     12        \u001b[36m1.0386\u001b[0m       0.6102        0.9883  0.1129\n",
      "     13        1.0438       0.6102        1.0029  0.1109\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.1845\u001b[0m       \u001b[32m0.4936\u001b[0m        \u001b[35m1.0477\u001b[0m  0.1113\n",
      "      2        \u001b[36m1.0769\u001b[0m       \u001b[32m0.5763\u001b[0m        \u001b[35m0.9900\u001b[0m  0.1108\n",
      "      3        \u001b[36m1.0215\u001b[0m       \u001b[32m0.6250\u001b[0m        \u001b[35m0.9752\u001b[0m  0.1194\n",
      "      4        \u001b[36m1.0187\u001b[0m       0.6144        \u001b[35m0.9612\u001b[0m  0.1396\n",
      "      5        1.0368       0.6038        0.9614  0.1313\n",
      "      6        1.0645       0.5847        0.9735  0.1235\n",
      "      7        1.0803       0.5826        0.9752  0.1216\n",
      "      8        1.0420       0.5890        0.9678  0.1219\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.3067\u001b[0m       \u001b[32m0.2797\u001b[0m        \u001b[35m1.1605\u001b[0m  0.1295\n",
      "      2        \u001b[36m1.1349\u001b[0m       \u001b[32m0.5487\u001b[0m        \u001b[35m1.0708\u001b[0m  0.1316\n",
      "      3        \u001b[36m1.0892\u001b[0m       \u001b[32m0.5720\u001b[0m        \u001b[35m1.0589\u001b[0m  0.1315\n",
      "      4        \u001b[36m1.0743\u001b[0m       0.5657        \u001b[35m1.0477\u001b[0m  0.1293\n",
      "      5        1.0754       0.5699        \u001b[35m1.0415\u001b[0m  0.1315\n",
      "      6        1.0896       0.5699        \u001b[35m1.0353\u001b[0m  0.1232\n",
      "      7        1.0839       \u001b[32m0.5742\u001b[0m        \u001b[35m1.0271\u001b[0m  0.1296\n",
      "      8        1.0883       0.5699        \u001b[35m1.0131\u001b[0m  0.1412\n",
      "      9        \u001b[36m1.0544\u001b[0m       \u001b[32m0.5784\u001b[0m        \u001b[35m0.9868\u001b[0m  0.1315\n",
      "     10        \u001b[36m1.0490\u001b[0m       0.5720        \u001b[35m0.9728\u001b[0m  0.1310\n",
      "     11        1.0573       0.5699        0.9838  0.1294\n",
      "     12        1.0769       0.5784        0.9909  0.1315\n",
      "     13        1.0716       \u001b[32m0.5953\u001b[0m        0.9911  0.1293\n",
      "     14        \u001b[36m1.0422\u001b[0m       \u001b[32m0.5975\u001b[0m        0.9895  0.1328\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.1825\u001b[0m       \u001b[32m0.4025\u001b[0m        \u001b[35m1.0839\u001b[0m  0.1132\n",
      "      2        \u001b[36m1.0734\u001b[0m       \u001b[32m0.5360\u001b[0m        \u001b[35m1.0229\u001b[0m  0.1212\n",
      "      3        \u001b[36m1.0694\u001b[0m       \u001b[32m0.6038\u001b[0m        \u001b[35m1.0086\u001b[0m  0.1132\n",
      "      4        \u001b[36m1.0562\u001b[0m       0.6017        \u001b[35m0.9884\u001b[0m  0.1211\n",
      "      5        \u001b[36m1.0440\u001b[0m       0.5911        \u001b[35m0.9601\u001b[0m  0.1107\n",
      "      6        1.0458       0.5890        \u001b[35m0.9449\u001b[0m  0.1213\n",
      "      7        1.0748       0.5805        0.9589  0.1190\n",
      "      8        1.0907       0.5911        0.9657  0.1109\n",
      "      9        1.0873       \u001b[32m0.6081\u001b[0m        0.9673  0.1105\n",
      "     10        1.0600       \u001b[32m0.6229\u001b[0m        0.9769  0.1107\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.1564\u001b[0m       \u001b[32m0.4979\u001b[0m        \u001b[35m1.0484\u001b[0m  0.1108\n",
      "      2        \u001b[36m1.1001\u001b[0m       \u001b[32m0.6038\u001b[0m        \u001b[35m1.0090\u001b[0m  0.1130\n",
      "      3        \u001b[36m1.0580\u001b[0m       0.5996        \u001b[35m0.9983\u001b[0m  0.1190\n",
      "      4        \u001b[36m1.0406\u001b[0m       0.5869        1.0001  0.1211\n",
      "      5        1.0654       0.5742        1.0091  0.1140\n",
      "      6        1.0830       0.5614        1.0196  0.1236\n",
      "      7        1.0781       0.5699        1.0265  0.1131\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.2585\u001b[0m       \u001b[32m0.4237\u001b[0m        \u001b[35m1.1069\u001b[0m  0.1109\n",
      "      2        \u001b[36m1.0882\u001b[0m       \u001b[32m0.5678\u001b[0m        \u001b[35m1.0342\u001b[0m  0.1113\n",
      "      3        \u001b[36m1.0419\u001b[0m       \u001b[32m0.5869\u001b[0m        \u001b[35m1.0234\u001b[0m  0.1109\n",
      "      4        1.0482       0.5847        \u001b[35m1.0227\u001b[0m  0.1107\n",
      "      5        1.0669       0.5742        \u001b[35m1.0188\u001b[0m  0.1189\n",
      "      6        1.0638       0.5593        \u001b[35m1.0035\u001b[0m  0.1106\n",
      "      7        1.0679       0.5530        \u001b[35m0.9777\u001b[0m  0.1107\n",
      "      8        1.0707       0.5742        \u001b[35m0.9594\u001b[0m  0.1107\n",
      "      9        1.0656       \u001b[32m0.5932\u001b[0m        \u001b[35m0.9553\u001b[0m  0.1189\n",
      "     10        1.0642       \u001b[32m0.6123\u001b[0m        0.9619  0.1215\n",
      "     11        1.0471       0.6102        0.9757  0.1215\n",
      "     12        \u001b[36m1.0298\u001b[0m       0.6123        0.9919  0.1208\n",
      "     13        1.0594       \u001b[32m0.6165\u001b[0m        1.0020  0.1195\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.1266\u001b[0m       \u001b[32m0.4852\u001b[0m        \u001b[35m1.0646\u001b[0m  0.1105\n",
      "      2        \u001b[36m1.0846\u001b[0m       \u001b[32m0.5932\u001b[0m        \u001b[35m1.0372\u001b[0m  0.1108\n",
      "      3        \u001b[36m1.0663\u001b[0m       \u001b[32m0.5953\u001b[0m        \u001b[35m1.0204\u001b[0m  0.1202\n",
      "      4        \u001b[36m1.0631\u001b[0m       \u001b[32m0.5975\u001b[0m        \u001b[35m1.0004\u001b[0m  0.1112\n",
      "      5        \u001b[36m1.0460\u001b[0m       \u001b[32m0.6038\u001b[0m        \u001b[35m0.9740\u001b[0m  0.1106\n",
      "      6        \u001b[36m1.0434\u001b[0m       0.5996        \u001b[35m0.9551\u001b[0m  0.1109\n",
      "      7        1.0648       0.5953        \u001b[35m0.9508\u001b[0m  0.1107\n",
      "      8        1.0674       \u001b[32m0.6102\u001b[0m        0.9523  0.1116\n",
      "      9        1.0493       0.6102        0.9606  0.1213\n",
      "     10        \u001b[36m1.0292\u001b[0m       0.5890        0.9743  0.1190\n",
      "     11        1.0431       0.5805        0.9903  0.1110\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.1626\u001b[0m       \u001b[32m0.4364\u001b[0m        \u001b[35m1.0870\u001b[0m  0.1210\n",
      "      2        \u001b[36m1.1003\u001b[0m       \u001b[32m0.5466\u001b[0m        \u001b[35m1.0343\u001b[0m  0.1236\n",
      "      3        \u001b[36m1.0614\u001b[0m       \u001b[32m0.6292\u001b[0m        \u001b[35m1.0073\u001b[0m  0.1109\n",
      "      4        \u001b[36m1.0466\u001b[0m       0.6271        \u001b[35m0.9895\u001b[0m  0.1112\n",
      "      5        1.0677       0.6059        \u001b[35m0.9869\u001b[0m  0.1115\n",
      "      6        1.0904       0.5932        0.9951  0.1140\n",
      "      7        1.0755       0.5805        1.0038  0.1190\n",
      "      8        1.0891       0.5678        1.0072  0.1113\n",
      "      9        1.1001       0.5869        1.0046  0.1212\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.1966\u001b[0m       \u001b[32m0.3623\u001b[0m        \u001b[35m1.1256\u001b[0m  0.1131\n",
      "      2        \u001b[36m1.1177\u001b[0m       \u001b[32m0.4767\u001b[0m        \u001b[35m1.0602\u001b[0m  0.1212\n",
      "      3        \u001b[36m1.0683\u001b[0m       \u001b[32m0.6038\u001b[0m        \u001b[35m1.0443\u001b[0m  0.1234\n",
      "      4        \u001b[36m1.0676\u001b[0m       0.5953        \u001b[35m1.0386\u001b[0m  0.1212\n",
      "      5        1.0720       0.5932        \u001b[35m1.0286\u001b[0m  0.1212\n",
      "      6        1.0742       0.6017        \u001b[35m1.0192\u001b[0m  0.1232\n",
      "      7        1.0727       0.6017        \u001b[35m1.0104\u001b[0m  0.1235\n",
      "      8        1.0750       0.5975        \u001b[35m1.0014\u001b[0m  0.1236\n",
      "      9        1.0748       0.5975        \u001b[35m0.9944\u001b[0m  0.1126\n",
      "     10        1.0880       0.5911        0.9945  0.1200\n",
      "     11        1.0710       0.5890        \u001b[35m0.9916\u001b[0m  0.1035\n",
      "     12        \u001b[36m1.0523\u001b[0m       \u001b[32m0.6059\u001b[0m        \u001b[35m0.9840\u001b[0m  0.1111\n",
      "     13        \u001b[36m1.0371\u001b[0m       \u001b[32m0.6123\u001b[0m        \u001b[35m0.9822\u001b[0m  0.1112\n",
      "     14        1.0472       \u001b[32m0.6144\u001b[0m        0.9876  0.1219\n",
      "     15        1.0595       \u001b[32m0.6271\u001b[0m        0.9967  0.1236\n",
      "     16        1.0587       \u001b[32m0.6335\u001b[0m        1.0077  0.1215\n",
      "     17        1.0443       0.6335        1.0187  0.1194\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.1080\u001b[0m       \u001b[32m0.4873\u001b[0m        \u001b[35m1.0487\u001b[0m  0.1110\n",
      "      2        \u001b[36m1.0600\u001b[0m       \u001b[32m0.5932\u001b[0m        \u001b[35m1.0144\u001b[0m  0.1190\n",
      "      3        \u001b[36m1.0515\u001b[0m       \u001b[32m0.6144\u001b[0m        \u001b[35m0.9920\u001b[0m  0.1191\n",
      "      4        \u001b[36m1.0329\u001b[0m       0.6144        \u001b[35m0.9640\u001b[0m  0.1188\n",
      "      5        \u001b[36m1.0277\u001b[0m       0.6017        \u001b[35m0.9553\u001b[0m  0.1196\n",
      "      6        1.0597       0.6059        0.9591  0.1135\n",
      "      7        1.0754       0.5932        0.9618  0.1210\n",
      "      8        1.0617       0.5890        0.9594  0.1214\n",
      "      9        1.0304       0.6038        0.9653  0.1188\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.1522\u001b[0m       \u001b[32m0.5191\u001b[0m        \u001b[35m1.0476\u001b[0m  0.1106\n",
      "      2        \u001b[36m1.0682\u001b[0m       \u001b[32m0.5742\u001b[0m        \u001b[35m0.9898\u001b[0m  0.1119\n",
      "      3        \u001b[36m1.0166\u001b[0m       \u001b[32m0.6144\u001b[0m        \u001b[35m0.9606\u001b[0m  0.1108\n",
      "      4        \u001b[36m1.0142\u001b[0m       \u001b[32m0.6292\u001b[0m        \u001b[35m0.9446\u001b[0m  0.1108\n",
      "      5        1.0439       0.5720        0.9638  0.1131\n",
      "      6        1.0577       0.5360        0.9865  0.1210\n",
      "      7        1.0730       0.5445        0.9910  0.1209\n",
      "      8        1.0514       0.5784        0.9829  0.1195\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.1378\u001b[0m       \u001b[32m0.4534\u001b[0m        \u001b[35m1.0883\u001b[0m  0.1105\n",
      "      2        \u001b[36m1.1010\u001b[0m       \u001b[32m0.5657\u001b[0m        \u001b[35m1.0472\u001b[0m  0.1108\n",
      "      3        \u001b[36m1.0560\u001b[0m       \u001b[32m0.6229\u001b[0m        \u001b[35m1.0305\u001b[0m  0.1111\n",
      "      4        \u001b[36m1.0511\u001b[0m       0.6229        \u001b[35m1.0144\u001b[0m  0.1217\n",
      "      5        \u001b[36m1.0510\u001b[0m       0.6102        \u001b[35m0.9974\u001b[0m  0.1212\n",
      "      6        1.0635       0.6102        \u001b[35m0.9808\u001b[0m  0.1207\n",
      "      7        1.0602       0.6059        \u001b[35m0.9680\u001b[0m  0.1211\n",
      "      8        1.0722       0.6038        \u001b[35m0.9601\u001b[0m  0.1213\n",
      "      9        1.0822       0.6017        0.9621  0.1108\n",
      "     10        1.0943       0.5932        0.9698  0.1126\n",
      "     11        1.0798       0.6059        0.9766  0.1215\n",
      "     12        \u001b[36m1.0350\u001b[0m       \u001b[32m0.6271\u001b[0m        0.9737  0.1112\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.1127\u001b[0m       \u001b[32m0.5678\u001b[0m        \u001b[35m1.0607\u001b[0m  0.1233\n",
      "      2        \u001b[36m1.0719\u001b[0m       \u001b[32m0.5847\u001b[0m        \u001b[35m1.0154\u001b[0m  0.1239\n",
      "      3        1.0733       \u001b[32m0.6165\u001b[0m        \u001b[35m0.9626\u001b[0m  0.1320\n",
      "      4        1.0876       0.6081        1.0215  0.1317\n",
      "      5        1.0940       0.6038        1.0853  0.1293\n",
      "      6        1.0882       \u001b[32m0.6186\u001b[0m        1.0600  0.1215\n",
      "      7        1.0746       \u001b[32m0.6208\u001b[0m        1.0275  0.1220\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.1397\u001b[0m       \u001b[32m0.6059\u001b[0m        \u001b[35m1.0349\u001b[0m  0.1236\n",
      "      2        \u001b[36m1.0554\u001b[0m       0.5551        \u001b[35m1.0208\u001b[0m  0.1213\n",
      "      3        1.0772       0.5742        \u001b[35m1.0148\u001b[0m  0.1293\n",
      "      4        1.0611       0.5996        1.0218  0.1313\n",
      "      5        1.0687       \u001b[32m0.6102\u001b[0m        \u001b[35m1.0142\u001b[0m  0.1236\n",
      "      6        1.0837       0.6102        1.0242  0.1314\n",
      "      7        \u001b[36m1.0553\u001b[0m       0.6081        1.0234  0.1315\n",
      "      8        \u001b[36m1.0493\u001b[0m       0.6081        1.0257  0.1235\n",
      "      9        1.0718       0.6102        1.0281  0.1213\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.0956\u001b[0m       \u001b[32m0.5657\u001b[0m        \u001b[35m1.0405\u001b[0m  0.1317\n",
      "      2        \u001b[36m1.0666\u001b[0m       0.5466        \u001b[35m1.0145\u001b[0m  0.1315\n",
      "      3        1.0766       \u001b[32m0.6186\u001b[0m        \u001b[35m0.9960\u001b[0m  0.1317\n",
      "      4        \u001b[36m1.0652\u001b[0m       \u001b[32m0.6356\u001b[0m        1.0057  0.1295\n",
      "      5        1.0794       0.6208        1.0249  0.1190\n",
      "      6        1.0746       0.6335        1.0407  0.1107\n",
      "      7        1.0754       \u001b[32m0.6419\u001b[0m        1.0611  0.1106\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.0977\u001b[0m       \u001b[32m0.6123\u001b[0m        \u001b[35m1.0179\u001b[0m  0.1189\n",
      "      2        \u001b[36m1.0656\u001b[0m       0.5551        \u001b[35m0.9922\u001b[0m  0.1252\n",
      "      3        1.0834       0.5784        \u001b[35m0.9819\u001b[0m  0.1122\n",
      "      4        \u001b[36m1.0600\u001b[0m       0.6123        1.0031  0.1132\n",
      "      5        1.0794       \u001b[32m0.6208\u001b[0m        1.0372  0.1106\n",
      "      6        1.0828       0.6208        1.0679  0.1130\n",
      "      7        1.1000       \u001b[32m0.6314\u001b[0m        1.0740  0.1188\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.1136\u001b[0m       \u001b[32m0.6144\u001b[0m        \u001b[35m1.0043\u001b[0m  0.1211\n",
      "      2        \u001b[36m1.0552\u001b[0m       0.5975        \u001b[35m0.9574\u001b[0m  0.1115\n",
      "      3        1.0586       0.5975        0.9807  0.1109\n",
      "      4        1.0629       0.5975        1.0202  0.1131\n",
      "      5        1.0632       0.6038        1.0501  0.1213\n",
      "      6        1.0731       0.6017        1.0519  0.1193\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.0922\u001b[0m       \u001b[32m0.6081\u001b[0m        \u001b[35m0.9433\u001b[0m  0.1187\n",
      "      2        \u001b[36m1.0366\u001b[0m       0.5784        0.9678  0.1189\n",
      "      3        1.0698       0.6038        0.9905  0.1190\n",
      "      4        1.0604       \u001b[32m0.6356\u001b[0m        1.0189  0.1217\n",
      "      5        1.1122       \u001b[32m0.6377\u001b[0m        1.0519  0.1215\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.1284\u001b[0m       \u001b[32m0.5551\u001b[0m        \u001b[35m1.0228\u001b[0m  0.1134\n",
      "      2        \u001b[36m1.0908\u001b[0m       \u001b[32m0.5847\u001b[0m        \u001b[35m1.0189\u001b[0m  0.1111\n",
      "      3        \u001b[36m1.0690\u001b[0m       \u001b[32m0.5890\u001b[0m        1.0382  0.1192\n",
      "      4        \u001b[36m1.0555\u001b[0m       \u001b[32m0.6017\u001b[0m        \u001b[35m1.0120\u001b[0m  0.1218\n",
      "      5        1.0755       0.6017        1.0144  0.1132\n",
      "      6        1.0724       \u001b[32m0.6081\u001b[0m        1.0594  0.1190\n",
      "      7        1.0773       \u001b[32m0.6102\u001b[0m        1.0824  0.1107\n",
      "      8        1.0900       0.6102        1.0861  0.1108\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.1063\u001b[0m       \u001b[32m0.5784\u001b[0m        \u001b[35m1.0259\u001b[0m  0.1109\n",
      "      2        \u001b[36m1.0662\u001b[0m       0.5636        \u001b[35m1.0015\u001b[0m  0.1131\n",
      "      3        \u001b[36m1.0573\u001b[0m       \u001b[32m0.5975\u001b[0m        \u001b[35m0.9592\u001b[0m  0.1128\n",
      "      4        \u001b[36m1.0536\u001b[0m       \u001b[32m0.6186\u001b[0m        0.9955  0.1111\n",
      "      5        1.0681       \u001b[32m0.6292\u001b[0m        1.0291  0.1106\n",
      "      6        1.0767       0.6208        1.0514  0.1189\n",
      "      7        1.0739       \u001b[32m0.6356\u001b[0m        1.0358  0.1194\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.1470\u001b[0m       \u001b[32m0.6038\u001b[0m        \u001b[35m1.0551\u001b[0m  0.1206\n",
      "      2        \u001b[36m1.0673\u001b[0m       0.5890        \u001b[35m1.0109\u001b[0m  0.1213\n",
      "      3        \u001b[36m1.0619\u001b[0m       0.6017        \u001b[35m0.9465\u001b[0m  0.1208\n",
      "      4        1.0924       0.5763        1.0186  0.1112\n",
      "      5        \u001b[36m1.0598\u001b[0m       \u001b[32m0.6081\u001b[0m        1.0472  0.1214\n",
      "      6        1.0859       \u001b[32m0.6123\u001b[0m        1.0165  0.1131\n",
      "      7        1.1130       0.6102        1.0336  0.1195\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.0914\u001b[0m       \u001b[32m0.5932\u001b[0m        \u001b[35m1.0330\u001b[0m  0.1107\n",
      "      2        \u001b[36m1.0590\u001b[0m       0.5742        \u001b[35m0.9862\u001b[0m  0.1214\n",
      "      3        \u001b[36m1.0567\u001b[0m       \u001b[32m0.6038\u001b[0m        \u001b[35m0.9366\u001b[0m  0.1195\n",
      "      4        1.0660       \u001b[32m0.6165\u001b[0m        0.9435  0.1187\n",
      "      5        1.0646       \u001b[32m0.6186\u001b[0m        1.0114  0.1296\n",
      "      6        1.0676       0.6081        1.0462  0.1317\n",
      "      7        1.0853       0.6186        1.0556  0.1213\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.1342\u001b[0m       \u001b[32m0.6335\u001b[0m        \u001b[35m1.0466\u001b[0m  0.1111\n",
      "      2        \u001b[36m1.0453\u001b[0m       \u001b[32m0.6356\u001b[0m        \u001b[35m0.9772\u001b[0m  0.1127\n",
      "      3        1.0822       0.6165        \u001b[35m0.9599\u001b[0m  0.1217\n",
      "      4        1.1018       0.6335        0.9725  0.1208\n",
      "      5        1.0839       \u001b[32m0.6589\u001b[0m        1.0207  0.1108\n",
      "      6        1.0718       0.6250        1.0342  0.1110\n",
      "      7        1.1066       0.6123        1.0558  0.1212\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.1255\u001b[0m       \u001b[32m0.6059\u001b[0m        \u001b[35m1.0403\u001b[0m  0.1188\n",
      "      2        \u001b[36m1.0743\u001b[0m       0.5742        \u001b[35m1.0189\u001b[0m  0.1293\n",
      "      3        \u001b[36m1.0554\u001b[0m       0.5784        \u001b[35m0.9688\u001b[0m  0.1210\n",
      "      4        1.0698       0.6017        0.9876  0.1111\n",
      "      5        \u001b[36m1.0551\u001b[0m       0.5763        1.0248  0.1130\n",
      "      6        \u001b[36m1.0457\u001b[0m       \u001b[32m0.6081\u001b[0m        0.9725  0.1110\n",
      "      7        1.0542       0.6059        0.9891  0.1110\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.1249\u001b[0m       \u001b[32m0.5975\u001b[0m        \u001b[35m0.9757\u001b[0m  0.1192\n",
      "      2        \u001b[36m1.0620\u001b[0m       0.5657        0.9805  0.1187\n",
      "      3        1.0821       0.5911        1.0164  0.1210\n",
      "      4        1.0659       \u001b[32m0.6059\u001b[0m        1.0393  0.1123\n",
      "      5        1.0890       \u001b[32m0.6292\u001b[0m        1.0292  0.1109\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.1448\u001b[0m       \u001b[32m0.6144\u001b[0m        \u001b[35m0.9980\u001b[0m  0.1109\n",
      "      2        \u001b[36m1.0456\u001b[0m       0.5953        1.0063  0.1120\n",
      "      3        1.0981       0.5996        1.0026  0.1108\n",
      "      4        1.0507       \u001b[32m0.6229\u001b[0m        \u001b[35m0.9849\u001b[0m  0.1129\n",
      "      5        1.0876       0.6186        1.0338  0.1212\n",
      "      6        1.0876       \u001b[32m0.6335\u001b[0m        1.0650  0.1207\n",
      "      7        1.0838       0.6208        1.0800  0.1190\n",
      "      8        1.0877       0.6186        1.0841  0.1111\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.0981\u001b[0m       \u001b[32m0.6123\u001b[0m        \u001b[35m0.9773\u001b[0m  0.1108\n",
      "      2        \u001b[36m1.0345\u001b[0m       0.5996        \u001b[35m0.9508\u001b[0m  0.1107\n",
      "      3        1.1120       0.6059        1.0064  0.1108\n",
      "      4        1.0647       0.6102        1.0390  0.1191\n",
      "      5        1.1145       \u001b[32m0.6186\u001b[0m        1.0652  0.1116\n",
      "      6        1.0881       0.6186        1.0798  0.1117\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.0936\u001b[0m       \u001b[32m0.6144\u001b[0m        \u001b[35m1.0169\u001b[0m  0.1189\n",
      "      2        \u001b[36m1.0506\u001b[0m       0.5763        \u001b[35m0.9956\u001b[0m  0.1194\n",
      "      3        1.0866       \u001b[32m0.6165\u001b[0m        1.0011  0.1110\n",
      "      4        1.0689       0.6144        1.0313  0.1215\n",
      "      5        1.0765       \u001b[32m0.6271\u001b[0m        1.0647  0.1216\n",
      "      6        1.0844       0.6250        1.0425  0.1132\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.1164\u001b[0m       \u001b[32m0.6292\u001b[0m        \u001b[35m0.9769\u001b[0m  0.1110\n",
      "      2        \u001b[36m1.0537\u001b[0m       0.5699        \u001b[35m0.9675\u001b[0m  0.1114\n",
      "      3        1.0725       0.6102        \u001b[35m0.9520\u001b[0m  0.1129\n",
      "      4        1.0684       0.6271        0.9956  0.1319\n",
      "      5        1.0937       0.6144        1.0421  0.1315\n",
      "      6        1.1010       0.6208        1.0708  0.1212\n",
      "      7        1.0827       0.6123        1.0748  0.1321\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.1058\u001b[0m       \u001b[32m0.6102\u001b[0m        \u001b[35m0.9636\u001b[0m  0.1213\n",
      "      2        \u001b[36m1.0373\u001b[0m       \u001b[32m0.6208\u001b[0m        \u001b[35m0.9485\u001b[0m  0.1315\n",
      "      3        1.0790       0.6017        1.0075  0.1317\n",
      "      4        1.0799       \u001b[32m0.6292\u001b[0m        1.0470  0.1232\n",
      "      5        1.0729       0.6250        1.0252  0.1397\n",
      "      6        1.0866       0.6186        1.0307  0.1417\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.0971\u001b[0m       \u001b[32m0.6144\u001b[0m        \u001b[35m1.0002\u001b[0m  0.1313\n",
      "      2        \u001b[36m1.0497\u001b[0m       0.5996        \u001b[35m0.9446\u001b[0m  0.1411\n",
      "      3        1.0635       0.6123        0.9538  0.1221\n",
      "      4        1.0680       0.6144        1.0106  0.1341\n",
      "      5        1.0954       0.6144        1.0747  0.1227\n",
      "      6        1.0903       0.6144        1.0891  0.1295\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.1016\u001b[0m       \u001b[32m0.5975\u001b[0m        \u001b[35m0.9946\u001b[0m  0.1318\n",
      "      2        \u001b[36m1.0555\u001b[0m       0.5975        \u001b[35m0.9782\u001b[0m  0.1322\n",
      "      3        \u001b[36m1.0499\u001b[0m       \u001b[32m0.6229\u001b[0m        \u001b[35m0.9698\u001b[0m  0.1234\n",
      "      4        \u001b[36m1.0438\u001b[0m       \u001b[32m0.6271\u001b[0m        0.9930  0.1232\n",
      "      5        \u001b[36m1.0217\u001b[0m       0.6229        \u001b[35m0.9449\u001b[0m  0.1338\n",
      "      6        1.0448       0.6271        0.9922  0.1419\n",
      "      7        1.0566       0.6186        1.0461  0.1128\n",
      "      8        1.0636       0.6271        1.0512  0.1107\n",
      "      9        1.0886       0.6250        1.0511  0.1114\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.1626\u001b[0m       \u001b[32m0.3835\u001b[0m        \u001b[35m1.1136\u001b[0m  0.1112\n",
      "      2        \u001b[36m1.1522\u001b[0m       \u001b[32m0.4004\u001b[0m        \u001b[35m1.1012\u001b[0m  0.1109\n",
      "      3        \u001b[36m1.1278\u001b[0m       \u001b[32m0.4237\u001b[0m        \u001b[35m1.0825\u001b[0m  0.1111\n",
      "      4        \u001b[36m1.1112\u001b[0m       \u001b[32m0.4640\u001b[0m        \u001b[35m1.0614\u001b[0m  0.1092\n",
      "      5        \u001b[36m1.0820\u001b[0m       \u001b[32m0.4979\u001b[0m        \u001b[35m1.0415\u001b[0m  0.1111\n",
      "      6        \u001b[36m1.0703\u001b[0m       \u001b[32m0.5339\u001b[0m        \u001b[35m1.0275\u001b[0m  0.1105\n",
      "      7        \u001b[36m1.0564\u001b[0m       \u001b[32m0.5551\u001b[0m        \u001b[35m1.0199\u001b[0m  0.1109\n",
      "      8        1.0594       \u001b[32m0.5805\u001b[0m        \u001b[35m1.0148\u001b[0m  0.1108\n",
      "      9        \u001b[36m1.0476\u001b[0m       \u001b[32m0.5996\u001b[0m        \u001b[35m1.0100\u001b[0m  0.1106\n",
      "     10        \u001b[36m1.0369\u001b[0m       0.5890        \u001b[35m1.0054\u001b[0m  0.1109\n",
      "     11        1.0438       0.5847        \u001b[35m1.0016\u001b[0m  0.1114\n",
      "     12        1.0504       0.5763        \u001b[35m0.9987\u001b[0m  0.1131\n",
      "     13        1.0390       0.5678        \u001b[35m0.9967\u001b[0m  0.1109\n",
      "     14        1.0514       0.5551        \u001b[35m0.9956\u001b[0m  0.1090\n",
      "     15        1.0481       0.5424        \u001b[35m0.9953\u001b[0m  0.1088\n",
      "     16        1.0459       0.5424        0.9956  0.1090\n",
      "     17        1.0687       0.5381        0.9962  0.1168\n",
      "     18        1.0723       0.5381        0.9964  0.1112\n",
      "     19        1.0825       0.5381        0.9956  0.1111\n",
      "     20        1.0532       0.5403        \u001b[35m0.9939\u001b[0m  0.1111\n",
      "     21        1.0765       0.5424        \u001b[35m0.9917\u001b[0m  0.1191\n",
      "     22        1.0759       0.5445        \u001b[35m0.9890\u001b[0m  0.1105\n",
      "     23        1.0591       0.5487        \u001b[35m0.9858\u001b[0m  0.1111\n",
      "     24        1.0521       0.5614        \u001b[35m0.9830\u001b[0m  0.1025\n",
      "     25        1.0627       0.5699        \u001b[35m0.9803\u001b[0m  0.1107\n",
      "     26        1.0383       0.5699        \u001b[35m0.9778\u001b[0m  0.1109\n",
      "     27        1.0472       0.5763        \u001b[35m0.9759\u001b[0m  0.1107\n",
      "     28        1.0443       0.5826        \u001b[35m0.9745\u001b[0m  0.1111\n",
      "     29        1.0468       0.5805        \u001b[35m0.9736\u001b[0m  0.1192\n",
      "     30        \u001b[36m1.0306\u001b[0m       0.5847        \u001b[35m0.9730\u001b[0m  0.1105\n",
      "     31        1.0391       \u001b[32m0.6038\u001b[0m        0.9735  0.1108\n",
      "     32        1.0341       0.6017        0.9746  0.1131\n",
      "     33        1.0399       0.6017        0.9752  0.1109\n",
      "     34        1.0513       0.5996        0.9750  0.1110\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.1240\u001b[0m       \u001b[32m0.4449\u001b[0m        \u001b[35m1.0961\u001b[0m  0.1209\n",
      "      2        \u001b[36m1.1216\u001b[0m       \u001b[32m0.4555\u001b[0m        \u001b[35m1.0909\u001b[0m  0.1130\n",
      "      3        \u001b[36m1.1154\u001b[0m       \u001b[32m0.4746\u001b[0m        \u001b[35m1.0837\u001b[0m  0.1109\n",
      "      4        \u001b[36m1.1075\u001b[0m       \u001b[32m0.5000\u001b[0m        \u001b[35m1.0757\u001b[0m  0.1115\n",
      "      5        \u001b[36m1.0936\u001b[0m       \u001b[32m0.5381\u001b[0m        \u001b[35m1.0684\u001b[0m  0.1107\n",
      "      6        \u001b[36m1.0826\u001b[0m       \u001b[32m0.5466\u001b[0m        \u001b[35m1.0630\u001b[0m  0.1106\n",
      "      7        1.0832       \u001b[32m0.5593\u001b[0m        \u001b[35m1.0592\u001b[0m  0.1127\n",
      "      8        1.0830       \u001b[32m0.5763\u001b[0m        \u001b[35m1.0560\u001b[0m  0.1216\n",
      "      9        \u001b[36m1.0733\u001b[0m       \u001b[32m0.5847\u001b[0m        \u001b[35m1.0525\u001b[0m  0.1216\n",
      "     10        \u001b[36m1.0642\u001b[0m       \u001b[32m0.5953\u001b[0m        \u001b[35m1.0490\u001b[0m  0.1110\n",
      "     11        \u001b[36m1.0565\u001b[0m       0.5953        \u001b[35m1.0441\u001b[0m  0.1105\n",
      "     12        1.0569       \u001b[32m0.5975\u001b[0m        \u001b[35m1.0386\u001b[0m  0.1128\n",
      "     13        1.0602       \u001b[32m0.5996\u001b[0m        \u001b[35m1.0332\u001b[0m  0.1109\n",
      "     14        1.0576       0.5932        \u001b[35m1.0282\u001b[0m  0.1093\n",
      "     15        \u001b[36m1.0455\u001b[0m       0.5890        \u001b[35m1.0232\u001b[0m  0.1113\n",
      "     16        1.0597       0.5847        \u001b[35m1.0186\u001b[0m  0.1031\n",
      "     17        \u001b[36m1.0415\u001b[0m       0.5742        \u001b[35m1.0136\u001b[0m  0.1111\n",
      "     18        \u001b[36m1.0265\u001b[0m       0.5805        \u001b[35m1.0076\u001b[0m  0.1106\n",
      "     19        1.0422       0.5742        \u001b[35m1.0003\u001b[0m  0.1187\n",
      "     20        1.0380       0.5763        \u001b[35m0.9925\u001b[0m  0.1113\n",
      "     21        1.0389       0.5869        \u001b[35m0.9858\u001b[0m  0.1110\n",
      "     22        1.0456       0.5869        \u001b[35m0.9807\u001b[0m  0.1110\n",
      "     23        1.0531       0.5763        \u001b[35m0.9767\u001b[0m  0.1107\n",
      "     24        1.0510       0.5742        \u001b[35m0.9743\u001b[0m  0.1109\n",
      "     25        1.0665       0.5826        \u001b[35m0.9737\u001b[0m  0.1193\n",
      "     26        1.0709       0.5805        0.9742  0.1107\n",
      "     27        1.0872       0.5678        0.9748  0.1108\n",
      "     28        1.0610       0.5572        0.9749  0.1110\n",
      "     29        1.0833       0.5551        0.9747  0.1106\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.1684\u001b[0m       \u001b[32m0.3708\u001b[0m        \u001b[35m1.1117\u001b[0m  0.1108\n",
      "      2        1.1774       \u001b[32m0.3941\u001b[0m        \u001b[35m1.0948\u001b[0m  0.1107\n",
      "      3        \u001b[36m1.1417\u001b[0m       \u001b[32m0.4470\u001b[0m        \u001b[35m1.0701\u001b[0m  0.1110\n",
      "      4        \u001b[36m1.0986\u001b[0m       \u001b[32m0.5000\u001b[0m        \u001b[35m1.0423\u001b[0m  0.1108\n",
      "      5        \u001b[36m1.0722\u001b[0m       \u001b[32m0.5403\u001b[0m        \u001b[35m1.0182\u001b[0m  0.1109\n",
      "      6        \u001b[36m1.0468\u001b[0m       \u001b[32m0.5699\u001b[0m        \u001b[35m1.0036\u001b[0m  0.1112\n",
      "      7        \u001b[36m1.0439\u001b[0m       \u001b[32m0.5911\u001b[0m        \u001b[35m0.9939\u001b[0m  0.1109\n",
      "      8        \u001b[36m1.0399\u001b[0m       \u001b[32m0.6059\u001b[0m        \u001b[35m0.9845\u001b[0m  0.1109\n",
      "      9        \u001b[36m1.0118\u001b[0m       \u001b[32m0.6165\u001b[0m        \u001b[35m0.9757\u001b[0m  0.1294\n",
      "     10        \u001b[36m1.0080\u001b[0m       0.6059        \u001b[35m0.9676\u001b[0m  0.1112\n",
      "     11        1.0111       0.6017        \u001b[35m0.9593\u001b[0m  0.1109\n",
      "     12        1.0239       0.6123        \u001b[35m0.9523\u001b[0m  0.1108\n",
      "     13        1.0188       0.6102        \u001b[35m0.9468\u001b[0m  0.1316\n",
      "     14        1.0607       0.6081        \u001b[35m0.9429\u001b[0m  0.1339\n",
      "     15        1.0538       0.6059        \u001b[35m0.9406\u001b[0m  0.1235\n",
      "     16        1.0551       0.6017        \u001b[35m0.9391\u001b[0m  0.1112\n",
      "     17        1.0422       0.6017        \u001b[35m0.9380\u001b[0m  0.1189\n",
      "     18        1.0617       0.5911        \u001b[35m0.9372\u001b[0m  0.1109\n",
      "     19        1.0655       0.5890        \u001b[35m0.9366\u001b[0m  0.1215\n",
      "     20        1.0920       0.5890        \u001b[35m0.9364\u001b[0m  0.1214\n",
      "     21        1.0759       0.5869        0.9371  0.1113\n",
      "     22        1.0687       0.5826        0.9388  0.1219\n",
      "     23        1.0377       0.5805        0.9420  0.1295\n",
      "     24        1.0596       0.5784        0.9467  0.1108\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.2646\u001b[0m       \u001b[32m0.3220\u001b[0m        \u001b[35m1.1773\u001b[0m  0.1229\n",
      "      2        \u001b[36m1.2461\u001b[0m       \u001b[32m0.3453\u001b[0m        \u001b[35m1.1522\u001b[0m  0.1132\n",
      "      3        \u001b[36m1.2194\u001b[0m       \u001b[32m0.3814\u001b[0m        \u001b[35m1.1148\u001b[0m  0.1208\n",
      "      4        \u001b[36m1.1651\u001b[0m       \u001b[32m0.4195\u001b[0m        \u001b[35m1.0713\u001b[0m  0.1209\n",
      "      5        \u001b[36m1.1312\u001b[0m       \u001b[32m0.5021\u001b[0m        \u001b[35m1.0303\u001b[0m  0.1193\n",
      "      6        \u001b[36m1.0752\u001b[0m       \u001b[32m0.5742\u001b[0m        \u001b[35m1.0037\u001b[0m  0.1111\n",
      "      7        \u001b[36m1.0447\u001b[0m       \u001b[32m0.5805\u001b[0m        \u001b[35m0.9950\u001b[0m  0.1211\n",
      "      8        \u001b[36m1.0305\u001b[0m       \u001b[32m0.5996\u001b[0m        \u001b[35m0.9885\u001b[0m  0.1334\n",
      "      9        1.0367       \u001b[32m0.6017\u001b[0m        \u001b[35m0.9811\u001b[0m  0.1233\n",
      "     10        \u001b[36m1.0269\u001b[0m       \u001b[32m0.6059\u001b[0m        \u001b[35m0.9728\u001b[0m  0.1343\n",
      "     11        \u001b[36m1.0265\u001b[0m       0.6059        \u001b[35m0.9655\u001b[0m  0.1309\n",
      "     12        \u001b[36m1.0186\u001b[0m       0.6017        \u001b[35m0.9597\u001b[0m  0.1315\n",
      "     13        1.0236       0.6038        \u001b[35m0.9557\u001b[0m  0.1219\n",
      "     14        \u001b[36m1.0160\u001b[0m       0.5953        \u001b[35m0.9541\u001b[0m  0.1215\n",
      "     15        1.0399       0.5932        0.9546  0.1295\n",
      "     16        1.0637       0.5890        0.9565  0.1293\n",
      "     17        1.0493       0.5847        0.9593  0.1314\n",
      "     18        1.0587       0.5826        0.9618  0.1316\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.2043\u001b[0m       \u001b[32m0.3856\u001b[0m        \u001b[35m1.1172\u001b[0m  0.1217\n",
      "      2        \u001b[36m1.1969\u001b[0m       \u001b[32m0.4025\u001b[0m        \u001b[35m1.1039\u001b[0m  0.1294\n",
      "      3        \u001b[36m1.1719\u001b[0m       \u001b[32m0.4174\u001b[0m        \u001b[35m1.0839\u001b[0m  0.1320\n",
      "      4        \u001b[36m1.1470\u001b[0m       \u001b[32m0.4513\u001b[0m        \u001b[35m1.0618\u001b[0m  0.1337\n",
      "      5        \u001b[36m1.1178\u001b[0m       \u001b[32m0.4894\u001b[0m        \u001b[35m1.0407\u001b[0m  0.1523\n",
      "      6        \u001b[36m1.0649\u001b[0m       \u001b[32m0.5297\u001b[0m        \u001b[35m1.0268\u001b[0m  0.1317\n",
      "      7        \u001b[36m1.0506\u001b[0m       \u001b[32m0.5339\u001b[0m        \u001b[35m1.0226\u001b[0m  0.1314\n",
      "      8        1.0576       \u001b[32m0.5466\u001b[0m        \u001b[35m1.0203\u001b[0m  0.1322\n",
      "      9        1.0590       \u001b[32m0.5869\u001b[0m        \u001b[35m1.0194\u001b[0m  0.1317\n",
      "     10        \u001b[36m1.0423\u001b[0m       \u001b[32m0.6208\u001b[0m        \u001b[35m1.0186\u001b[0m  0.1317\n",
      "     11        1.0567       0.6102        \u001b[35m1.0163\u001b[0m  0.1338\n",
      "     12        1.0426       0.6102        \u001b[35m1.0130\u001b[0m  0.1437\n",
      "     13        1.0693       0.6017        \u001b[35m1.0107\u001b[0m  0.1315\n",
      "     14        1.0576       0.6038        \u001b[35m1.0087\u001b[0m  0.1396\n",
      "     15        1.0583       0.5996        \u001b[35m1.0069\u001b[0m  0.1213\n",
      "     16        1.0628       0.5890        \u001b[35m1.0050\u001b[0m  0.1114\n",
      "     17        1.0486       0.5869        \u001b[35m1.0028\u001b[0m  0.1211\n",
      "     18        1.0628       0.5742        \u001b[35m1.0001\u001b[0m  0.1232\n",
      "     19        1.0530       0.5699        \u001b[35m0.9970\u001b[0m  0.1115\n",
      "     20        1.0661       0.5678        \u001b[35m0.9914\u001b[0m  0.1112\n",
      "     21        1.0662       0.5678        \u001b[35m0.9845\u001b[0m  0.1218\n",
      "     22        1.0561       0.5657        \u001b[35m0.9774\u001b[0m  0.1214\n",
      "     23        1.0633       0.5763        \u001b[35m0.9697\u001b[0m  0.1199\n",
      "     24        1.0510       0.5805        \u001b[35m0.9620\u001b[0m  0.1294\n",
      "     25        1.0670       0.5805        \u001b[35m0.9548\u001b[0m  0.1216\n",
      "     26        1.0539       0.5911        \u001b[35m0.9487\u001b[0m  0.1215\n",
      "     27        1.0781       0.5996        \u001b[35m0.9437\u001b[0m  0.1109\n",
      "     28        1.0641       0.6102        \u001b[35m0.9394\u001b[0m  0.1189\n",
      "     29        1.0621       0.6123        \u001b[35m0.9364\u001b[0m  0.1114\n",
      "     30        1.0666       0.6081        \u001b[35m0.9349\u001b[0m  0.1189\n",
      "     31        1.0666       0.6081        0.9351  0.1189\n",
      "     32        1.0490       0.6123        0.9362  0.1120\n",
      "     33        \u001b[36m1.0408\u001b[0m       0.6144        0.9381  0.1111\n",
      "     34        \u001b[36m1.0292\u001b[0m       0.6186        0.9412  0.1214\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.2645\u001b[0m       \u001b[32m0.2648\u001b[0m        \u001b[35m1.1951\u001b[0m  0.1133\n",
      "      2        \u001b[36m1.2359\u001b[0m       \u001b[32m0.2797\u001b[0m        \u001b[35m1.1672\u001b[0m  0.1129\n",
      "      3        \u001b[36m1.1942\u001b[0m       \u001b[32m0.3220\u001b[0m        \u001b[35m1.1265\u001b[0m  0.1111\n",
      "      4        \u001b[36m1.1340\u001b[0m       \u001b[32m0.3877\u001b[0m        \u001b[35m1.0804\u001b[0m  0.1112\n",
      "      5        \u001b[36m1.0893\u001b[0m       \u001b[32m0.5106\u001b[0m        \u001b[35m1.0403\u001b[0m  0.1123\n",
      "      6        \u001b[36m1.0562\u001b[0m       \u001b[32m0.6017\u001b[0m        \u001b[35m1.0251\u001b[0m  0.1208\n",
      "      7        1.0585       0.6017        \u001b[35m1.0211\u001b[0m  0.1426\n",
      "      8        \u001b[36m1.0419\u001b[0m       \u001b[32m0.6038\u001b[0m        \u001b[35m1.0167\u001b[0m  0.1212\n",
      "      9        \u001b[36m1.0418\u001b[0m       0.6038        \u001b[35m1.0121\u001b[0m  0.1210\n",
      "     10        1.0496       \u001b[32m0.6123\u001b[0m        \u001b[35m1.0074\u001b[0m  0.1026\n",
      "     11        \u001b[36m1.0338\u001b[0m       0.6038        \u001b[35m1.0022\u001b[0m  0.1105\n",
      "     12        \u001b[36m1.0325\u001b[0m       0.6038        \u001b[35m0.9967\u001b[0m  0.1131\n",
      "     13        \u001b[36m1.0214\u001b[0m       0.6038        \u001b[35m0.9907\u001b[0m  0.1109\n",
      "     14        1.0302       0.6059        \u001b[35m0.9841\u001b[0m  0.1114\n",
      "     15        1.0237       0.5996        \u001b[35m0.9775\u001b[0m  0.1110\n",
      "     16        1.0425       0.5953        \u001b[35m0.9705\u001b[0m  0.1109\n",
      "     17        \u001b[36m1.0131\u001b[0m       0.6017        \u001b[35m0.9633\u001b[0m  0.1128\n",
      "     18        1.0414       0.6017        \u001b[35m0.9573\u001b[0m  0.1111\n",
      "     19        1.0349       0.5996        \u001b[35m0.9523\u001b[0m  0.1108\n",
      "     20        1.0419       0.5847        \u001b[35m0.9487\u001b[0m  0.1216\n",
      "     21        1.0440       0.5826        \u001b[35m0.9461\u001b[0m  0.1219\n",
      "     22        1.0454       0.5847        \u001b[35m0.9438\u001b[0m  0.1194\n",
      "     23        1.0750       0.5805        \u001b[35m0.9411\u001b[0m  0.1308\n",
      "     24        1.0587       0.5869        \u001b[35m0.9381\u001b[0m  0.1133\n",
      "     25        1.0477       0.5911        \u001b[35m0.9343\u001b[0m  0.1213\n",
      "     26        1.0702       0.5953        \u001b[35m0.9299\u001b[0m  0.1218\n",
      "     27        1.0682       0.6059        \u001b[35m0.9251\u001b[0m  0.1130\n",
      "     28        1.0609       0.5996        \u001b[35m0.9206\u001b[0m  0.1197\n",
      "     29        1.0599       0.6059        \u001b[35m0.9167\u001b[0m  0.1215\n",
      "     30        1.0423       0.6059        \u001b[35m0.9137\u001b[0m  0.1215\n",
      "     31        1.0438       \u001b[32m0.6165\u001b[0m        \u001b[35m0.9127\u001b[0m  0.1213\n",
      "     32        1.0273       \u001b[32m0.6208\u001b[0m        0.9141  0.1215\n",
      "     33        1.0177       \u001b[32m0.6314\u001b[0m        0.9180  0.1126\n",
      "     34        1.0164       \u001b[32m0.6441\u001b[0m        0.9243  0.1113\n",
      "     35        1.0253       0.6398        0.9335  0.1212\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.1949\u001b[0m       \u001b[32m0.2627\u001b[0m        \u001b[35m1.1638\u001b[0m  0.1312\n",
      "      2        \u001b[36m1.1701\u001b[0m       \u001b[32m0.2818\u001b[0m        \u001b[35m1.1456\u001b[0m  0.1112\n",
      "      3        \u001b[36m1.1482\u001b[0m       \u001b[32m0.3242\u001b[0m        \u001b[35m1.1194\u001b[0m  0.1108\n",
      "      4        \u001b[36m1.1138\u001b[0m       \u001b[32m0.4068\u001b[0m        \u001b[35m1.0918\u001b[0m  0.1140\n",
      "      5        \u001b[36m1.0921\u001b[0m       \u001b[32m0.5021\u001b[0m        \u001b[35m1.0726\u001b[0m  0.1130\n",
      "      6        \u001b[36m1.0760\u001b[0m       \u001b[32m0.5403\u001b[0m        \u001b[35m1.0669\u001b[0m  0.1211\n",
      "      7        1.0797       \u001b[32m0.5530\u001b[0m        \u001b[35m1.0621\u001b[0m  0.1133\n",
      "      8        \u001b[36m1.0751\u001b[0m       \u001b[32m0.5847\u001b[0m        \u001b[35m1.0566\u001b[0m  0.1130\n",
      "      9        \u001b[36m1.0709\u001b[0m       \u001b[32m0.5890\u001b[0m        \u001b[35m1.0512\u001b[0m  0.1111\n",
      "     10        \u001b[36m1.0688\u001b[0m       \u001b[32m0.5932\u001b[0m        \u001b[35m1.0453\u001b[0m  0.1190\n",
      "     11        \u001b[36m1.0668\u001b[0m       \u001b[32m0.6081\u001b[0m        \u001b[35m1.0391\u001b[0m  0.1107\n",
      "     12        \u001b[36m1.0538\u001b[0m       0.6038        \u001b[35m1.0327\u001b[0m  0.1110\n",
      "     13        \u001b[36m1.0523\u001b[0m       \u001b[32m0.6144\u001b[0m        \u001b[35m1.0261\u001b[0m  0.1213\n",
      "     14        1.0545       \u001b[32m0.6271\u001b[0m        \u001b[35m1.0189\u001b[0m  0.1214\n",
      "     15        1.0523       \u001b[32m0.6292\u001b[0m        \u001b[35m1.0121\u001b[0m  0.1214\n",
      "     16        1.0683       0.6292        \u001b[35m1.0059\u001b[0m  0.1204\n",
      "     17        1.0614       0.6250        \u001b[35m1.0004\u001b[0m  0.1241\n",
      "     18        1.0768       0.6250        \u001b[35m0.9964\u001b[0m  0.1232\n",
      "     19        1.0720       0.6186        \u001b[35m0.9936\u001b[0m  0.1129\n",
      "     20        1.0916       0.6186        \u001b[35m0.9922\u001b[0m  0.1296\n",
      "     21        1.0528       0.6165        \u001b[35m0.9914\u001b[0m  0.1315\n",
      "     22        1.0828       0.6165        \u001b[35m0.9910\u001b[0m  0.1196\n",
      "     23        1.0920       0.6123        \u001b[35m0.9903\u001b[0m  0.1317\n",
      "     24        1.0810       0.6059        \u001b[35m0.9893\u001b[0m  0.1311\n",
      "     25        1.0829       0.6059        \u001b[35m0.9882\u001b[0m  0.1232\n",
      "     26        1.0758       0.6038        \u001b[35m0.9872\u001b[0m  0.1298\n",
      "     27        1.0794       0.6017        \u001b[35m0.9865\u001b[0m  0.1215\n",
      "     28        1.0837       0.6059        \u001b[35m0.9862\u001b[0m  0.1110\n",
      "     29        1.0792       0.6038        \u001b[35m0.9860\u001b[0m  0.1108\n",
      "     30        1.0642       0.6059        \u001b[35m0.9858\u001b[0m  0.1106\n",
      "     31        1.0615       0.6038        0.9860  0.1112\n",
      "     32        1.0683       0.6059        0.9867  0.1314\n",
      "     33        1.0640       0.6208        0.9876  0.1300\n",
      "     34        \u001b[36m1.0494\u001b[0m       0.6165        0.9880  0.1257\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.2315\u001b[0m       \u001b[32m0.3242\u001b[0m        \u001b[35m1.1571\u001b[0m  0.1216\n",
      "      2        \u001b[36m1.1820\u001b[0m       \u001b[32m0.3411\u001b[0m        \u001b[35m1.1377\u001b[0m  0.1210\n",
      "      3        \u001b[36m1.1755\u001b[0m       \u001b[32m0.3644\u001b[0m        \u001b[35m1.1086\u001b[0m  0.1113\n",
      "      4        \u001b[36m1.1334\u001b[0m       \u001b[32m0.4153\u001b[0m        \u001b[35m1.0744\u001b[0m  0.1189\n",
      "      5        \u001b[36m1.0934\u001b[0m       \u001b[32m0.4852\u001b[0m        \u001b[35m1.0430\u001b[0m  0.1114\n",
      "      6        \u001b[36m1.0397\u001b[0m       \u001b[32m0.5424\u001b[0m        \u001b[35m1.0255\u001b[0m  0.1109\n",
      "      7        1.0497       \u001b[32m0.5487\u001b[0m        \u001b[35m1.0166\u001b[0m  0.1129\n",
      "      8        1.0465       \u001b[32m0.5720\u001b[0m        \u001b[35m1.0074\u001b[0m  0.1030\n",
      "      9        \u001b[36m1.0304\u001b[0m       \u001b[32m0.5826\u001b[0m        \u001b[35m0.9976\u001b[0m  0.1115\n",
      "     10        \u001b[36m1.0163\u001b[0m       \u001b[32m0.6038\u001b[0m        \u001b[35m0.9873\u001b[0m  0.1109\n",
      "     11        \u001b[36m1.0093\u001b[0m       \u001b[32m0.6165\u001b[0m        \u001b[35m0.9763\u001b[0m  0.1111\n",
      "     12        1.0227       \u001b[32m0.6271\u001b[0m        \u001b[35m0.9658\u001b[0m  0.1086\n",
      "     13        \u001b[36m1.0074\u001b[0m       0.6229        \u001b[35m0.9574\u001b[0m  0.1215\n",
      "     14        1.0172       0.6208        \u001b[35m0.9514\u001b[0m  0.1304\n",
      "     15        1.0239       0.6165        \u001b[35m0.9476\u001b[0m  0.1298\n",
      "     16        1.0236       0.6102        \u001b[35m0.9456\u001b[0m  0.1293\n",
      "     17        1.0350       0.6081        \u001b[35m0.9444\u001b[0m  0.1219\n",
      "     18        1.0853       0.6038        \u001b[35m0.9438\u001b[0m  0.1214\n",
      "     19        1.0680       0.6017        \u001b[35m0.9431\u001b[0m  0.1194\n",
      "     20        1.0675       0.5996        \u001b[35m0.9423\u001b[0m  0.1218\n",
      "     21        1.0690       0.5975        \u001b[35m0.9409\u001b[0m  0.1295\n",
      "     22        1.0596       0.5975        \u001b[35m0.9390\u001b[0m  0.1315\n",
      "     23        1.0637       0.5996        \u001b[35m0.9364\u001b[0m  0.1237\n",
      "     24        1.0569       0.6059        \u001b[35m0.9336\u001b[0m  0.1234\n",
      "     25        1.0632       0.6102        \u001b[35m0.9312\u001b[0m  0.1217\n",
      "     26        1.0531       0.6144        \u001b[35m0.9297\u001b[0m  0.1294\n",
      "     27        1.0372       0.6186        0.9297  0.1113\n",
      "     28        \u001b[36m1.0049\u001b[0m       0.6165        0.9313  0.1110\n",
      "     29        1.0276       0.6229        0.9347  0.1302\n",
      "     30        1.0247       0.6186        0.9405  0.1213\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.1575\u001b[0m       \u001b[32m0.4110\u001b[0m        \u001b[35m1.1296\u001b[0m  0.1214\n",
      "      2        \u001b[36m1.1498\u001b[0m       \u001b[32m0.4280\u001b[0m        \u001b[35m1.1192\u001b[0m  0.1234\n",
      "      3        \u001b[36m1.1472\u001b[0m       \u001b[32m0.4576\u001b[0m        \u001b[35m1.1040\u001b[0m  0.1220\n",
      "      4        \u001b[36m1.1220\u001b[0m       \u001b[32m0.4936\u001b[0m        \u001b[35m1.0874\u001b[0m  0.1215\n",
      "      5        \u001b[36m1.0967\u001b[0m       \u001b[32m0.5593\u001b[0m        \u001b[35m1.0732\u001b[0m  0.1315\n",
      "      6        \u001b[36m1.0875\u001b[0m       \u001b[32m0.5805\u001b[0m        \u001b[35m1.0644\u001b[0m  0.1317\n",
      "      7        \u001b[36m1.0761\u001b[0m       \u001b[32m0.5953\u001b[0m        \u001b[35m1.0599\u001b[0m  0.1217\n",
      "      8        \u001b[36m1.0720\u001b[0m       0.5932        \u001b[35m1.0568\u001b[0m  0.1215\n",
      "      9        1.0795       0.5932        \u001b[35m1.0542\u001b[0m  0.1191\n",
      "     10        1.0752       0.5869        \u001b[35m1.0519\u001b[0m  0.1215\n",
      "     11        \u001b[36m1.0659\u001b[0m       \u001b[32m0.5975\u001b[0m        \u001b[35m1.0489\u001b[0m  0.1316\n",
      "     12        1.0666       0.5911        \u001b[35m1.0457\u001b[0m  0.1235\n",
      "     13        1.0706       \u001b[32m0.6102\u001b[0m        \u001b[35m1.0427\u001b[0m  0.1220\n",
      "     14        1.0716       0.6102        \u001b[35m1.0397\u001b[0m  0.1213\n",
      "     15        \u001b[36m1.0574\u001b[0m       \u001b[32m0.6123\u001b[0m        \u001b[35m1.0366\u001b[0m  0.1110\n",
      "     16        1.0602       \u001b[32m0.6144\u001b[0m        \u001b[35m1.0326\u001b[0m  0.1129\n",
      "     17        1.0597       0.6081        \u001b[35m1.0275\u001b[0m  0.1105\n",
      "     18        1.0591       0.6123        \u001b[35m1.0210\u001b[0m  0.1110\n",
      "     19        \u001b[36m1.0427\u001b[0m       0.6144        \u001b[35m1.0144\u001b[0m  0.1109\n",
      "     20        1.0543       0.6081        \u001b[35m1.0077\u001b[0m  0.1190\n",
      "     21        1.0478       0.6038        \u001b[35m1.0002\u001b[0m  0.1213\n",
      "     22        \u001b[36m1.0339\u001b[0m       0.6038        \u001b[35m0.9911\u001b[0m  0.1190\n",
      "     23        1.0367       0.6017        \u001b[35m0.9807\u001b[0m  0.1290\n",
      "     24        1.0440       0.5996        \u001b[35m0.9688\u001b[0m  0.1106\n",
      "     25        1.0493       0.5953        \u001b[35m0.9561\u001b[0m  0.1107\n",
      "     26        1.0425       0.5953        \u001b[35m0.9447\u001b[0m  0.1114\n",
      "     27        1.0551       0.6038        \u001b[35m0.9354\u001b[0m  0.1106\n",
      "     28        1.0390       0.6081        \u001b[35m0.9280\u001b[0m  0.1239\n",
      "     29        1.0496       0.6081        \u001b[35m0.9221\u001b[0m  0.1127\n",
      "     30        1.0692       0.6081        \u001b[35m0.9175\u001b[0m  0.1189\n",
      "     31        1.0649       0.6144        \u001b[35m0.9141\u001b[0m  0.1113\n",
      "     32        1.0709       0.6144        \u001b[35m0.9117\u001b[0m  0.1110\n",
      "     33        1.0909       \u001b[32m0.6165\u001b[0m        \u001b[35m0.9100\u001b[0m  0.1129\n",
      "     34        1.0708       \u001b[32m0.6208\u001b[0m        \u001b[35m0.9091\u001b[0m  0.1190\n",
      "     35        1.1006       0.6208        \u001b[35m0.9090\u001b[0m  0.1107\n",
      "     36        1.0508       0.6186        0.9095  0.1109\n",
      "     37        1.0632       \u001b[32m0.6250\u001b[0m        0.9108  0.1131\n",
      "     38        1.0604       0.6229        0.9136  0.1208\n",
      "     39        1.0772       0.6208        0.9184  0.1110\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.1382\u001b[0m       \u001b[32m0.4619\u001b[0m        \u001b[35m1.0891\u001b[0m  0.1111\n",
      "      2        \u001b[36m1.1319\u001b[0m       \u001b[32m0.4703\u001b[0m        \u001b[35m1.0808\u001b[0m  0.1191\n",
      "      3        \u001b[36m1.1230\u001b[0m       \u001b[32m0.4915\u001b[0m        \u001b[35m1.0688\u001b[0m  0.1110\n",
      "      4        \u001b[36m1.1076\u001b[0m       \u001b[32m0.5233\u001b[0m        \u001b[35m1.0553\u001b[0m  0.1107\n",
      "      5        \u001b[36m1.0826\u001b[0m       \u001b[32m0.5614\u001b[0m        \u001b[35m1.0433\u001b[0m  0.1295\n",
      "      6        \u001b[36m1.0746\u001b[0m       \u001b[32m0.5678\u001b[0m        \u001b[35m1.0353\u001b[0m  0.1218\n",
      "      7        \u001b[36m1.0604\u001b[0m       \u001b[32m0.5805\u001b[0m        \u001b[35m1.0310\u001b[0m  0.1211\n",
      "      8        \u001b[36m1.0593\u001b[0m       0.5699        \u001b[35m1.0270\u001b[0m  0.1133\n",
      "      9        \u001b[36m1.0581\u001b[0m       0.5742        \u001b[35m1.0213\u001b[0m  0.1116\n",
      "     10        1.0608       \u001b[32m0.5932\u001b[0m        \u001b[35m1.0150\u001b[0m  0.1108\n",
      "     11        \u001b[36m1.0499\u001b[0m       \u001b[32m0.6038\u001b[0m        \u001b[35m1.0082\u001b[0m  0.1209\n",
      "     12        1.0518       \u001b[32m0.6081\u001b[0m        \u001b[35m1.0005\u001b[0m  0.1187\n",
      "     13        \u001b[36m1.0443\u001b[0m       0.6059        \u001b[35m0.9918\u001b[0m  0.1212\n",
      "     14        \u001b[36m1.0385\u001b[0m       \u001b[32m0.6102\u001b[0m        \u001b[35m0.9814\u001b[0m  0.1108\n",
      "     15        \u001b[36m1.0371\u001b[0m       \u001b[32m0.6123\u001b[0m        \u001b[35m0.9718\u001b[0m  0.1107\n",
      "     16        \u001b[36m1.0279\u001b[0m       0.6038        \u001b[35m0.9646\u001b[0m  0.1108\n",
      "     17        1.0324       0.6038        \u001b[35m0.9600\u001b[0m  0.1190\n",
      "     18        1.0383       0.6038        \u001b[35m0.9581\u001b[0m  0.1110\n",
      "     19        1.0437       0.6038        0.9584  0.1111\n",
      "     20        1.0386       0.5975        0.9599  0.1188\n",
      "     21        1.0540       0.5932        0.9621  0.1107\n",
      "     22        1.0622       0.5996        0.9649  0.1135\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.1938\u001b[0m       \u001b[32m0.4258\u001b[0m        \u001b[35m1.1054\u001b[0m  0.1029\n",
      "      2        1.1954       \u001b[32m0.4364\u001b[0m        \u001b[35m1.0938\u001b[0m  0.1116\n",
      "      3        \u001b[36m1.1699\u001b[0m       \u001b[32m0.4661\u001b[0m        \u001b[35m1.0765\u001b[0m  0.1111\n",
      "      4        \u001b[36m1.1293\u001b[0m       \u001b[32m0.5000\u001b[0m        \u001b[35m1.0559\u001b[0m  0.1191\n",
      "      5        \u001b[36m1.1206\u001b[0m       \u001b[32m0.5466\u001b[0m        \u001b[35m1.0360\u001b[0m  0.1108\n",
      "      6        \u001b[36m1.0846\u001b[0m       \u001b[32m0.5975\u001b[0m        \u001b[35m1.0219\u001b[0m  0.1109\n",
      "      7        \u001b[36m1.0638\u001b[0m       \u001b[32m0.6144\u001b[0m        \u001b[35m1.0159\u001b[0m  0.1108\n",
      "      8        \u001b[36m1.0530\u001b[0m       \u001b[32m0.6229\u001b[0m        \u001b[35m1.0148\u001b[0m  0.1106\n",
      "      9        1.0552       \u001b[32m0.6250\u001b[0m        1.0150  0.1112\n",
      "     10        1.0636       0.6208        \u001b[35m1.0147\u001b[0m  0.1109\n",
      "     11        1.0632       \u001b[32m0.6271\u001b[0m        \u001b[35m1.0135\u001b[0m  0.1296\n",
      "     12        \u001b[36m1.0517\u001b[0m       0.6186        \u001b[35m1.0108\u001b[0m  0.1215\n",
      "     13        1.0693       0.6144        \u001b[35m1.0070\u001b[0m  0.1210\n",
      "     14        1.0637       0.6165        \u001b[35m1.0032\u001b[0m  0.1107\n",
      "     15        \u001b[36m1.0498\u001b[0m       0.6165        \u001b[35m0.9996\u001b[0m  0.1212\n",
      "     16        1.0598       0.6059        \u001b[35m0.9955\u001b[0m  0.1113\n",
      "     17        1.0691       0.6038        \u001b[35m0.9917\u001b[0m  0.1111\n",
      "     18        \u001b[36m1.0476\u001b[0m       0.5953        \u001b[35m0.9882\u001b[0m  0.1212\n",
      "     19        1.0814       0.5932        \u001b[35m0.9851\u001b[0m  0.1197\n",
      "     20        1.0754       0.5890        \u001b[35m0.9828\u001b[0m  0.1111\n",
      "     21        1.0787       0.5890        \u001b[35m0.9808\u001b[0m  0.1109\n",
      "     22        1.0504       0.5869        \u001b[35m0.9794\u001b[0m  0.1209\n",
      "     23        1.0834       0.5869        \u001b[35m0.9787\u001b[0m  0.1110\n",
      "     24        1.0554       0.5847        \u001b[35m0.9778\u001b[0m  0.1112\n",
      "     25        1.0828       0.5932        \u001b[35m0.9758\u001b[0m  0.1107\n",
      "     26        1.0591       0.5911        \u001b[35m0.9726\u001b[0m  0.1131\n",
      "     27        1.0616       0.5975        \u001b[35m0.9691\u001b[0m  0.1192\n",
      "     28        1.0576       0.6059        \u001b[35m0.9656\u001b[0m  0.1188\n",
      "     29        1.0618       0.6081        \u001b[35m0.9624\u001b[0m  0.1109\n",
      "     30        1.0619       0.6186        \u001b[35m0.9590\u001b[0m  0.1108\n",
      "     31        1.0583       0.6250        \u001b[35m0.9562\u001b[0m  0.1190\n",
      "     32        1.0493       0.6229        \u001b[35m0.9546\u001b[0m  0.1212\n",
      "     33        1.0672       \u001b[32m0.6356\u001b[0m        \u001b[35m0.9542\u001b[0m  0.1192\n",
      "     34        1.0533       \u001b[32m0.6377\u001b[0m        \u001b[35m0.9539\u001b[0m  0.1109\n",
      "     35        1.0626       \u001b[32m0.6398\u001b[0m        \u001b[35m0.9535\u001b[0m  0.1113\n",
      "     36        1.0671       0.6398        0.9536  0.1131\n",
      "     37        1.0552       0.6335        0.9546  0.1110\n",
      "     38        1.0753       0.6292        0.9556  0.1091\n",
      "     39        1.0810       0.6314        0.9571  0.1191\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.1657\u001b[0m       \u001b[32m0.4386\u001b[0m        \u001b[35m1.0830\u001b[0m  0.1109\n",
      "      2        \u001b[36m1.1463\u001b[0m       \u001b[32m0.4640\u001b[0m        \u001b[35m1.0735\u001b[0m  0.1211\n",
      "      3        \u001b[36m1.1264\u001b[0m       \u001b[32m0.4788\u001b[0m        \u001b[35m1.0594\u001b[0m  0.1130\n",
      "      4        \u001b[36m1.0930\u001b[0m       \u001b[32m0.4979\u001b[0m        \u001b[35m1.0430\u001b[0m  0.1213\n",
      "      5        1.0961       \u001b[32m0.5551\u001b[0m        \u001b[35m1.0272\u001b[0m  0.1212\n",
      "      6        \u001b[36m1.0720\u001b[0m       \u001b[32m0.5932\u001b[0m        \u001b[35m1.0160\u001b[0m  0.1232\n",
      "      7        \u001b[36m1.0513\u001b[0m       \u001b[32m0.5975\u001b[0m        \u001b[35m1.0099\u001b[0m  0.1235\n",
      "      8        \u001b[36m1.0360\u001b[0m       \u001b[32m0.6102\u001b[0m        \u001b[35m1.0050\u001b[0m  0.1214\n",
      "      9        \u001b[36m1.0330\u001b[0m       0.6038        \u001b[35m0.9985\u001b[0m  0.1221\n",
      "     10        1.0372       0.6059        \u001b[35m0.9909\u001b[0m  0.1270\n",
      "     11        \u001b[36m1.0202\u001b[0m       0.6038        \u001b[35m0.9815\u001b[0m  0.1279\n",
      "     12        \u001b[36m1.0132\u001b[0m       0.6059        \u001b[35m0.9726\u001b[0m  0.1295\n",
      "     13        1.0196       0.6038        \u001b[35m0.9663\u001b[0m  0.1216\n",
      "     14        \u001b[36m1.0104\u001b[0m       0.5975        \u001b[35m0.9619\u001b[0m  0.1294\n",
      "     15        \u001b[36m1.0082\u001b[0m       0.5996        \u001b[35m0.9588\u001b[0m  0.1315\n",
      "     16        1.0320       0.5911        \u001b[35m0.9566\u001b[0m  0.1214\n",
      "     17        1.0499       0.5826        \u001b[35m0.9554\u001b[0m  0.1499\n",
      "     18        1.0378       0.5805        \u001b[35m0.9541\u001b[0m  0.1296\n",
      "     19        1.0712       0.5826        \u001b[35m0.9523\u001b[0m  0.1212\n",
      "     20        1.0690       0.5784        \u001b[35m0.9501\u001b[0m  0.1215\n",
      "     21        1.0766       0.5826        \u001b[35m0.9472\u001b[0m  0.1213\n",
      "     22        1.0694       0.5826        \u001b[35m0.9436\u001b[0m  0.1212\n",
      "     23        1.0410       0.5953        \u001b[35m0.9402\u001b[0m  0.1293\n",
      "     24        1.0566       0.5911        \u001b[35m0.9373\u001b[0m  0.1318\n",
      "     25        1.0590       0.5911        \u001b[35m0.9352\u001b[0m  0.1422\n",
      "     26        1.0383       0.5784        \u001b[35m0.9348\u001b[0m  0.1519\n",
      "     27        1.0432       0.5911        0.9367  0.1314\n",
      "     28        1.0427       0.5932        0.9413  0.1210\n",
      "     29        1.0264       0.5953        0.9486  0.1151\n",
      "     30        1.0335       0.5953        0.9579  0.1130\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.2646\u001b[0m       \u001b[32m0.3220\u001b[0m        \u001b[35m1.1922\u001b[0m  0.1116\n",
      "      2        \u001b[36m1.2372\u001b[0m       \u001b[32m0.3475\u001b[0m        \u001b[35m1.1701\u001b[0m  0.1210\n",
      "      3        \u001b[36m1.1959\u001b[0m       \u001b[32m0.3941\u001b[0m        \u001b[35m1.1378\u001b[0m  0.1111\n",
      "      4        \u001b[36m1.1555\u001b[0m       \u001b[32m0.4407\u001b[0m        \u001b[35m1.1017\u001b[0m  0.1112\n",
      "      5        \u001b[36m1.1159\u001b[0m       \u001b[32m0.4936\u001b[0m        \u001b[35m1.0697\u001b[0m  0.1216\n",
      "      6        \u001b[36m1.0968\u001b[0m       \u001b[32m0.5233\u001b[0m        \u001b[35m1.0492\u001b[0m  0.1214\n",
      "      7        \u001b[36m1.0789\u001b[0m       \u001b[32m0.5297\u001b[0m        \u001b[35m1.0397\u001b[0m  0.1108\n",
      "      8        \u001b[36m1.0663\u001b[0m       \u001b[32m0.5636\u001b[0m        \u001b[35m1.0332\u001b[0m  0.1116\n",
      "      9        \u001b[36m1.0562\u001b[0m       \u001b[32m0.5826\u001b[0m        \u001b[35m1.0271\u001b[0m  0.1189\n",
      "     10        \u001b[36m1.0531\u001b[0m       \u001b[32m0.5975\u001b[0m        \u001b[35m1.0214\u001b[0m  0.1107\n",
      "     11        1.0542       0.5975        \u001b[35m1.0156\u001b[0m  0.1112\n",
      "     12        1.0559       \u001b[32m0.6038\u001b[0m        \u001b[35m1.0099\u001b[0m  0.1129\n",
      "     13        \u001b[36m1.0504\u001b[0m       0.5932        \u001b[35m1.0042\u001b[0m  0.1111\n",
      "     14        1.0532       0.6038        \u001b[35m0.9985\u001b[0m  0.1111\n",
      "     15        \u001b[36m1.0437\u001b[0m       0.5911        \u001b[35m0.9929\u001b[0m  0.1109\n",
      "     16        1.0439       0.5847        \u001b[35m0.9878\u001b[0m  0.1130\n",
      "     17        \u001b[36m1.0253\u001b[0m       0.5847        \u001b[35m0.9826\u001b[0m  0.1110\n",
      "     18        1.0507       0.5826        \u001b[35m0.9779\u001b[0m  0.1115\n",
      "     19        1.0413       0.5805        \u001b[35m0.9728\u001b[0m  0.1109\n",
      "     20        1.0587       0.5847        \u001b[35m0.9671\u001b[0m  0.1112\n",
      "     21        1.0591       0.5932        \u001b[35m0.9614\u001b[0m  0.1110\n",
      "     22        1.0789       0.5911        \u001b[35m0.9568\u001b[0m  0.1198\n",
      "     23        1.0617       0.5932        \u001b[35m0.9528\u001b[0m  0.1109\n",
      "     24        1.0610       0.5911        \u001b[35m0.9493\u001b[0m  0.1110\n",
      "     25        1.0705       0.5953        \u001b[35m0.9459\u001b[0m  0.1135\n",
      "     26        1.0935       0.5975        \u001b[35m0.9428\u001b[0m  0.1110\n",
      "     27        1.0688       0.6038        \u001b[35m0.9398\u001b[0m  0.1111\n",
      "     28        1.0710       \u001b[32m0.6081\u001b[0m        \u001b[35m0.9371\u001b[0m  0.1130\n",
      "     29        1.0772       0.6081        \u001b[35m0.9347\u001b[0m  0.1210\n",
      "     30        1.0606       0.5996        \u001b[35m0.9331\u001b[0m  0.1109\n",
      "     31        1.0354       0.6081        \u001b[35m0.9323\u001b[0m  0.1107\n",
      "     32        1.0483       \u001b[32m0.6102\u001b[0m        0.9334  0.1130\n",
      "     33        1.0659       \u001b[32m0.6123\u001b[0m        0.9359  0.1110\n",
      "     34        1.0635       0.6123        0.9401  0.1215\n",
      "     35        1.0539       \u001b[32m0.6144\u001b[0m        0.9461  0.1215\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.1589\u001b[0m       \u001b[32m0.4301\u001b[0m        \u001b[35m1.1231\u001b[0m  0.1029\n",
      "      2        \u001b[36m1.1531\u001b[0m       \u001b[32m0.4470\u001b[0m        \u001b[35m1.1145\u001b[0m  0.1109\n",
      "      3        \u001b[36m1.1330\u001b[0m       \u001b[32m0.4619\u001b[0m        \u001b[35m1.1019\u001b[0m  0.1109\n",
      "      4        \u001b[36m1.1246\u001b[0m       \u001b[32m0.4873\u001b[0m        \u001b[35m1.0882\u001b[0m  0.1107\n",
      "      5        \u001b[36m1.1071\u001b[0m       \u001b[32m0.5169\u001b[0m        \u001b[35m1.0754\u001b[0m  0.1106\n",
      "      6        \u001b[36m1.0800\u001b[0m       \u001b[32m0.5381\u001b[0m        \u001b[35m1.0651\u001b[0m  0.1114\n",
      "      7        1.0801       \u001b[32m0.5530\u001b[0m        \u001b[35m1.0588\u001b[0m  0.1207\n",
      "      8        \u001b[36m1.0791\u001b[0m       \u001b[32m0.5720\u001b[0m        \u001b[35m1.0531\u001b[0m  0.1197\n",
      "      9        \u001b[36m1.0681\u001b[0m       \u001b[32m0.5763\u001b[0m        \u001b[35m1.0474\u001b[0m  0.1110\n",
      "     10        1.0731       \u001b[32m0.5890\u001b[0m        \u001b[35m1.0419\u001b[0m  0.1109\n",
      "     11        \u001b[36m1.0626\u001b[0m       \u001b[32m0.5911\u001b[0m        \u001b[35m1.0364\u001b[0m  0.1104\n",
      "     12        \u001b[36m1.0613\u001b[0m       \u001b[32m0.6059\u001b[0m        \u001b[35m1.0319\u001b[0m  0.1107\n",
      "     13        1.0623       0.6059        \u001b[35m1.0279\u001b[0m  0.1135\n",
      "     14        1.0759       0.6059        \u001b[35m1.0250\u001b[0m  0.1195\n",
      "     15        1.0653       0.6017        \u001b[35m1.0223\u001b[0m  0.1107\n",
      "     16        1.0725       0.6017        \u001b[35m1.0196\u001b[0m  0.1127\n",
      "     17        1.0865       0.5996        \u001b[35m1.0165\u001b[0m  0.1131\n",
      "     18        1.0738       0.5975        \u001b[35m1.0132\u001b[0m  0.1111\n",
      "     19        1.0791       0.5953        \u001b[35m1.0086\u001b[0m  0.1108\n",
      "     20        1.0847       0.5911        \u001b[35m1.0031\u001b[0m  0.1107\n",
      "     21        1.0673       0.5869        \u001b[35m0.9975\u001b[0m  0.1129\n",
      "     22        1.0721       0.5847        \u001b[35m0.9920\u001b[0m  0.1115\n",
      "     23        \u001b[36m1.0601\u001b[0m       0.5826        \u001b[35m0.9865\u001b[0m  0.1111\n",
      "     24        1.0755       0.5699        \u001b[35m0.9810\u001b[0m  0.1108\n",
      "     25        1.0715       0.5657        \u001b[35m0.9765\u001b[0m  0.1108\n",
      "     26        \u001b[36m1.0525\u001b[0m       0.5784        \u001b[35m0.9732\u001b[0m  0.1190\n",
      "     27        \u001b[36m1.0513\u001b[0m       0.5890        \u001b[35m0.9713\u001b[0m  0.1108\n",
      "     28        \u001b[36m1.0505\u001b[0m       0.5890        \u001b[35m0.9712\u001b[0m  0.1112\n",
      "     29        1.0511       0.5890        0.9723  0.1111\n",
      "     30        1.0699       0.5805        0.9742  0.1196\n",
      "     31        1.0705       0.5826        0.9763  0.1107\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.1715\u001b[0m       \u001b[32m0.4534\u001b[0m        \u001b[35m1.1058\u001b[0m  0.1186\n",
      "      2        \u001b[36m1.1676\u001b[0m       \u001b[32m0.4725\u001b[0m        \u001b[35m1.0956\u001b[0m  0.1116\n",
      "      3        \u001b[36m1.1506\u001b[0m       \u001b[32m0.4915\u001b[0m        \u001b[35m1.0800\u001b[0m  0.1110\n",
      "      4        \u001b[36m1.1241\u001b[0m       \u001b[32m0.5297\u001b[0m        \u001b[35m1.0621\u001b[0m  0.1112\n",
      "      5        \u001b[36m1.0961\u001b[0m       \u001b[32m0.5381\u001b[0m        \u001b[35m1.0441\u001b[0m  0.1110\n",
      "      6        \u001b[36m1.0813\u001b[0m       \u001b[32m0.5572\u001b[0m        \u001b[35m1.0298\u001b[0m  0.1219\n",
      "      7        \u001b[36m1.0722\u001b[0m       \u001b[32m0.5614\u001b[0m        \u001b[35m1.0212\u001b[0m  0.1219\n",
      "      8        \u001b[36m1.0655\u001b[0m       \u001b[32m0.5678\u001b[0m        \u001b[35m1.0147\u001b[0m  0.1109\n",
      "      9        \u001b[36m1.0522\u001b[0m       \u001b[32m0.5826\u001b[0m        \u001b[35m1.0086\u001b[0m  0.1108\n",
      "     10        1.0561       0.5826        \u001b[35m1.0036\u001b[0m  0.1111\n",
      "     11        \u001b[36m1.0459\u001b[0m       0.5742        \u001b[35m0.9993\u001b[0m  0.1105\n",
      "     12        1.0493       0.5784        \u001b[35m0.9952\u001b[0m  0.1128\n",
      "     13        1.0531       0.5763        \u001b[35m0.9920\u001b[0m  0.1111\n",
      "     14        \u001b[36m1.0421\u001b[0m       0.5720        \u001b[35m0.9906\u001b[0m  0.1107\n",
      "     15        1.0444       0.5678        0.9911  0.1109\n",
      "     16        1.0531       0.5487        0.9941  0.1110\n",
      "     17        1.0431       0.5360        0.9983  0.1108\n",
      "     18        1.0547       0.5360        1.0034  0.1106\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.1416\u001b[0m       \u001b[32m0.4174\u001b[0m        \u001b[35m1.1031\u001b[0m  0.1108\n",
      "      2        \u001b[36m1.1277\u001b[0m       \u001b[32m0.4301\u001b[0m        \u001b[35m1.0945\u001b[0m  0.1110\n",
      "      3        \u001b[36m1.1134\u001b[0m       \u001b[32m0.4619\u001b[0m        \u001b[35m1.0819\u001b[0m  0.1393\n",
      "      4        \u001b[36m1.0987\u001b[0m       \u001b[32m0.5169\u001b[0m        \u001b[35m1.0678\u001b[0m  0.1112\n",
      "      5        \u001b[36m1.0843\u001b[0m       \u001b[32m0.5530\u001b[0m        \u001b[35m1.0547\u001b[0m  0.1109\n",
      "      6        \u001b[36m1.0749\u001b[0m       \u001b[32m0.5699\u001b[0m        \u001b[35m1.0448\u001b[0m  0.1215\n",
      "      7        \u001b[36m1.0661\u001b[0m       \u001b[32m0.5847\u001b[0m        \u001b[35m1.0380\u001b[0m  0.1214\n",
      "      8        \u001b[36m1.0532\u001b[0m       \u001b[32m0.5911\u001b[0m        \u001b[35m1.0329\u001b[0m  0.1211\n",
      "      9        1.0605       \u001b[32m0.5953\u001b[0m        \u001b[35m1.0278\u001b[0m  0.1213\n",
      "     10        1.0567       \u001b[32m0.6038\u001b[0m        \u001b[35m1.0226\u001b[0m  0.1111\n",
      "     11        1.0579       0.5975        \u001b[35m1.0174\u001b[0m  0.1110\n",
      "     12        1.0554       0.5975        \u001b[35m1.0125\u001b[0m  0.1112\n",
      "     13        1.0535       0.6038        \u001b[35m1.0080\u001b[0m  0.1112\n",
      "     14        1.0611       0.5996        \u001b[35m1.0039\u001b[0m  0.1506\n",
      "     15        1.0558       0.5932        \u001b[35m0.9998\u001b[0m  0.1235\n",
      "     16        1.0539       0.5890        \u001b[35m0.9959\u001b[0m  0.1233\n",
      "     17        \u001b[36m1.0414\u001b[0m       0.5890        \u001b[35m0.9925\u001b[0m  0.1215\n",
      "     18        1.0641       0.5784        \u001b[35m0.9893\u001b[0m  0.1233\n",
      "     19        1.0415       0.5805        \u001b[35m0.9857\u001b[0m  0.1210\n",
      "     20        1.0533       0.5699        \u001b[35m0.9812\u001b[0m  0.1219\n",
      "     21        1.0772       0.5720        \u001b[35m0.9764\u001b[0m  0.1232\n",
      "     22        1.0525       0.5784        \u001b[35m0.9708\u001b[0m  0.1213\n",
      "     23        1.0418       0.5911        \u001b[35m0.9646\u001b[0m  0.1213\n",
      "     24        1.0651       0.5847        \u001b[35m0.9582\u001b[0m  0.1294\n",
      "     25        1.0617       0.5975        \u001b[35m0.9528\u001b[0m  0.1267\n",
      "     26        1.0634       0.6017        \u001b[35m0.9478\u001b[0m  0.1212\n",
      "     27        1.0564       0.5911        \u001b[35m0.9439\u001b[0m  0.1295\n",
      "     28        1.0518       0.5911        \u001b[35m0.9418\u001b[0m  0.1213\n",
      "     29        1.0572       0.5975        \u001b[35m0.9409\u001b[0m  0.1215\n",
      "     30        \u001b[36m1.0410\u001b[0m       0.5890        \u001b[35m0.9404\u001b[0m  0.1215\n",
      "     31        1.0572       0.5953        0.9408  0.1216\n",
      "     32        \u001b[36m1.0315\u001b[0m       0.6038        0.9410  0.1214\n",
      "     33        1.0344       0.6038        0.9407  0.1237\n",
      "     34        1.0587       \u001b[32m0.6081\u001b[0m        \u001b[35m0.9402\u001b[0m  0.1217\n",
      "     35        1.0597       \u001b[32m0.6186\u001b[0m        \u001b[35m0.9390\u001b[0m  0.1297\n",
      "     36        1.0322       \u001b[32m0.6250\u001b[0m        \u001b[35m0.9372\u001b[0m  0.1322\n",
      "     37        1.0442       0.6229        \u001b[35m0.9353\u001b[0m  0.1131\n",
      "     38        1.0499       \u001b[32m0.6335\u001b[0m        \u001b[35m0.9338\u001b[0m  0.1192\n",
      "     39        \u001b[36m1.0301\u001b[0m       \u001b[32m0.6377\u001b[0m        \u001b[35m0.9330\u001b[0m  0.1109\n",
      "     40        \u001b[36m1.0157\u001b[0m       0.6356        0.9339  0.1130\n",
      "     41        1.0381       \u001b[32m0.6398\u001b[0m        0.9364  0.1120\n",
      "     42        1.0348       0.6377        0.9404  0.1214\n",
      "     43        1.0299       0.6229        0.9459  0.1286\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.1687\u001b[0m       \u001b[32m0.3750\u001b[0m        \u001b[35m1.1374\u001b[0m  0.1213\n",
      "      2        \u001b[36m1.1574\u001b[0m       \u001b[32m0.3919\u001b[0m        \u001b[35m1.1260\u001b[0m  0.1109\n",
      "      3        \u001b[36m1.1485\u001b[0m       \u001b[32m0.4280\u001b[0m        \u001b[35m1.1096\u001b[0m  0.1130\n",
      "      4        \u001b[36m1.1184\u001b[0m       \u001b[32m0.4555\u001b[0m        \u001b[35m1.0910\u001b[0m  0.1216\n",
      "      5        \u001b[36m1.0940\u001b[0m       \u001b[32m0.5106\u001b[0m        \u001b[35m1.0742\u001b[0m  0.1108\n",
      "      6        \u001b[36m1.0836\u001b[0m       \u001b[32m0.5508\u001b[0m        \u001b[35m1.0627\u001b[0m  0.1213\n",
      "      7        \u001b[36m1.0715\u001b[0m       \u001b[32m0.5890\u001b[0m        \u001b[35m1.0570\u001b[0m  0.1132\n",
      "      8        \u001b[36m1.0636\u001b[0m       0.5869        \u001b[35m1.0525\u001b[0m  0.1191\n",
      "      9        \u001b[36m1.0610\u001b[0m       \u001b[32m0.5932\u001b[0m        \u001b[35m1.0481\u001b[0m  0.1108\n",
      "     10        1.0697       \u001b[32m0.5975\u001b[0m        \u001b[35m1.0436\u001b[0m  0.1110\n",
      "     11        1.0634       0.5932        \u001b[35m1.0399\u001b[0m  0.1114\n",
      "     12        1.0674       0.5932        \u001b[35m1.0364\u001b[0m  0.1110\n",
      "     13        1.0660       0.5953        \u001b[35m1.0336\u001b[0m  0.1109\n",
      "     14        \u001b[36m1.0589\u001b[0m       0.5932        \u001b[35m1.0320\u001b[0m  0.1131\n",
      "     15        1.0632       0.5869        \u001b[35m1.0315\u001b[0m  0.1110\n",
      "     16        1.0601       0.5847        1.0324  0.1112\n",
      "     17        \u001b[36m1.0511\u001b[0m       0.5869        1.0341  0.1117\n",
      "     18        1.0534       0.5847        1.0359  0.1108\n",
      "     19        1.0656       0.5784        1.0374  0.1105\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.1889\u001b[0m       \u001b[32m0.3750\u001b[0m        \u001b[35m1.1452\u001b[0m  0.1106\n",
      "      2        1.1904       \u001b[32m0.3877\u001b[0m        \u001b[35m1.1307\u001b[0m  0.1109\n",
      "      3        \u001b[36m1.1646\u001b[0m       \u001b[32m0.4301\u001b[0m        \u001b[35m1.1086\u001b[0m  0.1190\n",
      "      4        \u001b[36m1.1364\u001b[0m       \u001b[32m0.4534\u001b[0m        \u001b[35m1.0831\u001b[0m  0.1222\n",
      "      5        \u001b[36m1.1129\u001b[0m       \u001b[32m0.5000\u001b[0m        \u001b[35m1.0589\u001b[0m  0.1215\n",
      "      6        \u001b[36m1.0831\u001b[0m       \u001b[32m0.5699\u001b[0m        \u001b[35m1.0426\u001b[0m  0.1189\n",
      "      7        1.0835       \u001b[32m0.5847\u001b[0m        \u001b[35m1.0375\u001b[0m  0.1106\n",
      "      8        \u001b[36m1.0709\u001b[0m       0.5826        \u001b[35m1.0343\u001b[0m  0.1215\n",
      "      9        \u001b[36m1.0471\u001b[0m       \u001b[32m0.5953\u001b[0m        \u001b[35m1.0305\u001b[0m  0.1131\n",
      "     10        1.0596       0.5911        \u001b[35m1.0259\u001b[0m  0.1130\n",
      "     11        \u001b[36m1.0442\u001b[0m       0.5911        \u001b[35m1.0199\u001b[0m  0.1115\n",
      "     12        \u001b[36m1.0404\u001b[0m       0.5869        \u001b[35m1.0130\u001b[0m  0.1108\n",
      "     13        1.0443       \u001b[32m0.6017\u001b[0m        \u001b[35m1.0064\u001b[0m  0.1030\n",
      "     14        1.0500       0.5975        \u001b[35m0.9997\u001b[0m  0.1113\n",
      "     15        1.0559       0.5953        \u001b[35m0.9933\u001b[0m  0.1112\n",
      "     16        1.0435       0.5911        \u001b[35m0.9874\u001b[0m  0.1106\n",
      "     17        1.0523       0.5932        \u001b[35m0.9819\u001b[0m  0.1105\n",
      "     18        1.0616       0.5890        \u001b[35m0.9767\u001b[0m  0.1194\n",
      "     19        \u001b[36m1.0375\u001b[0m       0.5826        \u001b[35m0.9729\u001b[0m  0.1212\n",
      "     20        1.0604       0.5847        \u001b[35m0.9698\u001b[0m  0.1214\n",
      "     21        1.0668       0.5847        \u001b[35m0.9674\u001b[0m  0.1192\n",
      "     22        1.0742       0.5890        \u001b[35m0.9662\u001b[0m  0.1212\n",
      "     23        1.1054       0.5826        0.9664  0.1132\n",
      "     24        1.0793       0.5699        0.9674  0.1108\n",
      "     25        1.0720       0.5720        0.9693  0.1110\n",
      "     26        1.0953       0.5678        0.9715  0.1211\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.1775\u001b[0m       \u001b[32m0.4174\u001b[0m        \u001b[35m1.1080\u001b[0m  0.1115\n",
      "      2        \u001b[36m1.1602\u001b[0m       \u001b[32m0.4280\u001b[0m        \u001b[35m1.0966\u001b[0m  0.1110\n",
      "      3        \u001b[36m1.1436\u001b[0m       \u001b[32m0.4492\u001b[0m        \u001b[35m1.0806\u001b[0m  0.1129\n",
      "      4        \u001b[36m1.1299\u001b[0m       \u001b[32m0.4767\u001b[0m        \u001b[35m1.0630\u001b[0m  0.1129\n",
      "      5        \u001b[36m1.0977\u001b[0m       \u001b[32m0.5064\u001b[0m        \u001b[35m1.0476\u001b[0m  0.1111\n",
      "      6        \u001b[36m1.0895\u001b[0m       \u001b[32m0.5212\u001b[0m        \u001b[35m1.0373\u001b[0m  0.1107\n",
      "      7        \u001b[36m1.0706\u001b[0m       \u001b[32m0.5445\u001b[0m        \u001b[35m1.0311\u001b[0m  0.1109\n",
      "      8        \u001b[36m1.0640\u001b[0m       \u001b[32m0.5657\u001b[0m        \u001b[35m1.0273\u001b[0m  0.1112\n",
      "      9        \u001b[36m1.0631\u001b[0m       \u001b[32m0.5784\u001b[0m        \u001b[35m1.0242\u001b[0m  0.1110\n",
      "     10        \u001b[36m1.0575\u001b[0m       \u001b[32m0.5911\u001b[0m        \u001b[35m1.0217\u001b[0m  0.1109\n",
      "     11        1.0615       \u001b[32m0.5975\u001b[0m        \u001b[35m1.0196\u001b[0m  0.1109\n",
      "     12        1.0589       0.5932        \u001b[35m1.0169\u001b[0m  0.1190\n",
      "     13        1.0649       0.5847        \u001b[35m1.0137\u001b[0m  0.1192\n",
      "     14        1.0587       0.5847        \u001b[35m1.0115\u001b[0m  0.1106\n",
      "     15        \u001b[36m1.0440\u001b[0m       0.5784        \u001b[35m1.0081\u001b[0m  0.1110\n",
      "     16        1.0537       0.5720        \u001b[35m1.0045\u001b[0m  0.1191\n",
      "     17        1.0498       0.5720        \u001b[35m1.0009\u001b[0m  0.1217\n",
      "     18        \u001b[36m1.0414\u001b[0m       0.5636        \u001b[35m0.9967\u001b[0m  0.1027\n",
      "     19        1.0417       0.5720        \u001b[35m0.9915\u001b[0m  0.1111\n",
      "     20        1.0523       0.5720        \u001b[35m0.9860\u001b[0m  0.1115\n",
      "     21        \u001b[36m1.0378\u001b[0m       0.5763        \u001b[35m0.9813\u001b[0m  0.1112\n",
      "     22        1.0603       0.5847        \u001b[35m0.9774\u001b[0m  0.1111\n",
      "     23        1.0531       0.5826        \u001b[35m0.9741\u001b[0m  0.1106\n",
      "     24        1.0547       0.5826        \u001b[35m0.9719\u001b[0m  0.1095\n",
      "     25        1.0615       0.5826        \u001b[35m0.9704\u001b[0m  0.1109\n",
      "     26        1.0801       0.5847        \u001b[35m0.9690\u001b[0m  0.1116\n",
      "     27        1.0782       0.5869        \u001b[35m0.9677\u001b[0m  0.1094\n",
      "     28        1.0790       0.5911        \u001b[35m0.9665\u001b[0m  0.1136\n",
      "     29        1.0767       0.5911        \u001b[35m0.9656\u001b[0m  0.1100\n",
      "     30        1.0906       0.5932        \u001b[35m0.9649\u001b[0m  0.1111\n",
      "     31        1.0894       \u001b[32m0.6017\u001b[0m        \u001b[35m0.9646\u001b[0m  0.1111\n",
      "     32        1.0913       0.5975        0.9647  0.1108\n",
      "     33        1.0745       0.5996        0.9653  0.1192\n",
      "     34        1.0547       \u001b[32m0.6038\u001b[0m        0.9673  0.1111\n",
      "     35        1.0686       \u001b[32m0.6102\u001b[0m        0.9708  0.1191\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.2847\u001b[0m       \u001b[32m0.2521\u001b[0m        \u001b[35m1.1775\u001b[0m  0.1112\n",
      "      2        \u001b[36m1.2477\u001b[0m       \u001b[32m0.2754\u001b[0m        \u001b[35m1.1543\u001b[0m  0.1193\n",
      "      3        \u001b[36m1.2300\u001b[0m       \u001b[32m0.3284\u001b[0m        \u001b[35m1.1199\u001b[0m  0.1211\n",
      "      4        \u001b[36m1.1786\u001b[0m       \u001b[32m0.4174\u001b[0m        \u001b[35m1.0813\u001b[0m  0.1213\n",
      "      5        \u001b[36m1.1073\u001b[0m       \u001b[32m0.5085\u001b[0m        \u001b[35m1.0484\u001b[0m  0.1129\n",
      "      6        \u001b[36m1.0873\u001b[0m       \u001b[32m0.5572\u001b[0m        \u001b[35m1.0332\u001b[0m  0.1194\n",
      "      7        \u001b[36m1.0784\u001b[0m       \u001b[32m0.5720\u001b[0m        \u001b[35m1.0285\u001b[0m  0.1297\n",
      "      8        \u001b[36m1.0487\u001b[0m       \u001b[32m0.5890\u001b[0m        \u001b[35m1.0252\u001b[0m  0.1462\n",
      "      9        1.0535       \u001b[32m0.5932\u001b[0m        \u001b[35m1.0232\u001b[0m  0.1337\n",
      "     10        1.0581       \u001b[32m0.5996\u001b[0m        \u001b[35m1.0216\u001b[0m  0.1311\n",
      "     11        1.0529       0.5975        \u001b[35m1.0203\u001b[0m  0.1219\n",
      "     12        1.0673       \u001b[32m0.6081\u001b[0m        \u001b[35m1.0193\u001b[0m  0.1134\n",
      "     13        1.0589       0.5996        \u001b[35m1.0184\u001b[0m  0.1109\n",
      "     14        1.0590       0.5975        \u001b[35m1.0171\u001b[0m  0.1109\n",
      "     15        1.0559       0.5911        \u001b[35m1.0162\u001b[0m  0.1109\n",
      "     16        1.0502       0.5890        \u001b[35m1.0157\u001b[0m  0.1214\n",
      "     17        1.0728       0.5890        \u001b[35m1.0156\u001b[0m  0.1217\n",
      "     18        1.0698       0.5869        \u001b[35m1.0146\u001b[0m  0.1213\n",
      "     19        1.0580       0.5847        \u001b[35m1.0135\u001b[0m  0.1216\n",
      "     20        1.0800       0.5805        \u001b[35m1.0125\u001b[0m  0.1316\n",
      "     21        1.0781       0.5869        \u001b[35m1.0111\u001b[0m  0.1299\n",
      "     22        1.0729       0.5847        \u001b[35m1.0089\u001b[0m  0.1222\n",
      "     23        1.0681       0.5932        \u001b[35m1.0058\u001b[0m  0.1217\n",
      "     24        1.0939       0.5953        \u001b[35m1.0019\u001b[0m  0.1298\n",
      "     25        \u001b[36m1.0447\u001b[0m       0.5975        \u001b[35m0.9968\u001b[0m  0.1316\n",
      "     26        1.0680       \u001b[32m0.6123\u001b[0m        \u001b[35m0.9916\u001b[0m  0.1234\n",
      "     27        1.0830       \u001b[32m0.6229\u001b[0m        \u001b[35m0.9864\u001b[0m  0.1236\n",
      "     28        1.0659       0.6208        \u001b[35m0.9815\u001b[0m  0.1232\n",
      "     29        1.0722       0.6165        \u001b[35m0.9766\u001b[0m  0.1215\n",
      "     30        1.0561       0.6123        \u001b[35m0.9716\u001b[0m  0.1213\n",
      "     31        1.0494       0.6081        \u001b[35m0.9672\u001b[0m  0.1235\n",
      "     32        1.0485       0.6102        \u001b[35m0.9625\u001b[0m  0.1238\n",
      "     33        1.0568       0.6165        \u001b[35m0.9580\u001b[0m  0.1234\n",
      "     34        1.0557       0.6186        \u001b[35m0.9555\u001b[0m  0.1211\n",
      "     35        1.0608       0.6186        \u001b[35m0.9546\u001b[0m  0.1236\n",
      "     36        1.0675       0.6229        \u001b[35m0.9538\u001b[0m  0.1214\n",
      "     37        1.0551       \u001b[32m0.6250\u001b[0m        0.9542  0.1326\n",
      "     38        1.0854       0.6229        0.9556  0.1297\n",
      "     39        1.0669       \u001b[32m0.6271\u001b[0m        0.9582  0.1215\n",
      "     40        1.0687       \u001b[32m0.6292\u001b[0m        0.9623  0.1212\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.1601\u001b[0m       \u001b[32m0.4470\u001b[0m        \u001b[35m1.0946\u001b[0m  0.1110\n",
      "      2        \u001b[36m1.0913\u001b[0m       \u001b[32m0.5318\u001b[0m        \u001b[35m1.0499\u001b[0m  0.1213\n",
      "      3        \u001b[36m1.0626\u001b[0m       \u001b[32m0.5678\u001b[0m        \u001b[35m1.0303\u001b[0m  0.1192\n",
      "      4        \u001b[36m1.0528\u001b[0m       \u001b[32m0.5847\u001b[0m        \u001b[35m1.0126\u001b[0m  0.1108\n",
      "      5        1.0583       \u001b[32m0.5996\u001b[0m        \u001b[35m0.9950\u001b[0m  0.1110\n",
      "      6        \u001b[36m1.0523\u001b[0m       0.5996        \u001b[35m0.9829\u001b[0m  0.1190\n",
      "      7        \u001b[36m1.0413\u001b[0m       0.5763        \u001b[35m0.9759\u001b[0m  0.1213\n",
      "      8        1.0656       0.5699        \u001b[35m0.9693\u001b[0m  0.1384\n",
      "      9        1.0766       0.5826        \u001b[35m0.9595\u001b[0m  0.1205\n",
      "     10        1.0592       0.5890        \u001b[35m0.9529\u001b[0m  0.1109\n",
      "     11        1.0685       0.5953        0.9578  0.1109\n",
      "     12        \u001b[36m1.0353\u001b[0m       0.5996        0.9677  0.1108\n",
      "     13        1.0485       0.5996        0.9766  0.1111\n",
      "     14        1.0384       0.5996        0.9817  0.1109\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.1933\u001b[0m       \u001b[32m0.4047\u001b[0m        \u001b[35m1.1031\u001b[0m  0.1108\n",
      "      2        \u001b[36m1.1013\u001b[0m       \u001b[32m0.5360\u001b[0m        \u001b[35m1.0421\u001b[0m  0.1110\n",
      "      3        \u001b[36m1.0619\u001b[0m       \u001b[32m0.5657\u001b[0m        \u001b[35m1.0348\u001b[0m  0.1132\n",
      "      4        \u001b[36m1.0602\u001b[0m       0.5657        \u001b[35m1.0241\u001b[0m  0.1292\n",
      "      5        1.0651       0.5614        \u001b[35m1.0114\u001b[0m  0.1110\n",
      "      6        1.0861       0.5614        \u001b[35m1.0029\u001b[0m  0.1108\n",
      "      7        1.0703       \u001b[32m0.5742\u001b[0m        \u001b[35m0.9979\u001b[0m  0.1116\n",
      "      8        1.0732       \u001b[32m0.5869\u001b[0m        \u001b[35m0.9935\u001b[0m  0.1121\n",
      "      9        1.0885       0.5869        \u001b[35m0.9877\u001b[0m  0.1115\n",
      "     10        1.0654       \u001b[32m0.5932\u001b[0m        \u001b[35m0.9842\u001b[0m  0.1111\n",
      "     11        \u001b[36m1.0544\u001b[0m       \u001b[32m0.6017\u001b[0m        \u001b[35m0.9823\u001b[0m  0.1108\n",
      "     12        1.0559       \u001b[32m0.6102\u001b[0m        \u001b[35m0.9785\u001b[0m  0.1132\n",
      "     13        1.0577       \u001b[32m0.6165\u001b[0m        0.9806  0.1137\n",
      "     14        \u001b[36m1.0439\u001b[0m       0.6123        0.9864  0.1190\n",
      "     15        1.0683       0.6144        0.9919  0.1108\n",
      "     16        1.0630       \u001b[32m0.6186\u001b[0m        0.9980  0.1108\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.1554\u001b[0m       \u001b[32m0.4809\u001b[0m        \u001b[35m1.0857\u001b[0m  0.1028\n",
      "      2        \u001b[36m1.0798\u001b[0m       \u001b[32m0.6059\u001b[0m        \u001b[35m1.0267\u001b[0m  0.1109\n",
      "      3        \u001b[36m1.0513\u001b[0m       \u001b[32m0.6081\u001b[0m        \u001b[35m1.0054\u001b[0m  0.1112\n",
      "      4        \u001b[36m1.0255\u001b[0m       \u001b[32m0.6102\u001b[0m        \u001b[35m0.9736\u001b[0m  0.1087\n",
      "      5        \u001b[36m1.0238\u001b[0m       0.6081        \u001b[35m0.9454\u001b[0m  0.1120\n",
      "      6        1.0433       0.6038        \u001b[35m0.9353\u001b[0m  0.1212\n",
      "      7        1.0810       0.6017        \u001b[35m0.9309\u001b[0m  0.1214\n",
      "      8        1.0642       0.6102        \u001b[35m0.9295\u001b[0m  0.1113\n",
      "      9        1.0708       0.6081        0.9355  0.1111\n",
      "     10        \u001b[36m1.0232\u001b[0m       0.6017        0.9611  0.1111\n",
      "     11        \u001b[36m1.0226\u001b[0m       0.5784        0.9959  0.1132\n",
      "     12        1.0404       0.5720        1.0111  0.1215\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.1558\u001b[0m       \u001b[32m0.4492\u001b[0m        \u001b[35m1.0953\u001b[0m  0.1313\n",
      "      2        \u001b[36m1.1019\u001b[0m       \u001b[32m0.5254\u001b[0m        \u001b[35m1.0558\u001b[0m  0.1214\n",
      "      3        \u001b[36m1.0759\u001b[0m       \u001b[32m0.5847\u001b[0m        \u001b[35m1.0479\u001b[0m  0.1217\n",
      "      4        \u001b[36m1.0738\u001b[0m       \u001b[32m0.6081\u001b[0m        \u001b[35m1.0452\u001b[0m  0.1221\n",
      "      5        \u001b[36m1.0636\u001b[0m       0.5911        \u001b[35m1.0440\u001b[0m  0.1112\n",
      "      6        \u001b[36m1.0635\u001b[0m       0.5763        1.0441  0.1217\n",
      "      7        1.0763       0.5636        \u001b[35m1.0376\u001b[0m  0.1419\n",
      "      8        1.0753       0.5657        \u001b[35m1.0184\u001b[0m  0.1317\n",
      "      9        1.0766       0.5720        \u001b[35m0.9911\u001b[0m  0.1214\n",
      "     10        1.0710       0.5847        \u001b[35m0.9683\u001b[0m  0.1295\n",
      "     11        1.0699       0.5996        \u001b[35m0.9547\u001b[0m  0.1214\n",
      "     12        1.0812       0.6059        \u001b[35m0.9527\u001b[0m  0.1133\n",
      "     13        \u001b[36m1.0449\u001b[0m       0.6081        0.9675  0.1107\n",
      "     14        1.0474       0.6059        0.9942  0.1111\n",
      "     15        1.0509       0.5932        1.0313  0.1219\n",
      "     16        1.0539       0.5890        1.0518  0.1212\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.1682\u001b[0m       \u001b[32m0.5000\u001b[0m        \u001b[35m1.0986\u001b[0m  0.1111\n",
      "      2        \u001b[36m1.1126\u001b[0m       \u001b[32m0.5466\u001b[0m        \u001b[35m1.0525\u001b[0m  0.1106\n",
      "      3        \u001b[36m1.0845\u001b[0m       \u001b[32m0.5869\u001b[0m        \u001b[35m1.0413\u001b[0m  0.1126\n",
      "      4        \u001b[36m1.0752\u001b[0m       \u001b[32m0.6081\u001b[0m        \u001b[35m1.0345\u001b[0m  0.1051\n",
      "      5        \u001b[36m1.0733\u001b[0m       0.6081        \u001b[35m1.0248\u001b[0m  0.1113\n",
      "      6        \u001b[36m1.0662\u001b[0m       0.5551        \u001b[35m1.0201\u001b[0m  0.1109\n",
      "      7        \u001b[36m1.0517\u001b[0m       0.5466        \u001b[35m1.0080\u001b[0m  0.1196\n",
      "      8        1.0583       0.5297        \u001b[35m0.9985\u001b[0m  0.1212\n",
      "      9        1.0702       0.5466        \u001b[35m0.9914\u001b[0m  0.1116\n",
      "     10        1.0604       0.5614        \u001b[35m0.9834\u001b[0m  0.1291\n",
      "     11        1.0909       0.5784        \u001b[35m0.9739\u001b[0m  0.1209\n",
      "     12        1.0625       0.5975        \u001b[35m0.9656\u001b[0m  0.1217\n",
      "     13        1.0684       \u001b[32m0.6208\u001b[0m        0.9663  0.1132\n",
      "     14        \u001b[36m1.0291\u001b[0m       \u001b[32m0.6229\u001b[0m        0.9817  0.1092\n",
      "     15        1.0333       0.6165        0.9972  0.1213\n",
      "     16        1.0547       0.6165        1.0064  0.1321\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.1697\u001b[0m       \u001b[32m0.4216\u001b[0m        \u001b[35m1.1082\u001b[0m  0.1189\n",
      "      2        \u001b[36m1.0940\u001b[0m       \u001b[32m0.5742\u001b[0m        \u001b[35m1.0464\u001b[0m  0.1191\n",
      "      3        \u001b[36m1.0617\u001b[0m       \u001b[32m0.6038\u001b[0m        \u001b[35m1.0242\u001b[0m  0.1107\n",
      "      4        \u001b[36m1.0557\u001b[0m       \u001b[32m0.6081\u001b[0m        \u001b[35m0.9908\u001b[0m  0.1218\n",
      "      5        \u001b[36m1.0242\u001b[0m       0.6081        \u001b[35m0.9606\u001b[0m  0.1229\n",
      "      6        1.0292       0.6017        \u001b[35m0.9487\u001b[0m  0.1214\n",
      "      7        1.0649       0.5869        0.9503  0.1212\n",
      "      8        1.0970       0.5657        0.9558  0.1188\n",
      "      9        1.0588       0.5487        0.9690  0.1212\n",
      "     10        1.0789       0.5487        0.9878  0.1134\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.1854\u001b[0m       \u001b[32m0.4492\u001b[0m        \u001b[35m1.0897\u001b[0m  0.1216\n",
      "      2        \u001b[36m1.0990\u001b[0m       \u001b[32m0.5932\u001b[0m        \u001b[35m1.0149\u001b[0m  0.1132\n",
      "      3        \u001b[36m1.0498\u001b[0m       \u001b[32m0.6081\u001b[0m        \u001b[35m0.9874\u001b[0m  0.1108\n",
      "      4        \u001b[36m1.0212\u001b[0m       \u001b[32m0.6165\u001b[0m        \u001b[35m0.9680\u001b[0m  0.1217\n",
      "      5        1.0451       \u001b[32m0.6335\u001b[0m        \u001b[35m0.9610\u001b[0m  0.1215\n",
      "      6        1.0573       0.6165        \u001b[35m0.9567\u001b[0m  0.1110\n",
      "      7        1.0750       0.6229        \u001b[35m0.9503\u001b[0m  0.1110\n",
      "      8        1.0723       0.6123        \u001b[35m0.9497\u001b[0m  0.1107\n",
      "      9        1.0254       0.6038        0.9631  0.1131\n",
      "     10        1.0388       0.5953        0.9797  0.1116\n",
      "     11        1.0432       0.5932        0.9958  0.1212\n",
      "     12        1.0662       0.5975        1.0088  0.1316\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.2748\u001b[0m       \u001b[32m0.3390\u001b[0m        \u001b[35m1.1298\u001b[0m  0.1317\n",
      "      2        \u001b[36m1.1262\u001b[0m       \u001b[32m0.5275\u001b[0m        \u001b[35m1.0410\u001b[0m  0.1295\n",
      "      3        \u001b[36m1.0672\u001b[0m       \u001b[32m0.5614\u001b[0m        \u001b[35m1.0279\u001b[0m  0.1293\n",
      "      4        \u001b[36m1.0486\u001b[0m       \u001b[32m0.5847\u001b[0m        \u001b[35m1.0240\u001b[0m  0.1216\n",
      "      5        1.0659       \u001b[32m0.5953\u001b[0m        \u001b[35m1.0211\u001b[0m  0.1211\n",
      "      6        1.0894       0.5869        \u001b[35m1.0146\u001b[0m  0.1212\n",
      "      7        1.1045       0.5911        \u001b[35m1.0115\u001b[0m  0.1233\n",
      "      8        1.0814       0.5890        \u001b[35m1.0064\u001b[0m  0.1292\n",
      "      9        1.0726       0.5890        \u001b[35m0.9935\u001b[0m  0.1293\n",
      "     10        \u001b[36m1.0476\u001b[0m       \u001b[32m0.6081\u001b[0m        \u001b[35m0.9816\u001b[0m  0.1218\n",
      "     11        1.0588       \u001b[32m0.6102\u001b[0m        \u001b[35m0.9742\u001b[0m  0.1214\n",
      "     12        1.0545       0.6059        \u001b[35m0.9720\u001b[0m  0.1194\n",
      "     13        1.0572       \u001b[32m0.6123\u001b[0m        0.9755  0.1312\n",
      "     14        1.0615       \u001b[32m0.6271\u001b[0m        0.9788  0.1232\n",
      "     15        1.0491       0.6186        0.9817  0.1216\n",
      "     16        \u001b[36m1.0459\u001b[0m       0.6186        0.9873  0.1219\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.1411\u001b[0m       \u001b[32m0.4322\u001b[0m        \u001b[35m1.0616\u001b[0m  0.1293\n",
      "      2        \u001b[36m1.0869\u001b[0m       \u001b[32m0.5530\u001b[0m        \u001b[35m1.0180\u001b[0m  0.1211\n",
      "      3        \u001b[36m1.0371\u001b[0m       \u001b[32m0.5932\u001b[0m        \u001b[35m1.0102\u001b[0m  0.1316\n",
      "      4        1.0474       \u001b[32m0.5953\u001b[0m        \u001b[35m0.9902\u001b[0m  0.1189\n",
      "      5        \u001b[36m1.0309\u001b[0m       \u001b[32m0.5996\u001b[0m        \u001b[35m0.9628\u001b[0m  0.1109\n",
      "      6        \u001b[36m1.0271\u001b[0m       0.5975        \u001b[35m0.9458\u001b[0m  0.1105\n",
      "      7        1.0555       0.5953        \u001b[35m0.9445\u001b[0m  0.1111\n",
      "      8        1.0734       0.5996        0.9479  0.1109\n",
      "      9        1.0634       0.5911        0.9582  0.1112\n",
      "     10        \u001b[36m1.0228\u001b[0m       0.5847        0.9818  0.1190\n",
      "     11        1.0341       0.5784        1.0100  0.1106\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.2040\u001b[0m       \u001b[32m0.4619\u001b[0m        \u001b[35m1.0798\u001b[0m  0.1110\n",
      "      2        \u001b[36m1.1138\u001b[0m       \u001b[32m0.5975\u001b[0m        \u001b[35m0.9904\u001b[0m  0.1110\n",
      "      3        \u001b[36m1.0480\u001b[0m       0.5932        \u001b[35m0.9745\u001b[0m  0.1108\n",
      "      4        \u001b[36m1.0358\u001b[0m       0.5869        \u001b[35m0.9667\u001b[0m  0.1109\n",
      "      5        1.0481       0.5847        \u001b[35m0.9581\u001b[0m  0.1112\n",
      "      6        1.0664       0.5784        \u001b[35m0.9546\u001b[0m  0.1214\n",
      "      7        1.0431       0.5742        0.9579  0.1107\n",
      "      8        1.0617       0.5657        0.9608  0.1110\n",
      "      9        1.0596       0.5869        0.9636  0.1109\n",
      "     10        \u001b[36m1.0292\u001b[0m       0.5975        0.9601  0.1108\n",
      "     11        \u001b[36m1.0142\u001b[0m       \u001b[32m0.6123\u001b[0m        \u001b[35m0.9471\u001b[0m  0.1112\n",
      "     12        1.0184       \u001b[32m0.6165\u001b[0m        \u001b[35m0.9375\u001b[0m  0.1111\n",
      "     13        1.0205       0.6165        \u001b[35m0.9369\u001b[0m  0.1109\n",
      "     14        1.0198       0.6144        \u001b[35m0.9329\u001b[0m  0.1108\n",
      "     15        1.0190       \u001b[32m0.6229\u001b[0m        0.9329  0.1110\n",
      "     16        1.0489       0.6208        0.9401  0.1317\n",
      "     17        1.0303       0.6229        0.9655  0.1294\n",
      "     18        1.0190       \u001b[32m0.6271\u001b[0m        0.9999  0.1214\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.1827\u001b[0m       \u001b[32m0.4110\u001b[0m        \u001b[35m1.1119\u001b[0m  0.1107\n",
      "      2        \u001b[36m1.0918\u001b[0m       \u001b[32m0.6186\u001b[0m        \u001b[35m1.0472\u001b[0m  0.1108\n",
      "      3        \u001b[36m1.0735\u001b[0m       0.6186        \u001b[35m1.0264\u001b[0m  0.1136\n",
      "      4        \u001b[36m1.0541\u001b[0m       0.6102        \u001b[35m1.0025\u001b[0m  0.1118\n",
      "      5        1.0585       0.5996        \u001b[35m0.9824\u001b[0m  0.1107\n",
      "      6        \u001b[36m1.0482\u001b[0m       0.5932        \u001b[35m0.9659\u001b[0m  0.1114\n",
      "      7        \u001b[36m1.0473\u001b[0m       0.5953        \u001b[35m0.9487\u001b[0m  0.1104\n",
      "      8        1.0628       0.6102        \u001b[35m0.9380\u001b[0m  0.1107\n",
      "      9        1.0786       \u001b[32m0.6250\u001b[0m        \u001b[35m0.9306\u001b[0m  0.1107\n",
      "     10        1.0516       \u001b[32m0.6335\u001b[0m        \u001b[35m0.9258\u001b[0m  0.1109\n",
      "     11        1.0606       \u001b[32m0.6441\u001b[0m        \u001b[35m0.9251\u001b[0m  0.1187\n",
      "     12        1.0553       0.6398        0.9283  0.1107\n",
      "     13        \u001b[36m1.0179\u001b[0m       0.6356        0.9384  0.1129\n",
      "     14        1.0382       0.6419        0.9540  0.1109\n",
      "     15        1.0315       0.6441        0.9728  0.1217\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.1028\u001b[0m       \u001b[32m0.5445\u001b[0m        \u001b[35m1.0069\u001b[0m  0.1111\n",
      "      2        \u001b[36m1.0697\u001b[0m       \u001b[32m0.5720\u001b[0m        \u001b[35m0.9835\u001b[0m  0.1200\n",
      "      3        \u001b[36m1.0330\u001b[0m       \u001b[32m0.6208\u001b[0m        \u001b[35m0.9690\u001b[0m  0.1217\n",
      "      4        \u001b[36m1.0181\u001b[0m       0.6017        \u001b[35m0.9518\u001b[0m  0.1215\n",
      "      5        \u001b[36m1.0146\u001b[0m       0.6017        \u001b[35m0.9465\u001b[0m  0.1211\n",
      "      6        1.0601       0.5996        0.9495  0.1213\n",
      "      7        1.0479       0.5975        0.9523  0.1212\n",
      "      8        1.0237       0.5975        0.9540  0.1109\n",
      "      9        1.0471       0.6038        0.9573  0.1116\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.1380\u001b[0m       \u001b[32m0.4174\u001b[0m        \u001b[35m1.0976\u001b[0m  0.1212\n",
      "      2        \u001b[36m1.1055\u001b[0m       \u001b[32m0.5551\u001b[0m        \u001b[35m1.0419\u001b[0m  0.1191\n",
      "      3        \u001b[36m1.0682\u001b[0m       \u001b[32m0.5657\u001b[0m        \u001b[35m1.0239\u001b[0m  0.1212\n",
      "      4        \u001b[36m1.0590\u001b[0m       \u001b[32m0.5869\u001b[0m        \u001b[35m0.9997\u001b[0m  0.1212\n",
      "      5        \u001b[36m1.0360\u001b[0m       0.5805        \u001b[35m0.9740\u001b[0m  0.1292\n",
      "      6        1.0377       0.5636        \u001b[35m0.9593\u001b[0m  0.1195\n",
      "      7        1.0676       0.5869        \u001b[35m0.9514\u001b[0m  0.1214\n",
      "      8        1.0561       \u001b[32m0.5932\u001b[0m        \u001b[35m0.9436\u001b[0m  0.1193\n",
      "      9        1.0631       \u001b[32m0.6059\u001b[0m        \u001b[35m0.9394\u001b[0m  0.1292\n",
      "     10        1.0445       \u001b[32m0.6081\u001b[0m        0.9419  0.1226\n",
      "     11        \u001b[36m1.0119\u001b[0m       0.6038        0.9607  0.1128\n",
      "     12        1.0467       0.5911        0.9948  0.1112\n",
      "     13        1.0420       0.5953        1.0190  0.1108\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.1410\u001b[0m       \u001b[32m0.5636\u001b[0m        \u001b[35m1.0137\u001b[0m  0.1106\n",
      "      2        \u001b[36m1.0726\u001b[0m       \u001b[32m0.6123\u001b[0m        \u001b[35m0.9902\u001b[0m  0.1215\n",
      "      3        \u001b[36m1.0295\u001b[0m       0.6123        \u001b[35m0.9817\u001b[0m  0.1128\n",
      "      4        1.0319       0.6038        0.9906  0.1108\n",
      "      5        1.0497       0.5720        1.0099  0.1106\n",
      "      6        1.0821       0.5636        1.0264  0.1235\n",
      "      7        1.0920       0.5487        1.0349  0.1215\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.2541\u001b[0m       \u001b[32m0.3814\u001b[0m        \u001b[35m1.1407\u001b[0m  0.1215\n",
      "      2        \u001b[36m1.1215\u001b[0m       \u001b[32m0.5106\u001b[0m        \u001b[35m1.0242\u001b[0m  0.1212\n",
      "      3        \u001b[36m1.0556\u001b[0m       \u001b[32m0.5996\u001b[0m        \u001b[35m0.9892\u001b[0m  0.1324\n",
      "      4        \u001b[36m1.0483\u001b[0m       \u001b[32m0.6208\u001b[0m        \u001b[35m0.9617\u001b[0m  0.1114\n",
      "      5        \u001b[36m1.0399\u001b[0m       0.6059        \u001b[35m0.9453\u001b[0m  0.1216\n",
      "      6        1.0681       0.6102        \u001b[35m0.9394\u001b[0m  0.1141\n",
      "      7        1.0697       0.6038        0.9429  0.1131\n",
      "      8        1.0769       0.5890        0.9519  0.1233\n",
      "      9        1.0655       0.6123        0.9641  0.1218\n",
      "     10        1.0815       0.6144        0.9849  0.1231\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.1748\u001b[0m       \u001b[32m0.4386\u001b[0m        \u001b[35m1.0441\u001b[0m  0.1341\n",
      "      2        \u001b[36m1.0946\u001b[0m       \u001b[32m0.6123\u001b[0m        \u001b[35m0.9939\u001b[0m  0.1212\n",
      "      3        \u001b[36m1.0334\u001b[0m       0.6123        \u001b[35m0.9877\u001b[0m  0.1202\n",
      "      4        \u001b[36m1.0158\u001b[0m       0.5975        \u001b[35m0.9842\u001b[0m  0.1196\n",
      "      5        1.0417       0.5593        0.9870  0.1212\n",
      "      6        1.0767       0.5530        0.9875  0.1133\n",
      "      7        1.1089       0.5466        \u001b[35m0.9842\u001b[0m  0.1213\n",
      "      8        1.1074       0.5530        \u001b[35m0.9772\u001b[0m  0.1133\n",
      "      9        1.0760       0.5805        \u001b[35m0.9754\u001b[0m  0.1119\n",
      "     10        1.0492       0.6017        0.9801  0.1214\n",
      "     11        1.0505       \u001b[32m0.6271\u001b[0m        0.9893  0.1216\n",
      "     12        1.0508       \u001b[32m0.6292\u001b[0m        0.9910  0.1212\n",
      "     13        1.0463       0.6208        0.9941  0.1109\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.1648\u001b[0m       \u001b[32m0.4534\u001b[0m        \u001b[35m1.0674\u001b[0m  0.1215\n",
      "      2        \u001b[36m1.0760\u001b[0m       \u001b[32m0.5953\u001b[0m        \u001b[35m1.0106\u001b[0m  0.1232\n",
      "      3        \u001b[36m1.0334\u001b[0m       \u001b[32m0.6102\u001b[0m        \u001b[35m0.9970\u001b[0m  0.1314\n",
      "      4        \u001b[36m1.0295\u001b[0m       \u001b[32m0.6123\u001b[0m        \u001b[35m0.9825\u001b[0m  0.1519\n",
      "      5        1.0606       0.5953        \u001b[35m0.9756\u001b[0m  0.1316\n",
      "      6        1.0624       0.5996        \u001b[35m0.9693\u001b[0m  0.1294\n",
      "      7        1.0702       0.5996        \u001b[35m0.9551\u001b[0m  0.1315\n",
      "      8        1.0387       \u001b[32m0.6144\u001b[0m        \u001b[35m0.9496\u001b[0m  0.1316\n",
      "      9        1.0392       \u001b[32m0.6208\u001b[0m        0.9655  0.1296\n",
      "     10        1.0397       0.6165        0.9809  0.1212\n",
      "     11        1.0648       0.6165        0.9887  0.1296\n",
      "     12        1.0819       0.6144        0.9975  0.1300\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.2627\u001b[0m       \u001b[32m0.2669\u001b[0m        \u001b[35m1.1474\u001b[0m  0.1323\n",
      "      2        \u001b[36m1.1225\u001b[0m       \u001b[32m0.5297\u001b[0m        \u001b[35m1.0737\u001b[0m  0.1318\n",
      "      3        \u001b[36m1.0779\u001b[0m       \u001b[32m0.5911\u001b[0m        \u001b[35m1.0645\u001b[0m  0.1306\n",
      "      4        1.0841       \u001b[32m0.6017\u001b[0m        \u001b[35m1.0544\u001b[0m  0.1314\n",
      "      5        \u001b[36m1.0672\u001b[0m       \u001b[32m0.6038\u001b[0m        \u001b[35m1.0338\u001b[0m  0.1332\n",
      "      6        1.0798       \u001b[32m0.6123\u001b[0m        \u001b[35m1.0045\u001b[0m  0.1213\n",
      "      7        \u001b[36m1.0526\u001b[0m       \u001b[32m0.6271\u001b[0m        \u001b[35m0.9745\u001b[0m  0.1313\n",
      "      8        \u001b[36m1.0471\u001b[0m       0.6229        \u001b[35m0.9530\u001b[0m  0.1422\n",
      "      9        \u001b[36m1.0295\u001b[0m       0.6229        \u001b[35m0.9461\u001b[0m  0.1297\n",
      "     10        1.0736       0.6208        0.9497  0.1314\n",
      "     11        1.0596       0.6081        0.9604  0.1421\n",
      "     12        1.0732       0.5975        0.9738  0.1314\n",
      "     13        \u001b[36m1.0270\u001b[0m       0.5975        0.9872  0.1235\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.2101\u001b[0m       \u001b[32m0.3475\u001b[0m        \u001b[35m1.1245\u001b[0m  0.1132\n",
      "      2        \u001b[36m1.0868\u001b[0m       \u001b[32m0.5742\u001b[0m        \u001b[35m1.0708\u001b[0m  0.1215\n",
      "      3        \u001b[36m1.0557\u001b[0m       \u001b[32m0.5763\u001b[0m        \u001b[35m1.0581\u001b[0m  0.1217\n",
      "      4        1.0639       0.5657        \u001b[35m1.0519\u001b[0m  0.1108\n",
      "      5        1.0558       0.5678        \u001b[35m1.0425\u001b[0m  0.1213\n",
      "      6        1.0873       0.5614        \u001b[35m1.0264\u001b[0m  0.1212\n",
      "      7        1.0774       0.5636        \u001b[35m1.0011\u001b[0m  0.1312\n",
      "      8        \u001b[36m1.0550\u001b[0m       \u001b[32m0.5847\u001b[0m        \u001b[35m0.9654\u001b[0m  0.1296\n",
      "      9        \u001b[36m1.0375\u001b[0m       \u001b[32m0.5932\u001b[0m        \u001b[35m0.9453\u001b[0m  0.1191\n",
      "     10        1.0395       \u001b[32m0.5975\u001b[0m        \u001b[35m0.9442\u001b[0m  0.1200\n",
      "     11        1.0815       0.5911        0.9446  0.1221\n",
      "     12        1.0700       0.5975        0.9487  0.1116\n",
      "     13        1.0626       \u001b[32m0.6038\u001b[0m        0.9607  0.1190\n",
      "     14        1.0523       0.5975        0.9851  0.1213\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.2260\u001b[0m       \u001b[32m0.3962\u001b[0m        \u001b[35m1.1270\u001b[0m  0.1122\n",
      "      2        \u001b[36m1.1191\u001b[0m       \u001b[32m0.5699\u001b[0m        \u001b[35m1.0197\u001b[0m  0.1252\n",
      "      3        \u001b[36m1.0573\u001b[0m       \u001b[32m0.6038\u001b[0m        \u001b[35m0.9914\u001b[0m  0.1232\n",
      "      4        \u001b[36m1.0251\u001b[0m       \u001b[32m0.6123\u001b[0m        \u001b[35m0.9663\u001b[0m  0.1232\n",
      "      5        1.0315       \u001b[32m0.6250\u001b[0m        \u001b[35m0.9370\u001b[0m  0.1192\n",
      "      6        1.0362       0.6165        \u001b[35m0.9210\u001b[0m  0.1182\n",
      "      7        1.0435       \u001b[32m0.6314\u001b[0m        \u001b[35m0.9207\u001b[0m  0.1182\n",
      "      8        1.0510       0.6208        0.9320  0.1202\n",
      "      9        \u001b[36m1.0202\u001b[0m       0.6123        0.9488  0.1232\n",
      "     10        1.0277       0.6123        0.9675  0.1182\n",
      "     11        1.0546       0.5932        0.9814  0.1172\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.1471\u001b[0m       \u001b[32m0.5699\u001b[0m        \u001b[35m1.0003\u001b[0m  0.1262\n",
      "      2        \u001b[36m1.0588\u001b[0m       \u001b[32m0.5911\u001b[0m        \u001b[35m0.9568\u001b[0m  0.1331\n",
      "      3        1.0630       0.5614        0.9742  0.1192\n",
      "      4        1.1034       \u001b[32m0.6081\u001b[0m        1.0305  0.1192\n",
      "      5        1.0857       \u001b[32m0.6335\u001b[0m        1.0668  0.1162\n",
      "      6        1.0893       \u001b[32m0.6441\u001b[0m        1.0863  0.1212\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.0913\u001b[0m       \u001b[32m0.5975\u001b[0m        \u001b[35m1.0247\u001b[0m  0.1142\n",
      "      2        \u001b[36m1.0550\u001b[0m       0.5826        \u001b[35m0.9540\u001b[0m  0.1142\n",
      "      3        1.0569       0.5932        0.9595  0.1152\n",
      "      4        \u001b[36m1.0394\u001b[0m       \u001b[32m0.6059\u001b[0m        0.9795  0.1172\n",
      "      5        \u001b[36m1.0349\u001b[0m       0.5996        1.0009  0.1232\n",
      "      6        1.0515       0.5932        1.0053  0.1411\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.1516\u001b[0m       \u001b[32m0.6123\u001b[0m        \u001b[35m1.0434\u001b[0m  0.1152\n",
      "      2        \u001b[36m1.0589\u001b[0m       0.5975        \u001b[35m0.9583\u001b[0m  0.1252\n",
      "      3        1.1038       0.5911        \u001b[35m0.9567\u001b[0m  0.1271\n",
      "      4        1.0850       0.6123        1.0217  0.1510\n",
      "      5        1.0871       0.6123        1.0671  0.1351\n",
      "      6        1.0988       0.6123        1.0830  0.1281\n",
      "      7        1.0994       \u001b[32m0.6144\u001b[0m        1.0950  0.1281\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.1005\u001b[0m       \u001b[32m0.5847\u001b[0m        \u001b[35m1.0212\u001b[0m  0.1430\n",
      "      2        \u001b[36m1.0307\u001b[0m       0.5805        \u001b[35m0.9732\u001b[0m  0.1301\n",
      "      3        1.0925       0.5805        \u001b[35m0.9662\u001b[0m  0.1271\n",
      "      4        1.0533       \u001b[32m0.6186\u001b[0m        1.0084  0.1411\n",
      "      5        1.0527       \u001b[32m0.6292\u001b[0m        1.0462  0.1271\n",
      "      6        1.0745       0.6271        1.0579  0.1232\n",
      "      7        1.0726       0.6250        1.0710  0.1401\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.1064\u001b[0m       \u001b[32m0.6144\u001b[0m        \u001b[35m1.0553\u001b[0m  0.1371\n",
      "      2        \u001b[36m1.0641\u001b[0m       0.5975        \u001b[35m1.0320\u001b[0m  0.1172\n",
      "      3        1.0754       0.5996        \u001b[35m1.0299\u001b[0m  0.1311\n",
      "      4        1.1024       0.6017        1.0440  0.1579\n",
      "      5        1.0911       \u001b[32m0.6229\u001b[0m        1.0498  0.1451\n",
      "      6        1.1011       0.6229        1.0496  0.1190\n",
      "      7        1.0778       0.6186        1.0545  0.1217\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.0899\u001b[0m       \u001b[32m0.6123\u001b[0m        \u001b[35m0.9599\u001b[0m  0.1316\n",
      "      2        \u001b[36m1.0700\u001b[0m       0.5784        0.9669  0.1233\n",
      "      3        1.0862       0.5763        1.0287  0.1218\n",
      "      4        1.0737       \u001b[32m0.6144\u001b[0m        1.0063  0.1313\n",
      "      5        1.0770       0.6144        1.0012  0.1326\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.0841\u001b[0m       \u001b[32m0.5720\u001b[0m        \u001b[35m1.0362\u001b[0m  0.1313\n",
      "      2        \u001b[36m1.0456\u001b[0m       \u001b[32m0.5890\u001b[0m        \u001b[35m0.9888\u001b[0m  0.1237\n",
      "      3        1.0570       \u001b[32m0.6017\u001b[0m        \u001b[35m0.9440\u001b[0m  0.1214\n",
      "      4        1.0626       \u001b[32m0.6292\u001b[0m        0.9478  0.1213\n",
      "      5        \u001b[36m1.0325\u001b[0m       0.6186        0.9998  0.1218\n",
      "      6        1.0674       0.6081        1.0322  0.1211\n",
      "      7        1.0925       \u001b[32m0.6314\u001b[0m        1.0620  0.1215\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.1194\u001b[0m       \u001b[32m0.6102\u001b[0m        \u001b[35m0.9786\u001b[0m  0.1317\n",
      "      2        \u001b[36m1.0555\u001b[0m       0.6038        \u001b[35m0.9635\u001b[0m  0.1233\n",
      "      3        1.0675       \u001b[32m0.6250\u001b[0m        0.9960  0.1214\n",
      "      4        1.0705       0.6186        1.0235  0.1190\n",
      "      5        1.0728       \u001b[32m0.6314\u001b[0m        1.0115  0.1316\n",
      "      6        1.0692       0.6229        1.0199  0.1136\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.1583\u001b[0m       \u001b[32m0.5847\u001b[0m        \u001b[35m1.0406\u001b[0m  0.1214\n",
      "      2        \u001b[36m1.0826\u001b[0m       0.5847        \u001b[35m1.0153\u001b[0m  0.1211\n",
      "      3        1.1010       \u001b[32m0.6059\u001b[0m        \u001b[35m0.9993\u001b[0m  0.1110\n",
      "      4        \u001b[36m1.0695\u001b[0m       \u001b[32m0.6208\u001b[0m        1.0197  0.1109\n",
      "      5        \u001b[36m1.0643\u001b[0m       0.6144        \u001b[35m0.9971\u001b[0m  0.1234\n",
      "      6        \u001b[36m1.0536\u001b[0m       0.6081        1.0139  0.1220\n",
      "      7        1.0583       0.6144        1.0131  0.1194\n",
      "      8        1.0586       0.6208        0.9987  0.1211\n",
      "      9        \u001b[36m1.0463\u001b[0m       \u001b[32m0.6229\u001b[0m        \u001b[35m0.9948\u001b[0m  0.1624\n",
      "     10        1.0867       \u001b[32m0.6271\u001b[0m        1.0089  0.1337\n",
      "     11        1.0589       \u001b[32m0.6314\u001b[0m        1.0058  0.1213\n",
      "     12        1.0837       \u001b[32m0.6335\u001b[0m        0.9957  0.1316\n",
      "     13        1.0579       0.6271        0.9978  0.1317\n",
      "     14        1.0622       0.6229        \u001b[35m0.9899\u001b[0m  0.1217\n",
      "     15        1.0629       0.6229        1.0058  0.1227\n",
      "     16        1.0579       0.6250        1.0068  0.1218\n",
      "     17        1.0825       0.6250        1.0274  0.1253\n",
      "     18        1.0598       0.6186        1.0550  0.1354\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.1353\u001b[0m       \u001b[32m0.6059\u001b[0m        \u001b[35m0.9936\u001b[0m  0.1295\n",
      "      2        \u001b[36m1.0849\u001b[0m       0.5720        1.0042  0.1319\n",
      "      3        1.0884       0.5720        1.0293  0.1235\n",
      "      4        1.0901       0.5953        1.0172  0.1316\n",
      "      5        1.1177       \u001b[32m0.6271\u001b[0m        1.0278  0.1314\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.1134\u001b[0m       \u001b[32m0.6102\u001b[0m        \u001b[35m1.0150\u001b[0m  0.1215\n",
      "      2        \u001b[36m1.0566\u001b[0m       0.5932        \u001b[35m0.9482\u001b[0m  0.1238\n",
      "      3        1.0930       0.5763        1.0026  0.1227\n",
      "      4        1.0737       0.5869        1.0475  0.1297\n",
      "      5        1.0779       0.6038        1.0612  0.1209\n",
      "      6        1.0739       0.6081        1.0567  0.1108\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.0959\u001b[0m       \u001b[32m0.5890\u001b[0m        \u001b[35m0.9976\u001b[0m  0.1237\n",
      "      2        \u001b[36m1.0303\u001b[0m       0.5869        \u001b[35m0.9732\u001b[0m  0.1215\n",
      "      3        1.0576       0.5763        0.9836  0.1191\n",
      "      4        1.1068       \u001b[32m0.6102\u001b[0m        1.0263  0.1216\n",
      "      5        1.0668       \u001b[32m0.6208\u001b[0m        1.0707  0.1109\n",
      "      6        1.0798       \u001b[32m0.6250\u001b[0m        1.0679  0.1110\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.1070\u001b[0m       \u001b[32m0.6186\u001b[0m        \u001b[35m1.0209\u001b[0m  0.1110\n",
      "      2        \u001b[36m1.0628\u001b[0m       0.5869        \u001b[35m0.9928\u001b[0m  0.1130\n",
      "      3        1.0805       0.5614        0.9942  0.1117\n",
      "      4        1.0767       0.6165        1.0174  0.1088\n",
      "      5        \u001b[36m1.0513\u001b[0m       \u001b[32m0.6292\u001b[0m        1.0384  0.1109\n",
      "      6        1.0766       0.6186        1.0521  0.1106\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.0976\u001b[0m       \u001b[32m0.6123\u001b[0m        \u001b[35m0.9785\u001b[0m  0.1107\n",
      "      2        \u001b[36m1.0629\u001b[0m       0.5932        \u001b[35m0.9665\u001b[0m  0.1107\n",
      "      3        \u001b[36m1.0424\u001b[0m       0.5932        0.9963  0.1103\n",
      "      4        1.0686       0.5826        1.0171  0.1211\n",
      "      5        1.0709       0.5784        1.0359  0.1114\n",
      "      6        1.0576       0.5763        1.0298  0.1196\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.1221\u001b[0m       \u001b[32m0.6017\u001b[0m        \u001b[35m1.0066\u001b[0m  0.1116\n",
      "      2        \u001b[36m1.0699\u001b[0m       0.5953        \u001b[35m0.9614\u001b[0m  0.1108\n",
      "      3        1.0793       0.5614        1.0256  0.1109\n",
      "      4        1.0868       0.5763        1.0458  0.1089\n",
      "      5        1.0875       0.5847        1.0740  0.1111\n",
      "      6        1.0992       \u001b[32m0.6081\u001b[0m        1.0540  0.1107\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.1335\u001b[0m       \u001b[32m0.6123\u001b[0m        \u001b[35m1.0069\u001b[0m  0.1106\n",
      "      2        \u001b[36m1.0388\u001b[0m       0.5784        \u001b[35m0.9537\u001b[0m  0.1112\n",
      "      3        1.0811       0.6081        \u001b[35m0.9427\u001b[0m  0.1106\n",
      "      4        1.0763       \u001b[32m0.6165\u001b[0m        1.0124  0.1107\n",
      "      5        1.0691       0.6102        1.0478  0.1113\n",
      "      6        1.0822       0.6102        1.0472  0.1110\n",
      "      7        1.0960       \u001b[32m0.6229\u001b[0m        1.0467  0.1107\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.1529\u001b[0m       \u001b[32m0.5869\u001b[0m        \u001b[35m1.0444\u001b[0m  0.1028\n",
      "      2        \u001b[36m1.0615\u001b[0m       \u001b[32m0.5911\u001b[0m        \u001b[35m0.9804\u001b[0m  0.1116\n",
      "      3        1.1013       0.5720        1.0236  0.1088\n",
      "      4        1.1175       0.5699        1.0778  0.1109\n",
      "      5        1.0955       \u001b[32m0.6081\u001b[0m        1.0868  0.1110\n",
      "      6        1.0909       0.6081        1.0820  0.1106\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.1024\u001b[0m       \u001b[32m0.6017\u001b[0m        \u001b[35m1.0126\u001b[0m  0.1027\n",
      "      2        \u001b[36m1.0851\u001b[0m       0.5403        \u001b[35m0.9815\u001b[0m  0.1111\n",
      "      3        \u001b[36m1.0602\u001b[0m       0.5805        \u001b[35m0.9623\u001b[0m  0.1112\n",
      "      4        \u001b[36m1.0548\u001b[0m       \u001b[32m0.6059\u001b[0m        0.9818  0.1215\n",
      "      5        1.0649       \u001b[32m0.6292\u001b[0m        0.9981  0.1214\n",
      "      6        1.0728       0.6102        1.0351  0.1189\n",
      "      7        1.0915       0.6144        1.0481  0.1110\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.0917\u001b[0m       \u001b[32m0.6102\u001b[0m        \u001b[35m0.9741\u001b[0m  0.1023\n",
      "      2        \u001b[36m1.0606\u001b[0m       0.5445        \u001b[35m0.9732\u001b[0m  0.1110\n",
      "      3        1.1213       0.5932        0.9826  0.1106\n",
      "      4        1.0809       \u001b[32m0.6292\u001b[0m        1.0256  0.1108\n",
      "      5        1.0701       0.6165        1.0450  0.1029\n",
      "      6        1.0716       0.6165        1.0462  0.1108\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.1031\u001b[0m       \u001b[32m0.6081\u001b[0m        \u001b[35m1.0426\u001b[0m  0.1105\n",
      "      2        \u001b[36m1.0564\u001b[0m       0.5975        \u001b[35m0.9878\u001b[0m  0.1107\n",
      "      3        \u001b[36m1.0358\u001b[0m       \u001b[32m0.6229\u001b[0m        \u001b[35m0.9058\u001b[0m  0.1027\n",
      "      4        1.0732       0.5932        0.9474  0.1109\n",
      "      5        1.0560       0.6208        1.0376  0.1038\n",
      "      6        1.0786       \u001b[32m0.6314\u001b[0m        1.0583  0.1107\n",
      "      7        1.0761       0.6081        1.0482  0.1118\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.1665\u001b[0m       \u001b[32m0.4237\u001b[0m        \u001b[35m1.0884\u001b[0m  0.1314\n",
      "      2        \u001b[36m1.0955\u001b[0m       \u001b[32m0.5744\u001b[0m        \u001b[35m1.0483\u001b[0m  0.1336\n",
      "      3        \u001b[36m1.0790\u001b[0m       \u001b[32m0.6164\u001b[0m        \u001b[35m1.0333\u001b[0m  0.1212\n",
      "      4        1.0846       0.6050        \u001b[35m1.0263\u001b[0m  0.1292\n",
      "      5        \u001b[36m1.0671\u001b[0m       0.5897        \u001b[35m1.0208\u001b[0m  0.1313\n",
      "      6        \u001b[36m1.0599\u001b[0m       0.5859        \u001b[35m1.0121\u001b[0m  0.1316\n",
      "      7        1.0671       0.5878        \u001b[35m0.9899\u001b[0m  0.1309\n",
      "      8        \u001b[36m1.0414\u001b[0m       0.5992        \u001b[35m0.9688\u001b[0m  0.1232\n",
      "      9        1.0608       0.5973        \u001b[35m0.9567\u001b[0m  0.1210\n",
      "     10        1.0736       0.6107        \u001b[35m0.9467\u001b[0m  0.1212\n",
      "     11        1.0685       0.6107        \u001b[35m0.9433\u001b[0m  0.1310\n",
      "     12        \u001b[36m1.0292\u001b[0m       \u001b[32m0.6240\u001b[0m        0.9616  0.1315\n",
      "     13        1.0463       \u001b[32m0.6298\u001b[0m        0.9862  0.1292\n",
      "     14        1.0588       0.6260        1.0051  0.1313\n",
      "     15        1.0637       \u001b[32m0.6355\u001b[0m        1.0168  0.1308\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "\n",
      "Accuracy: 0.62\n",
      "\n",
      "Parameters: {'batch_size': 175, 'lr': 0.01, 'max_epochs': 40, 'module__dropout': 0.5, 'optimizer': <class 'torch.optim.sgd.SGD'>, 'optimizer__momentum': 1.0}\n"
     ]
    }
   ],
   "source": [
    "cvGS = GridSearchCV(net,\n",
    "                    parameters,\n",
    "                    refit=True, \n",
    "                    cv=10\n",
    "                    \n",
    "                    \n",
    "                    , \n",
    "                    scoring='accuracy', \n",
    "                    verbose=1)\n",
    "\n",
    "cvGS.fit(X_train_standard, y_train)\n",
    "\n",
    "print(\"\")\n",
    "print(\"Accuracy: {:.2f}\".format(cvGS.best_score_))\n",
    "print(\"\")\n",
    "print(\"Parameters: {}\".format(cvGS.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "e659ff2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.1591\u001b[0m       \u001b[32m0.6086\u001b[0m        \u001b[35m0.9811\u001b[0m  0.2006\n",
      "      2        \u001b[36m1.0358\u001b[0m       0.5971        \u001b[35m0.9693\u001b[0m  0.1048\n",
      "      3        1.0859       0.6057        0.9860  0.0899\n",
      "      4        1.0742       0.5857        0.9925  0.0980\n",
      "      5        1.0495       \u001b[32m0.6400\u001b[0m        0.9807  0.0918\n",
      "      6        1.0538       0.6171        0.9775  0.1113\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.0897\u001b[0m       \u001b[32m0.6029\u001b[0m        \u001b[35m1.0180\u001b[0m  0.1109\n",
      "      2        \u001b[36m1.0208\u001b[0m       \u001b[32m0.6057\u001b[0m        \u001b[35m0.9643\u001b[0m  0.1132\n",
      "      3        1.0355       0.5686        0.9654  0.1125\n",
      "      4        1.0545       0.5943        0.9856  0.0985\n",
      "      5        1.0709       0.5886        1.0166  0.1129\n",
      "      6        1.0576       0.6029        1.0145  0.2083\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.0966\u001b[0m       \u001b[32m0.5686\u001b[0m        \u001b[35m1.0254\u001b[0m  0.0902\n",
      "      2        \u001b[36m1.0516\u001b[0m       \u001b[32m0.6114\u001b[0m        \u001b[35m0.9900\u001b[0m  0.1041\n",
      "      3        \u001b[36m1.0415\u001b[0m       0.5857        \u001b[35m0.9694\u001b[0m  0.1061\n",
      "      4        1.0534       0.5800        \u001b[35m0.9580\u001b[0m  0.1341\n",
      "      5        \u001b[36m1.0312\u001b[0m       0.5914        0.9924  0.1134\n",
      "      6        1.0656       0.5914        1.0260  0.1056\n",
      "      7        1.0751       0.5886        1.0477  0.1115\n",
      "      8        1.0618       0.5771        1.0606  0.0995\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "\n",
      "Accuracy: 0.61\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# cross validation\n",
    "\n",
    "cvRS = RandomizedSearchCV(net, \n",
    "                           parameters, \n",
    "                           refit=False, \n",
    "                           cv=3, \n",
    "                           scoring='accuracy', \n",
    "                           verbose=0)\n",
    "\n",
    "cvRS.fit(X_train_standard, y_train)\n",
    "\n",
    "print(\"\")\n",
    "print(\"Accuracy: {:.2f}\".format(cvRS.best_score_))\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b678102",
   "metadata": {},
   "source": [
    "### Chapter 6: Building a SVM<a class=\"anchor\" id=\"chapter06\"></a>\n",
    "\n",
    "### 6.1. Parameters <a class=\"anchor\" id=\"section_6_1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "be9c4a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_classifier = SVC(kernel = 'linear', random_state = 0)\n",
    "\n",
    "svm_classifier.fit(X_smote_train, y_smote_train)\n",
    "\n",
    "y_pred_svm = svm_classifier.predict(X_test_standard)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7583cf41",
   "metadata": {},
   "source": [
    "### 6.2. Training results <a class=\"anchor\" id=\"section_6_2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "b51c9c41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, y_pred_svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c1e910",
   "metadata": {},
   "source": [
    "### 6.3. Hyperparameter tuning <a class=\"anchor\" id=\"section_6_3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "c9eefe38",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_classifier = SVC(kernel = 'rbf', random_state = 0)\n",
    "\n",
    "\n",
    "svm_classifier.fit(X_train_standard, y_train)\n",
    "\n",
    "y_pred_svm_tuned = svm_classifier.predict(X_test_standard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "478a8d7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 67.29 %\n",
      "Standard Deviation: 1.82 %\n"
     ]
    }
   ],
   "source": [
    "accuracy = cross_val_score(svm_classifier, X_train_standard, y_train, cv = 10)\n",
    "print(\"Accuracy: {:.2f} %\".format(accuracy.mean()*100))\n",
    "print(\"Standard Deviation: {:.2f} %\".format(accuracy.std()*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90bb2ce7",
   "metadata": {},
   "source": [
    "### Chapter 7: Final models <a class=\"anchor\" id=\"chapter07\"></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a403d26",
   "metadata": {},
   "source": [
    "### 7.1 MLP <a class=\"anchor\" id=\"section_7_1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "a203bb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultilayerPerceptron(nn.Module):\n",
    "    def __init__(self,\n",
    "        input_dim = input_layer,\n",
    "        hidden_dim = hidden_layer,\n",
    "        output_dim= output_layer,\n",
    "        dropout=0.5,\n",
    "    ):\n",
    "        \n",
    "        super(MultilayerPerceptron, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout) \n",
    "        \n",
    "        self.linear1 = nn.Linear(input_layer, hidden_layer)\n",
    "        self.linear2 = nn.Linear(hidden_layer, output_layer)\n",
    "        \n",
    "    def __getitem__(self,input_dim, hidden_dim,output_dim):\n",
    "        \n",
    "        return input_dim, hidden_dim, output_dim\n",
    "    \n",
    "    def forward(self, inputs, **kwargs):\n",
    "        inputs = F.relu(self.linear1(inputs)) # pass output of hidden layer through a relu activation function\n",
    "        inputs = self.dropout(inputs) # apply dropout\n",
    "        # removing the softmax activation function\n",
    "        \n",
    "        return inputs\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "net = NeuralNetClassifier(\n",
    "    MultilayerPerceptron,\n",
    "    callbacks = [EarlyStopping()],\n",
    "    criterion = nn.CrossEntropyLoss,\n",
    "    device = device, \n",
    ")\n",
    "\n",
    "parameters = {\n",
    "    'lr': [0.1],\n",
    "    'batch_size': [200],\n",
    "    'optimizer': [optim.SGD],\n",
    "    'max_epochs': [40], \n",
    "    'module__dropout': [0.5],\n",
    "    'optimizer__momentum': [1.0]\n",
    "}\n",
    "\n",
    "cvRS = RandomizedSearchCV(net, \n",
    "                           parameters, \n",
    "                           cv=3, \n",
    "                           scoring='accuracy', \n",
    "                           verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "fc576e16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.1539\u001b[0m       \u001b[32m0.4205\u001b[0m        \u001b[35m1.1221\u001b[0m  0.0471\n",
      "      2        \u001b[36m1.1297\u001b[0m       \u001b[32m0.4773\u001b[0m        \u001b[35m1.0827\u001b[0m  0.0270\n",
      "      3        \u001b[36m1.0962\u001b[0m       \u001b[32m0.6023\u001b[0m        \u001b[35m1.0519\u001b[0m  0.0156\n",
      "      4        1.0995       \u001b[32m0.6250\u001b[0m        \u001b[35m1.0403\u001b[0m  0.0233\n",
      "      5        \u001b[36m1.0779\u001b[0m       0.6250        \u001b[35m1.0290\u001b[0m  0.0203\n",
      "      6        \u001b[36m1.0473\u001b[0m       0.6250        \u001b[35m1.0179\u001b[0m  0.0289\n",
      "      7        1.0604       0.6023        \u001b[35m1.0081\u001b[0m  0.0290\n",
      "      8        \u001b[36m1.0370\u001b[0m       0.6136        \u001b[35m0.9980\u001b[0m  0.0289\n",
      "      9        1.0591       0.6250        \u001b[35m0.9855\u001b[0m  0.0303\n",
      "     10        1.0558       \u001b[32m0.6477\u001b[0m        \u001b[35m0.9725\u001b[0m  0.0306\n",
      "     11        \u001b[36m1.0185\u001b[0m       0.6364        \u001b[35m0.9624\u001b[0m  0.0227\n",
      "     12        1.0257       0.6364        \u001b[35m0.9561\u001b[0m  0.0284\n",
      "     13        1.1086       0.6364        \u001b[35m0.9550\u001b[0m  0.0311\n",
      "     14        1.0724       \u001b[32m0.6591\u001b[0m        0.9567  0.0302\n",
      "     15        1.0835       0.6477        0.9618  0.0225\n",
      "     16        1.0209       \u001b[32m0.6705\u001b[0m        0.9689  0.0283\n",
      "     17        1.0713       0.6705        0.9758  0.0311\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.1772\u001b[0m       \u001b[32m0.3864\u001b[0m        \u001b[35m1.0919\u001b[0m  0.0294\n",
      "      2        \u001b[36m1.1273\u001b[0m       \u001b[32m0.4545\u001b[0m        \u001b[35m1.0492\u001b[0m  0.0308\n",
      "      3        \u001b[36m1.0664\u001b[0m       \u001b[32m0.5114\u001b[0m        \u001b[35m1.0267\u001b[0m  0.0280\n",
      "      4        \u001b[36m1.0411\u001b[0m       \u001b[32m0.5682\u001b[0m        \u001b[35m1.0209\u001b[0m  0.0317\n",
      "      5        \u001b[36m1.0157\u001b[0m       \u001b[32m0.6023\u001b[0m        \u001b[35m1.0090\u001b[0m  0.0402\n",
      "      6        1.0257       0.5909        \u001b[35m0.9997\u001b[0m  0.0298\n",
      "      7        1.0240       0.5909        \u001b[35m0.9905\u001b[0m  0.0269\n",
      "      8        1.0193       0.5682        \u001b[35m0.9852\u001b[0m  0.0231\n",
      "      9        1.0590       0.5568        \u001b[35m0.9802\u001b[0m  0.0266\n",
      "     10        1.0187       0.5682        \u001b[35m0.9720\u001b[0m  0.0279\n",
      "     11        1.0683       0.5795        \u001b[35m0.9669\u001b[0m  0.0273\n",
      "     12        1.0659       0.5795        0.9679  0.0304\n",
      "     13        1.0502       \u001b[32m0.6136\u001b[0m        0.9720  0.0321\n",
      "     14        1.0853       0.5909        0.9754  0.0281\n",
      "     15        1.1113       0.6023        0.9812  0.0278\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.1387\u001b[0m       \u001b[32m0.3523\u001b[0m        \u001b[35m1.0999\u001b[0m  0.0255\n",
      "      2        \u001b[36m1.1151\u001b[0m       \u001b[32m0.3977\u001b[0m        \u001b[35m1.0562\u001b[0m  0.0294\n",
      "      3        \u001b[36m1.1023\u001b[0m       \u001b[32m0.5341\u001b[0m        \u001b[35m1.0279\u001b[0m  0.0304\n",
      "      4        \u001b[36m1.0631\u001b[0m       \u001b[32m0.5682\u001b[0m        \u001b[35m1.0137\u001b[0m  0.0303\n",
      "      5        \u001b[36m1.0576\u001b[0m       \u001b[32m0.5795\u001b[0m        \u001b[35m1.0022\u001b[0m  0.0223\n",
      "      6        \u001b[36m1.0435\u001b[0m       \u001b[32m0.5909\u001b[0m        \u001b[35m0.9905\u001b[0m  0.0304\n",
      "      7        1.0765       \u001b[32m0.6250\u001b[0m        \u001b[35m0.9854\u001b[0m  0.0223\n",
      "      8        1.1153       0.6136        0.9876  0.0202\n",
      "      9        1.0557       0.6023        0.9895  0.0304\n",
      "     10        1.1001       0.5795        0.9888  0.0224\n",
      "     11        \u001b[36m0.9980\u001b[0m       0.5909        0.9893  0.0283\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.1160\u001b[0m       \u001b[32m0.4773\u001b[0m        \u001b[35m1.0843\u001b[0m  0.0448\n",
      "      2        \u001b[36m1.0636\u001b[0m       \u001b[32m0.5000\u001b[0m        \u001b[35m1.0552\u001b[0m  0.0417\n",
      "      3        \u001b[36m1.0597\u001b[0m       \u001b[32m0.5833\u001b[0m        \u001b[35m1.0304\u001b[0m  0.0417\n",
      "      4        \u001b[36m1.0130\u001b[0m       0.5606        \u001b[35m1.0020\u001b[0m  0.0417\n",
      "      5        1.0624       0.5682        \u001b[35m0.9836\u001b[0m  0.0447\n",
      "      6        1.0357       0.5530        \u001b[35m0.9768\u001b[0m  0.0427\n",
      "      7        1.0701       0.5833        0.9783  0.0437\n",
      "      8        1.1053       \u001b[32m0.5985\u001b[0m        0.9770  0.0387\n",
      "      9        1.0541       \u001b[32m0.6061\u001b[0m        0.9776  0.0397\n",
      "     10        1.0538       0.6061        \u001b[35m0.9762\u001b[0m  0.0407\n",
      "     11        1.0589       0.6061        0.9861  0.0417\n",
      "     12        \u001b[36m0.9921\u001b[0m       0.5833        1.0058  0.0507\n",
      "     13        1.0613       0.5833        1.0203  0.0407\n",
      "     14        1.0574       0.6061        1.0385  0.0407\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "\n",
      "Accuracy: 0.59\n"
     ]
    }
   ],
   "source": [
    "cvRS.fit(X_test_standard, y_test)\n",
    "MLP_pred_final = cvRS.predict(X_test_standard)\n",
    "\n",
    "print(\"\")\n",
    "print(\"Accuracy: {:.2f}\".format(cvRS.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0538cc",
   "metadata": {},
   "source": [
    "### 7.2 SVM <a class=\"anchor\" id=\"section_7_2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "2dc2d0c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 62.52 %\n",
      "Standard Deviation: 2.95 %\n"
     ]
    }
   ],
   "source": [
    "svm_classifier = SVC(kernel = 'rbf', random_state = 0)\n",
    "\n",
    "svm_classifier.fit(X_test_standard, y_test)\n",
    "\n",
    "accuracy = cross_val_score(svm_classifier, X_test_standard, y_test, cv = 10)\n",
    "print(\"Accuracy: {:.2f} %\".format(accuracy.mean()*100))\n",
    "print(\"Standard Deviation: {:.2f} %\".format(accuracy.std()*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c9ece4",
   "metadata": {},
   "source": [
    "### 7.3 Confusion matrix <a class=\"anchor\" id=\"section_7_3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "496574fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 57 344]\n",
      " [ 11 244]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create confusion matrix\n",
    "# adapted from: https://www.stackvidhya.com/plot-confusion-matrix-in-python-and-why/\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cfmatrix_MLP = confusion_matrix(y_test, y_pred)\n",
    "print(cfmatrix_MLP)\n",
    "ax = sns.heatmap(cfmatrix_MLP, annot=True, cmap='Blues')\n",
    "\n",
    "ax.set_title('MLP Confusion Matrix')\n",
    "ax.set_xlabel('Predicted Values')\n",
    "ax.set_ylabel('Actual Values')\n",
    "\n",
    "ax.xaxis.set_ticklabels(['Not potable', 'Potable'])\n",
    "ax.yaxis.set_ticklabels(['Not potable', 'Potable'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "4ee36347",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[214 187]\n",
      " [141 114]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create confusion matrix\n",
    "# adapted from: https://www.stackvidhya.com/plot-confusion-matrix-in-python-and-why/\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cfmatrix_SVM = confusion_matrix(y_test, y_pred_svm)\n",
    "print(cfmatrix_SVM)\n",
    "ax = sns.heatmap(cfmatrix_SVM, annot=True, cmap='Blues')\n",
    "\n",
    "ax.set_title('SVM Confusion Matrix')\n",
    "ax.set_xlabel('Predicted Values')\n",
    "ax.set_ylabel('Actual Values')\n",
    "\n",
    "ax.xaxis.set_ticklabels(['Not potable', 'Potable'])\n",
    "ax.yaxis.set_ticklabels(['Not potable', 'Potable'])\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
